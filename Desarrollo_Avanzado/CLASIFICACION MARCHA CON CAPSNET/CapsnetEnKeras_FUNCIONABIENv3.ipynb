{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "CapsnetEnKeras.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oieoYd8_NJl4"
      },
      "source": [
        "# Parte 4: Clasificación con Capsnet\n",
        "\n",
        "La implementación de esta red Capsnet se ha basado en el código implementado por Aurélien Géron (https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOYsNIkfNJl7"
      },
      "source": [
        "### Resultados con la arquitectura anterior: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zeUMdGrNJl8"
      },
      "source": [
        "### Cambios realizados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvWOKZzaNJl8"
      },
      "source": [
        "Se aumenta de 100 a 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5eey6oGNJl8"
      },
      "source": [
        "### Nuevos resultados: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGdAPW3NNJl9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRy2Mth1NJl9"
      },
      "source": [
        "### 1 - Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cse2ICU7NJl-"
      },
      "source": [
        "# Tensorflow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#Helper libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Signal libraries\n",
        "from scipy import signal"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZdDJXPXNJl-"
      },
      "source": [
        "# Reset the default graph, in case you re-run this notebook without restarting the kernel:\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# Random seeds so that this notebook always produces the same output:\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(45)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4TwxUdJNJl_"
      },
      "source": [
        "### 2 - Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19rmC_7VNJl_"
      },
      "source": [
        "class ROutput:\n",
        "    def __init__(self, task, data):\n",
        "        self.task = task\n",
        "        self.data = data\n",
        "        \n",
        "class OutTaskData: \n",
        "    def __init__(self, task, data): \n",
        "        self.task = task\n",
        "        self.data = data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMzZss4DNJmA"
      },
      "source": [
        "import scipy.io as sio\n",
        "# Primero leemos los registros\n",
        "def read_outputs(rec):\n",
        "    '''read_outputs(\"userS0091f1.mat\")'''\n",
        "    mat = sio.loadmat(rec)\n",
        "    mdata = mat['session']\n",
        "    val = mdata[0,0]\n",
        "    #output = ROutput(np.array(val[\"task\"]), np.array(val[\"data\"]))\n",
        "    output = ROutput(np.array(val[\"task_EEG_p\"]), np.array(val[\"data_processed_EEG\"]))\n",
        "    return output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7EF2GDqNJmA"
      },
      "source": [
        "### Cargamos los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdNLHsqfNJmA"
      },
      "source": [
        "# Configuración\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import Perceptron\n",
        "#from keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "task1 = 402 # SE PUEDE CAMBIAR\n",
        "task2 = 404 # SE PUEDE CAMBIAR\n",
        "task_OneHotEnconding = {402: [1.,0.], 404: [0.,1.]}\n",
        "user = 'W29' # SE PUEDE CAMBIAR\n",
        "day = '0329'\n",
        "folder_day = 'W29-29_03_2021'\n",
        "total_records = 22 # CAMBIAR SI HAY MAS REGISTROS\n",
        "fm = 200\n",
        "electrodes_names_selected = ['F3', 'FZ', 'FC1','FCZ','C1','CZ','CP1','CPZ', 'FC5', 'FC3','C5','C3','CP5','CP3','P3',\n",
        "                             'PZ','F4','FC2','FC4','FC6','C2','C4','CP2','CP4','C6','CP6','P4','HR' ,'HL', 'VU', 'VD']\n",
        "number_channels = len(electrodes_names_selected)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW2fa7fbNJmB",
        "outputId": "b9539b4b-26eb-4cb1-8669-1001182fe2e4"
      },
      "source": [
        "lTaskData = []\n",
        "total_records_used = 0\n",
        "for i_rec in range(1,total_records+1):\n",
        "    i_rec_record = i_rec\n",
        "    if i_rec_record <10:\n",
        "        i_rec_record = \"0\"+str(i_rec_record)\n",
        "    if i_rec % 2 == 0: # Registros impares primero: USUARIO SIN MOVIMIENTO SOLO PENSANDO\n",
        "        record = \"./RegistrosProcesados2/W29_2021\"+day+\"_openloop_\"+str(i_rec_record)+\"_processed.mat\"\n",
        "        output = read_outputs(record) # output.task será y, output.data será x\n",
        "\n",
        "\n",
        "        output.task = np.transpose(output.task)\n",
        "        output.data = output.data.reshape((np.shape(output.data)[0],np.shape(output.data)[1]))\n",
        "        output.data = np.transpose(output.data)\n",
        "        #output.data = output.data.reshape((np.shape(output.data)[0],np.shape(output.data)[1],1))\n",
        "\n",
        "        outT = (output.task == task1) | (output.task == task2)\n",
        "        outData = output.data[0:np.shape(output.data)[0], outT[0,:]]\n",
        "        outTask = output.task[0, outT[0,:]]\n",
        "        outTD = OutTaskData(outTask, outData)\n",
        "\n",
        "        lTaskData.append(outTD)\n",
        "        total_records_used+=1\n",
        "print(total_records_used, total_records)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqrivmkzNJmC",
        "outputId": "4bc0c3fb-5e00-40cf-a804-b31e8abf83fd"
      },
      "source": [
        "# Vamos a coger 2 registros para el entrenamiento, 1 para el conjunto dev set, 1 para el test set\n",
        "X_train, y_train, X_dev, y_dev, X_test, y_test = [],[],[],[],[],[] \n",
        "for j in range(0,total_records_used-3): # Cogemos 18 registros para entrenamiento\n",
        "    X_train.append(lTaskData[j].data)\n",
        "    y_train.append(lTaskData[j].task)\n",
        "\n",
        "for j in range(total_records_used-3,total_records_used-1): # Cogemos 2 registros para el dev set\n",
        "    X_dev.append(lTaskData[j].data)\n",
        "    y_dev.append(lTaskData[j].task)\n",
        "for j in range(total_records_used-1,total_records_used): # Cogemos 2 registros para el test set\n",
        "    X_test.append(lTaskData[j].data)\n",
        "    y_test.append(lTaskData[j].task)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "#y_train = np.ravel(np.array(y_train))\n",
        "y_train = np.array(y_train)\n",
        "X_dev = np.array(X_dev)\n",
        "#y_dev = np.ravel(np.array(y_dev))\n",
        "y_dev = np.array(y_dev)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "#y_test = np.ravel(np.array(y_test))\n",
        "\n",
        "print (\"X_train:\",X_train.shape)\n",
        "print (\"y_train:\",y_train.shape)\n",
        "print (\"X_dev:\",X_dev.shape)\n",
        "print (\"y_dev:\",y_dev.shape)\n",
        "print (\"X_test:\",X_test.shape)\n",
        "print (\"y_test:\",y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# VENTANEO Y ONE HOT ENCODING \n",
        "window = 5\n",
        "samples_advance = 3\n",
        "\n",
        "# Ventaneo X_train\n",
        "\n",
        "X_train_l = []\n",
        "y_train_l = []\n",
        "for num_X_train in range(np.shape(X_train)[0]): # Para no mezclar registros\n",
        "    win_init = int(0)\n",
        "    window_position = 0\n",
        "    \n",
        "    for i in range(np.shape(X_train)[2]): # For each signal registered\n",
        "        win_end = int(win_init + window)\n",
        "        if win_end >= np.shape(X_train)[2]:\n",
        "            break\n",
        "\n",
        "        task = np.unique(y_train[num_X_train,win_init:win_end])\n",
        "\n",
        "        if len(task)==1:\n",
        "        #if task1 in task or task2 in task:\n",
        "            signal_window = X_train[num_X_train, :, win_init:win_end]\n",
        "            \n",
        "            #data_filtered = preprocessing(signal_window, fm, number_channels)\n",
        "            #X_train_l.append(data_filtered)\n",
        "            X_train_l.append(signal_window)\n",
        "            taskOH = task_OneHotEnconding[task[0]]\n",
        "            y_train_l.append(taskOH)\n",
        "            #y_train_l.append(task)\n",
        "            \n",
        "            \n",
        "        win_init += int(samples_advance)\n",
        "\n",
        "X_train_l = np.array(X_train_l)\n",
        "y_train_l = np.array(y_train_l)\n",
        "\n",
        "\n",
        "# Ventaneo X_dev\n",
        "X_dev_l = []\n",
        "y_dev_l = []\n",
        "for num_X_dev in range(np.shape(X_dev)[0]):\n",
        "    win_init = int(0)\n",
        "    window_position = 0\n",
        "    \n",
        "    for i in range(np.shape(X_dev)[2]): # For each signal registered\n",
        "        win_end = int(win_init + window)\n",
        "        if win_end >= np.shape(X_dev)[2]:\n",
        "            break\n",
        "\n",
        "        task = np.unique(y_dev[num_X_dev,win_init:win_end])\n",
        "\n",
        "        if len(task)==1:\n",
        "        #if task1 in task or task2 in task:\n",
        "            signal_window = X_dev[num_X_dev, :, win_init:win_end]\n",
        "            \n",
        "            #data_filtered = preprocessing(signal_window, fm, number_channels)\n",
        "            #X_train_l.append(data_filtered)\n",
        "            X_dev_l.append(signal_window)\n",
        "            taskOH = task_OneHotEnconding[task[0]]\n",
        "            y_dev_l.append(taskOH)\n",
        "            #y_dev_l.append(task)\n",
        "            \n",
        "        win_init += int(samples_advance)\n",
        "\n",
        "X_dev_l = np.array(X_dev_l)\n",
        "y_dev_l = np.array(y_dev_l)\n",
        "\n",
        "# Ventaneo X_test\n",
        "X_test_l = []\n",
        "y_test_l = []\n",
        "for num_X_test in range(np.shape(X_test)[0]): \n",
        "    win_init = int(0)\n",
        "    window_position = 0\n",
        "    \n",
        "    for i in range(np.shape(X_test)[2]): # For each signal registered\n",
        "        win_end = int(win_init + window)\n",
        "        if win_end >= np.shape(X_test)[2]:\n",
        "            break\n",
        "\n",
        "        task = np.unique(y_test[num_X_test,win_init:win_end])\n",
        "\n",
        "        if len(task)==1:\n",
        "        #if task1 in task or task2 in task:\n",
        "            signal_window = X_test[num_X_test, :, win_init:win_end]\n",
        "            \n",
        "            #data_filtered = preprocessing(signal_window, fm, number_channels)\n",
        "            #X_train_l.append(data_filtered)\n",
        "            X_test_l.append(signal_window)\n",
        "            taskOH = task_OneHotEnconding[task[0]]\n",
        "            #y_test_l.append(task)\n",
        "            y_test_l.append(taskOH)\n",
        "            \n",
        "        win_init += int(samples_advance)\n",
        "\n",
        "X_test_l = np.array(X_test_l)\n",
        "y_test_l = np.array(y_test_l)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train_l = X_train_l.reshape((np.shape(X_train_l)[0],np.shape(X_train_l)[1],np.shape(X_train_l)[2], 1))\n",
        "X_dev_l = X_dev_l.reshape((np.shape(X_dev_l)[0],np.shape(X_dev_l)[1],np.shape(X_dev_l)[2], 1))\n",
        "X_test_l = X_test_l.reshape((np.shape(X_test_l)[0],np.shape(X_test_l)[1],np.shape(X_test_l)[2], 1))\n",
        "\n",
        "print()\n",
        "print(\"ONE HOT ENCODER & WINDOWING:\")\n",
        "print (\"X_train:\",X_train_l.shape)\n",
        "print (\"y_train:\",y_train_l.shape)\n",
        "print (\"X_dev:\",X_dev_l.shape)\n",
        "print (\"y_dev:\",y_dev_l.shape)\n",
        "print (\"X_test:\",X_test_l.shape)\n",
        "print (\"y_test:\",y_test_l.shape)\n",
        "\n",
        "X_train = X_train_l\n",
        "y_train = y_train_l\n",
        "X_dev = X_dev_l\n",
        "y_dev = y_dev_l\n",
        "X_test = X_test_l\n",
        "y_test = y_test_l\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_dev = X_dev.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "y_train = y_train.astype('float32')\n",
        "y_dev = y_dev.astype('float32')\n",
        "y_test = y_test.astype('float32')\n",
        "print()\n",
        "print(\"RESHAPE:\")\n",
        "print (\"X_train:\",X_train_l.shape)\n",
        "print (\"y_train:\",y_train_l.shape)\n",
        "print (\"X_dev:\",X_dev_l.shape)\n",
        "print (\"y_dev:\",y_dev_l.shape)\n",
        "print (\"X_test:\",X_test_l.shape)\n",
        "print (\"y_test:\",y_test_l.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train: (8, 32, 49)\n",
            "y_train: (8, 49)\n",
            "X_dev: (2, 32, 49)\n",
            "y_dev: (2, 49)\n",
            "X_test: (1, 32, 49)\n",
            "y_test: (1, 49)\n",
            "\n",
            "ONE HOT ENCODER & WINDOWING:\n",
            "X_train: (104, 32, 5, 1)\n",
            "y_train: (104, 2)\n",
            "X_dev: (26, 32, 5, 1)\n",
            "y_dev: (26, 2)\n",
            "X_test: (13, 32, 5, 1)\n",
            "y_test: (13, 2)\n",
            "\n",
            "RESHAPE:\n",
            "X_train: (104, 32, 5, 1)\n",
            "y_train: (104, 2)\n",
            "X_dev: (26, 32, 5, 1)\n",
            "y_dev: (26, 2)\n",
            "X_test: (13, 32, 5, 1)\n",
            "y_test: (13, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gQbqp_NJmG"
      },
      "source": [
        "### 4 - Construcción de la Capsnet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "tDzWsLaYNJmG",
        "outputId": "73e01eec-8d13-4ed0-f573-d75fcf7970fa"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import csv\n",
        "import math\n",
        "import pandas\n",
        "\"\"\"\n",
        "def plot_log(filename, show=True):\n",
        "\n",
        "    data = pandas.read_csv(filename)\n",
        "\n",
        "    fig = plt.figure(figsize=(4,6))\n",
        "    fig.subplots_adjust(top=0.95, bottom=0.05, right=0.95)\n",
        "    fig.add_subplot(211)\n",
        "    for key in data.keys():\n",
        "        if key.find('loss') >= 0 and not key.find('val') >= 0:  # training loss\n",
        "            plt.plot(data['epoch'].values, data[key].values, label=key)\n",
        "    plt.legend()\n",
        "    plt.title('Training loss')\n",
        "\n",
        "    fig.add_subplot(212)\n",
        "    for key in data.keys():\n",
        "        if key.find('acc') >= 0:  # acc\n",
        "            plt.plot(data['epoch'].values, data[key].values, label=key)\n",
        "    plt.legend()\n",
        "    plt.title('Training and validation accuracy')\n",
        "\n",
        "    # fig.savefig('result/log.png')\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def combine_images(generated_images, height=None, width=None):\n",
        "    num = generated_images.shape[0]\n",
        "    if width is None and height is None:\n",
        "        width = int(math.sqrt(num))\n",
        "        height = int(math.ceil(float(num)/width))\n",
        "    elif width is not None and height is None:  # height not given\n",
        "        height = int(math.ceil(float(num)/width))\n",
        "    elif height is not None and width is None:  # width not given\n",
        "        width = int(math.ceil(float(num)/height))\n",
        "\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    plot_log('result_/sub_dependent_/log_fold.csv')\n",
        "\"\"\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef plot_log(filename, show=True):\\n\\n    data = pandas.read_csv(filename)\\n\\n    fig = plt.figure(figsize=(4,6))\\n    fig.subplots_adjust(top=0.95, bottom=0.05, right=0.95)\\n    fig.add_subplot(211)\\n    for key in data.keys():\\n        if key.find(\\'loss\\') >= 0 and not key.find(\\'val\\') >= 0:  # training loss\\n            plt.plot(data[\\'epoch\\'].values, data[key].values, label=key)\\n    plt.legend()\\n    plt.title(\\'Training loss\\')\\n\\n    fig.add_subplot(212)\\n    for key in data.keys():\\n        if key.find(\\'acc\\') >= 0:  # acc\\n            plt.plot(data[\\'epoch\\'].values, data[key].values, label=key)\\n    plt.legend()\\n    plt.title(\\'Training and validation accuracy\\')\\n\\n    # fig.savefig(\\'result/log.png\\')\\n    if show:\\n        plt.show()\\n\\n\\ndef combine_images(generated_images, height=None, width=None):\\n    num = generated_images.shape[0]\\n    if width is None and height is None:\\n        width = int(math.sqrt(num))\\n        height = int(math.ceil(float(num)/width))\\n    elif width is not None and height is None:  # height not given\\n        height = int(math.ceil(float(num)/width))\\n    elif height is not None and width is None:  # width not given\\n        width = int(math.ceil(float(num)/height))\\n\\n    shape = generated_images.shape[1:3]\\n    image = np.zeros((height*shape[0], width*shape[1]),\\n                     dtype=generated_images.dtype)\\n    for index, img in enumerate(generated_images):\\n        i = int(index/width)\\n        j = index % width\\n        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] =             img[:, :, 0]\\n    return image\\n\\nif __name__==\"__main__\":\\n    plot_log(\\'result_/sub_dependent_/log_fold.csv\\')\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KvwXwu0cNJmH",
        "outputId": "c966077a-ea53-4885-8f0b-18f44725a58a"
      },
      "source": [
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras import initializers, layers, regularizers\n",
        "\n",
        "\n",
        "class Length(layers.Layer):\n",
        "    \"\"\"\n",
        "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
        "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
        "    inputs: shape=[None, num_vectors, dim_vector]\n",
        "    output: shape=[None, num_vectors]\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Length, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "class Mask(layers.Layer):\n",
        "    \"\"\"\n",
        "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
        "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
        "    masked Tensor.\n",
        "    For example:\n",
        "        ```\n",
        "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
        "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
        "        out = Mask()(x)  # out.shape=[8, 6]\n",
        "        # or\n",
        "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
        "        ```\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if isinstance(inputs, list):  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
        "            assert len(inputs) == 2\n",
        "            inputs, mask = inputs\n",
        "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
        "            # compute lengths of capsules\n",
        "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
        "            # generate the mask which is a one-hot code.\n",
        "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
        "            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n",
        "\n",
        "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
        "        # mask.shape=[None, num_capsule]\n",
        "        # masked.shape=[None, num_capsule * dim_capsule]\n",
        "        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
        "        return masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if isinstance(input_shape[0], tuple):  # true label provided\n",
        "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
        "        else:  # no true label provided\n",
        "            return tuple([None, input_shape[1] * input_shape[2]])\n",
        "\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param vectors: some vectors to be squashed, N-dim tensor\n",
        "    :param axis: the axis to squash\n",
        "    :return: a Tensor with same shape as input vectors\n",
        "    \"\"\"\n",
        "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
        "    return scale * vectors\n",
        "\n",
        "\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
        "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
        "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
        "    \n",
        "    :param num_capsule: number of capsules in this layer\n",
        "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
        "    :param routings: number of iterations for the routing algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self, num_capsule, dim_capsule, routings,lam_regularize,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.lam_regularize = lam_regularize\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "\n",
        "        # Transform matrix\n",
        "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
        "                                        self.dim_capsule, self.input_dim_capsule],\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 regularizer=regularizers.l2(self.lam_regularize),\n",
        "                                 name='W')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
        "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
        "        #inputs_expand = K.expand_dims(inputs, 1)   SANDRA\n",
        "        inputs_expand = tf.expand_dims(inputs, 1)\n",
        "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
        "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
        "        #inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1]) SANDRA\n",
        "        inputs_tiled  = tf.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
        "        inputs_tiled  = tf.expand_dims(inputs_tiled, 4)\n",
        "        print(\"inputs_tiled\",np.shape(inputs_tiled))\n",
        "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
        "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
        "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
        "        # Regard the first two dimensions as `batch` dimension,\n",
        "        # then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
        "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
        "        \n",
        "        #inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs_tiled) SANDRA\n",
        "        inputs_hat = tf.map_fn(lambda x: tf.matmul(self.W, x), elems=inputs_tiled)\n",
        "        \n",
        "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
        "        #b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule]) SANDRA\n",
        "        b = tf.zeros(shape=[tf.shape(inputs_hat)[0], self.num_capsule, \n",
        "                      self.input_num_capsule, 1, 1])\n",
        "        \n",
        "        assert self.routings > 0, 'The routings should be > 0.'\n",
        "        for i in range(self.routings):\n",
        "            # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
        "            #c = tf.nn.softmax(b, dim=1)\n",
        "            # c =tf.compat.v1.math.softmax(b, axis = 1) SANDRA\n",
        "            c = layers.Softmax(axis=1)(b)\n",
        "            print(\"c\",np.shape(c))\n",
        "            \n",
        "\n",
        "            # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
        "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "            # The first two dimensions as `batch` dimension,\n",
        "            # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
        "            # outputs.shape=[None, num_capsule, dim_capsule]\n",
        "            #dot_c = K.batch_dot(c, inputs_hat, [2,2]) sandra\n",
        "            #print(\"dot_c\",np.shape(dot_c)) sandra\n",
        "\n",
        "            #outputs = squash(dot_c)  # [None, 10, 16] # sandra\n",
        "            outputs = tf.multiply(c, inputs_hat)\n",
        "            outputs = tf.reduce_sum(outputs, axis=2, keepdims=True)\n",
        "            outputs = squash(outputs, axis=-2)  # [None, 10, 1, 16, 1]\n",
        "      \n",
        "            if i < self.routings - 1:\n",
        "                # outputs.shape =  [None, num_capsule, dim_capsule]\n",
        "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "                # The first two dimensions as `batch` dimension,\n",
        "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
        "                # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
        "                # b += K.batch_dot(outputs, inputs_hat, [2, 3]) SANDRA\n",
        "                outputs_tiled = tf.tile(outputs, [1, 1, self.input_num_capsule, 1, 1])\n",
        "                agreement = tf.matmul(inputs_hat, outputs_tiled, transpose_a=True)\n",
        "                b = tf.add(b, agreement)\n",
        "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
        "        # Squeeze the outputs to remove useless axis:\n",
        "        #  From  --> outputs.shape=[None, num_capsule, 1, dim_capsule, 1]\n",
        "        #  To    --> outputs.shape=[None, num_capsule,    dim_capsule]\n",
        "        outputs = tf.squeeze(outputs, [2, 4])\n",
        "        return outputs\n",
        "    \n",
        "    \"\"\"\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'num_capsule': self.num_capsule,\n",
        "            'dim_capsule': self.dim_capsule,\n",
        "            'routings': self.routings\n",
        "        }\n",
        "        base_config = super(CapsuleLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "    \"\"\"\n",
        "\"\"\"\n",
        "class channel_attention(layers.Layer):\n",
        "\n",
        "    def __init__(self, weight_decay=0.00000004, scope=\"\", reuse=None,**kwargs):  #deap H=120,W=24,C=256; dreamer H=123,W=9,C=256\n",
        "        super(channel_attention, self).__init__(**kwargs)\n",
        "\n",
        "        self.weight_decay = weight_decay\n",
        "        self.scope = scope\n",
        "        self.reuse = reuse\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.H = input_shape[1]\n",
        "        self.W = input_shape[2]\n",
        "        self.C = input_shape[3]\n",
        "        self.w_c = self.add_weight(name=\"w_c\",\n",
        "                                   shape=[self.C, self.C],\n",
        "                                   dtype=tf.float32,\n",
        "                                   initializer=tf.orthogonal_initializer(),\n",
        "                                   regularizer=tf.contrib.layers.l1_regularizer(self.weight_decay))\n",
        "\n",
        "        self.b_c = self.add_weight(name=\"b_c\",\n",
        "                                   shape=[self.C],\n",
        "                                   dtype=tf.float32,\n",
        "                                   initializer=tf.zeros_initializer())\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        feature_map = inputs\n",
        "\n",
        "        transpose_feature_map = tf.transpose(tf.reduce_mean(feature_map, [1, 2], keep_dims=True),\n",
        "                                             perm=[0, 3, 1, 2])\n",
        "        channel_wise_attention_fm = tf.matmul(tf.reshape(transpose_feature_map,\n",
        "                                                         [-1, self.C]), self.w_c) + self.b_c\n",
        "        channel_wise_attention_fm = tf.nn.sigmoid(channel_wise_attention_fm)\n",
        "        #         channel_wise_attention_fm = tf.clip_by_value(tf.nn.relu(channel_wise_attention_fm),\n",
        "        #                                                      clip_value_min = 0,\n",
        "        #                                                      clip_value_max = 1)\n",
        "        attention = tf.reshape(tf.concat([channel_wise_attention_fm] * (self.H * self.W),\n",
        "                                         axis=1), [-1, self.H, self.W, self.C])\n",
        "        attended_fm = attention * feature_map\n",
        "        return attended_fm\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "class spatial_attention(layers.Layer):\n",
        "\n",
        "    def __init__(self, weight_decay=0.4, scope=\"\", reuse=None,**kwargs):\n",
        "        super(spatial_attention, self).__init__(**kwargs)\n",
        "        self.weight_decay = weight_decay\n",
        "        self.scope = scope\n",
        "        self.reuse = reuse\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.H = input_shape[1]\n",
        "        self.W = input_shape[2]\n",
        "        self.C = input_shape[3]\n",
        "        self.w_s = self.add_weight(name=\"w_s\",\n",
        "                                   shape=[self.C, 1],\n",
        "                                   dtype=tf.float32,\n",
        "                                   initializer=tf.orthogonal_initializer(),\n",
        "                                   regularizer=tf.contrib.layers.l1_regularizer(self.weight_decay))\n",
        "\n",
        "        self.b_s = self.add_weight(name=\"b_s\",\n",
        "                                   shape=[1],\n",
        "                                   dtype=tf.float32,\n",
        "                                   initializer=tf.zeros_initializer())\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        feature_map = inputs\n",
        "\n",
        "        spatial_attention_fm = tf.matmul(tf.reshape(feature_map, [-1, self.C]), self.w_s) + self.b_s\n",
        "        spatial_attention_fm = tf.nn.sigmoid(tf.reshape(spatial_attention_fm, [-1, self.W * self.H]))\n",
        "        #         spatial_attention_fm = tf.clip_by_value(tf.nn.relu(tf.reshape(spatial_attention_fm,\n",
        "        #                                                                       [-1, W * H])),\n",
        "        #                                                 clip_value_min = 0,\n",
        "        #                                                 clip_value_max = 1)\n",
        "        attention = tf.reshape(tf.concat([spatial_attention_fm] * self.C, axis=1), [-1, self.H, self.W, self.C])\n",
        "        attended_fm = attention * feature_map\n",
        "        return attended_fm\n",
        "\"\"\"\n",
        "\n",
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding, lam_regularize):\n",
        "    \"\"\"\n",
        "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
        "    :param dim_capsule: the dim of the output vector of capsule\n",
        "    :param n_channels: the number of types of capsules\n",
        "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
        "    \"\"\"\n",
        "    outputs = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
        "                           name='primarycap_conv2d',kernel_regularizer= regularizers.l2(lam_regularize))(inputs)\n",
        "    #if model_version == 'v2':     # MLF-CapsNet\n",
        "    #    outputs = layers.concatenate([inputs,outputs],axis=3,\n",
        "    #                                    name='concatenate')\n",
        "\n",
        "    #   outputs = layers.Conv2D(filters=256, kernel_size=1, strides=1, padding='valid',\n",
        "    #                          name='bottleneck_layer')(outputs)\n",
        "    #elif model_version == 'v1':   # MLF-CapsNet without bottleneck layer\n",
        "    #outputs = layers.concatenate([inputs, outputs], axis=3, name='concatenate')\n",
        "\n",
        "\n",
        "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(outputs)#(conca_maps)#(output)\n",
        "    return layers.Lambda(squash, name='primarycap_squash')(outputs)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# The following is another way to implement primary capsule layer. This is much slower.\n",
        "# Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
        "    outputs = []\n",
        "    for _ in range(n_channels):\n",
        "        output = layers.Conv2D(filters=dim_capsule, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
        "        outputs.append(layers.Reshape([output.get_shape().as_list()[1] ** 2, dim_capsule])(output))\n",
        "    outputs = layers.Concatenate(axis=1)(outputs)\n",
        "    return layers.Lambda(squash)(outputs)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "def conca(inputs):\n",
        "    #[a , b] = inputs\n",
        "    conca_maps = K.concatenate(inputs, axis=3)\n",
        "    return conca_maps\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "def add(inputs):\n",
        "    [a, b] = inputs\n",
        "    out = a + b\n",
        "    return out\n",
        "\"\"\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef add(inputs):\\n    [a, b] = inputs\\n    out = a + b\\n    return out\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQvXFC9GNJmK"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras import layers, models, optimizers,regularizers\n",
        "from keras.layers import InputSpec, Dense\n",
        "#from capsulelayers import CapsuleLayer, PrimaryCap, Length\n",
        "\n",
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "def deap_load(data_file,dimention,debaseline):\n",
        "    rnn_suffix = \".mat_win_128_rnn_dataset.pkl\"\n",
        "    label_suffix = \".mat_win_128_labels.pkl\"\n",
        "    arousal_or_valence = dimention\n",
        "    with_or_without = debaseline # 'yes','not'\n",
        "    dataset_dir = \"/home/bsipl_5/experiment/ijcnn-master/deap_shuffled_data/\" + with_or_without + \"_\" + arousal_or_valence + \"/\"\n",
        "\n",
        "    ###load training set\n",
        "    with open(dataset_dir + data_file + rnn_suffix, \"rb\") as fp:\n",
        "        rnn_datasets = pickle.load(fp)\n",
        "    with open(dataset_dir + data_file + label_suffix, \"rb\") as fp:\n",
        "        labels = pickle.load(fp)\n",
        "        labels = np.transpose(labels)\n",
        "\n",
        "    labels = np.asarray(pd.get_dummies(labels), dtype=np.int8)\n",
        "\n",
        "\n",
        "    # shuffle data\n",
        "    index = np.array(range(0, len(labels)))\n",
        "    np.random.shuffle(index)\n",
        "    rnn_datasets = rnn_datasets[index]  # .transpose(0,2,1)\n",
        "    labels = labels[index]\n",
        "\n",
        "    datasets = rnn_datasets.reshape(-1, 128, 32, 1).astype('float32')\n",
        "    labels = labels.astype('float32')\n",
        "\n",
        "    return datasets , labels\n",
        "\n",
        "def dreamer_load(sub,dimention,debaseline):\n",
        "    if debaseline == 'yes':\n",
        "        dataset_suffix = \"f_dataset.pkl\"\n",
        "        label_suffix = \"_labels.pkl\"\n",
        "        dataset_dir = \"/home/bsipl_5/experiment/Data/data_pre(-base)/\" + dimention + \"/\"\n",
        "    else:\n",
        "        dataset_suffix = \"_rnn_dataset.pkl\"\n",
        "        label_suffix = \"_labels.pkl\"\n",
        "        dataset_dir = '/home/bsipl_5/experiment/ijcnn-master/dreamer_shuffled_data/' + 'no_' + dimention + '/'\n",
        "\n",
        "    ###load training set\n",
        "    with open(dataset_dir + sub + dataset_suffix, \"rb\") as fp:\n",
        "        datasets = pickle.load(fp)\n",
        "    with open(dataset_dir + sub + '_' + dimention + label_suffix, \"rb\") as fp:\n",
        "        labels = pickle.load(fp)\n",
        "        labels = np.transpose(labels)\n",
        "\n",
        "    labels = labels > 3\n",
        "    labels = np.asarray(pd.get_dummies(labels), dtype=np.int8)\n",
        "\n",
        "\n",
        "    # shuffle data\n",
        "    index = np.array(range(0, len(labels)))\n",
        "    np.random.shuffle(index)\n",
        "    datasets = datasets[index]  # .transpose(0,2,1)\n",
        "    labels = labels[index]\n",
        "\n",
        "    datasets = datasets.reshape(-1, 128, 14, 1).astype('float32')\n",
        "    labels = labels.astype('float32')\n",
        "\n",
        "    return datasets , labels\n",
        "\"\"\"\n",
        "\n",
        "class CapsToScalars(layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CapsToScalars, self).__init__(**kwargs)\n",
        "        self.input_spec = InputSpec(min_ndim=3)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return K.sqrt(K.sum(K.square(inputs + K.epsilon()), axis=-1))\n",
        "\n",
        "def CapsNet(input_shape, n_class, routings, lam_regularize):\n",
        "    \"\"\"\n",
        "    A Capsule Network .\n",
        "    :param input_shape: data shape, 3d, [width, height, channels]\n",
        "    :param n_class: number of classes\n",
        "    :param routings: number of routing iterations\n",
        "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
        "            `eval_model` can also be used for training.\n",
        "    \"\"\"\n",
        "    x = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Layer 1: Just a conventional Conv2D layer, 对DEAP，kernel_size=9；对DREAMER，kernel_size=6\n",
        "    conv1 = layers.Conv2D(filters=4, kernel_size=3, strides=1, padding='valid', activation='relu', name='conv1',kernel_regularizer=regularizers.l2(lam_regularize))(x) #kernel_size=9\n",
        "\n",
        "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
        "    # 对DEAP，kernel_size=9；对DREAMER，kernel_size=6\n",
        "    # 对CapsNet，strides=2，pading=‘valid’；对MLF-CapsNet，stides=1，padding='same'\n",
        "    primarycaps = PrimaryCap(conv1, dim_capsule=4, n_channels=128, kernel_size=3, strides=2, padding='valid',lam_regularize = lam_regularize)\n",
        "    \n",
        "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
        "    micaps = CapsuleLayer(num_capsule=n_class, dim_capsule=8, routings=routings,\n",
        "                             name='micaps', lam_regularize = lam_regularize)(primarycaps)\n",
        "\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    out_caps = Length(name='capsnet')(micaps)\n",
        "\n",
        "    # Decoder network.\n",
        "    y = layers.Input(shape=(n_class,))\n",
        "    masked_by_y = Mask()([micaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
        "    masked = Mask()(micaps)  # Mask using the capsule with maximal length. For prediction\n",
        "\n",
        "    # Shared Decoder model in training and prediction\n",
        "    decoder = models.Sequential(name='decoder')\n",
        "    decoder.add(layers.Dense(512, activation='relu', input_dim=8*n_class))\n",
        "    decoder.add(layers.Dense(1024, activation='relu'))\n",
        "    decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
        "    decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
        "\n",
        "    # Models for training and evaluation (prediction)\n",
        "    train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
        "    eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
        "    \n",
        "    return train_model, eval_model\n",
        "    \"\"\"\n",
        "    # manipulate model\n",
        "    \n",
        "    noise = layers.Input(shape=(n_class, 8))\n",
        "    noised_micaps = layers.Add()([micaps, noise])\n",
        "    masked_noised_y = Mask()([noised_micaps, y])\n",
        "    manipulate_model = models.Model([x, y, noise], decoder(masked_noised_y))\n",
        "    \n",
        "    return train_model, eval_model, manipulate_model\n",
        "    \"\"\"\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    \"\"\"\n",
        "    out_caps = Length(name='capsnet')(micaps)\n",
        "\n",
        "    \n",
        "\n",
        "    # Models for training and evaluation (prediction)\n",
        "    y = layers.Input(shape=(n_class,))\n",
        "    train_model = models.Model([x, y], out_caps)\n",
        "    eval_model = models.Model(x, out_caps)\n",
        "    \"\"\"\n",
        "    #l = K.sqrt(K.sum(K.square(micaps + K.epsilon()), axis=-1))\n",
        "    \n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    #masked_by_y = Mask_CID()([digits_caps, y])  \n",
        "    #masked = Mask_CID()(digits_caps)\n",
        "    \n",
        "    \n",
        "    #  Decoder Network\n",
        "    print(\"HOLA\")\n",
        "    #model.add(Flatten())\n",
        "    y = tf.keras.Input(shape=(n_class,))\n",
        "    masked_by_y = Mask()([micaps, y])  \n",
        "    masked = Mask()(micaps)\n",
        "    hidden1 = tf.keras.layers.Dense( 512, activation='relu', name=\"hidden1\") (masked_by_y)\n",
        "    hidden2 = tf.keras.layers.Dense(1024, activation='relu', name=\"hidden2\") (hidden1)\n",
        "    decoder_output = tf.keras.layers.Dense(32 * 5, activation='sigmoid', name=\"decoder_output\") (hidden2)\n",
        "    print(\"HOLAAAAAAAAa\")\n",
        "\n",
        "    \n",
        "\n",
        "    train_model = models.Model([x, y], decoder_output)\n",
        "    eval_model = models.Model(x, decoder_output)\n",
        "    train_model.summary()\n",
        "    \n",
        "    # Decoder Network\n",
        "    decoder = models.Sequential(name='decoder')\n",
        "    decoder.add(Dense(512, activation='relu', name=\"hidden1\", output_dim=1024))\n",
        "    decoder.add(Dense(1024, activation='relu', name=\"hidden2\"))\n",
        "    decoder.add(Dense(2 * 5, activation='sigmoid', name=\"decoder_output\"))\n",
        "    \n",
        "    train_model = models.Model([x, y], [m_capsnet.output, decoder(masked_by_y)])\n",
        "    eval_model = models.Model(x, [m_capsnet.output, decoder(masked)])\n",
        "    train_model.summary()\n",
        "    \n",
        "    return train_model, eval_model\n",
        "    \"\"\"\n",
        "def margin_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
        "    :param y_true: [None, n_classes]\n",
        "    :param y_pred: [None, num_capsule]\n",
        "    :return: a scalar loss value.\n",
        "    \"\"\"\n",
        "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
        "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
        "\n",
        "    return K.mean(K.sum(L, 1))\n",
        "\n",
        "def train(model, data, args):\n",
        "    \"\"\"\n",
        "    Training a CapsuleNet\n",
        "    :param model: the CapsuleNet model\n",
        "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
        "    :param args: arguments\n",
        "    :return: The trained model\n",
        "    \"\"\"\n",
        "    # unpacking the data\n",
        "    (X_train, y_train), (X_dev, y_dev) = data\n",
        "\n",
        "    # callbacks\n",
        "    log = callbacks.CSVLogger(args.save_dir + '/' + 'log_fold.csv')\n",
        "    tb = callbacks.TensorBoard(log_dir=args.save_dir + '/tensorboard-logs_fold',\n",
        "                               batch_size=args.batch_size, histogram_freq=args.debug)\n",
        "    checkpoint = callbacks.ModelCheckpoint(args.save_dir + '/weights-{epoch:02d}_fold.h5', monitor='val_acc',\n",
        "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
        "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: args.lr * (1.0 ** epoch))\n",
        "\n",
        "    #EarlyStop = callbacks.EarlyStopping(monitor='val_capsnet_acc', patience=5)\n",
        "    # compile the model\n",
        "    model.compile(optimizer= optimizers.Adam(lr=args.lr),\n",
        "                  loss=margin_loss,\n",
        "                  loss_weights=[1., args.lam_recon],\n",
        "                  metrics={'capsnet': 'accuracy'})\n",
        "\n",
        "    \"\"\"\n",
        "    # Training without data augmentation:\n",
        "    model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs,\n",
        "              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint, lr_decay, EarlyStop])\n",
        "    \"\"\"\n",
        "\n",
        "    '''\n",
        "    # Training with validation set\n",
        "    model.fit([x_train, y_train], y_train ,  batch_size=args.batch_size, epochs=args.epochs,verbose = 1,\n",
        "              validation_split= 0.1 , callbacks=[log, tb, checkpoint, lr_decay])\n",
        "    '''\n",
        "\n",
        "    # Training without validation set\n",
        "    history = model.fit([X_train, y_train],[y_train, X_train], batch_size=args.batch_size, epochs=args.epochs,\n",
        "                validation_data=([X_dev, y_dev], [y_dev, X_dev]), callbacks=[log, tb, lr_decay])\n",
        "\n",
        "\n",
        "    #from utils import plot_log\n",
        "    #plot_log(args.save_dir + '/log.csv', show=True)\n",
        "\n",
        "    return history\n",
        "\n",
        "time_start_whole = time.time()\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_5bwR72aNJmL",
        "outputId": "1fbce830-33f6-4312-e18f-90cede4d870f"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from keras import callbacks\n",
        "from keras.utils.vis_utils import plot_model\n",
        "#from keras.utils import multi_gpu_model\n",
        "\n",
        "# setting the hyper parameters\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"Capsule Network on \" + folder_day)\n",
        "parser.add_argument('--epochs', default=100, type=int)  \n",
        "parser.add_argument('--batch_size', default=20, type=int)\n",
        "parser.add_argument('--lam_regularize', default=0.0, type=float,\n",
        "                    help=\"The coefficient for the regularizers\")\n",
        "parser.add_argument('--lam_recon', default=0.392, type=float,\n",
        "                        help=\"The coefficient for the loss of decoder\")\n",
        "parser.add_argument('-r', '--routings', default=2, type=int,\n",
        "                    help=\"Number of iterations used in routing algorithm. should > 0\")\n",
        "parser.add_argument('--debug', default=0, type=int,\n",
        "                    help=\"Save weights by TensorBoard\")\n",
        "parser.add_argument('--save_dir', default='./result_/sub_dependent_/') # other\n",
        "parser.add_argument('-t', '--testing', action='store_true',\n",
        "                    help=\"Test the trained model on testing dataset\")\n",
        "parser.add_argument('-w', '--weights', default=None,\n",
        "                    help=\"The path of the saved weights. Should be specified when testing\")\n",
        "parser.add_argument('--lr', default=0.00001, type=float,\n",
        "                    help=\"Initial learning rate\")  # v0:0.0001, v2:0.00001\n",
        "\n",
        "parser.add_argument('--gpus', default=2, type=int)\n",
        "\n",
        "#args = parser.parse_args()\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "\n",
        "print(time.asctime(time.localtime(time.time())))\n",
        "print(args)\n",
        "if not os.path.exists(args.save_dir):\n",
        "    os.makedirs(args.save_dir)\n",
        "\n",
        "#if dataset_name == 'dreamer':          # load dreamer data\n",
        "#    datasets,labels = dreamer_load(subject,dimention,debaseline)\n",
        "#else:  # load deap data\n",
        "#    datasets,labels = deap_load(subject,dimention,debaseline)\n",
        "\n",
        "args.save_dir = args.save_dir\n",
        "if not os.path.exists(args.save_dir):\n",
        "    os.makedirs(args.save_dir)\n",
        "\n",
        "#fold = 10\n",
        "#test_accuracy_allfold = np.zeros(shape=[0], dtype=float)\n",
        "#train_used_time_allfold = np.zeros(shape=[0], dtype=float)\n",
        "#test_used_time_allfold = np.zeros(shape=[0], dtype=float)\n",
        "#for curr_fold in range(fold):\n",
        "#fold_size = datasets.shape[0] // fold\n",
        "#indexes_list = [i for i in range(len(datasets))]\n",
        "#indexes = np.array(indexes_list)\n",
        "#split_list = [i for i in range(curr_fold * fold_size, (curr_fold + 1) * fold_size)]\n",
        "#split = np.array(split_list)\n",
        "#x_test = datasets[split]\n",
        "#y_test = labels[split]\n",
        "\n",
        "#split = np.array(list(set(indexes_list) ^ set(split_list)))\n",
        "#x_train = datasets[split]\n",
        "#y_train = labels[split]\n",
        "\n",
        "#train_sample = y_train.shape[0]\n",
        "#print(\"training examples:\", train_sample)\n",
        "#test_sample = y_test.shape[0]\n",
        "#print(\"test examples    :\", test_sample)\n",
        "\n",
        "# define model\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "    model, eval_model = CapsNet(input_shape=X_train.shape[1:],\n",
        "                                                  n_class=len(np.unique(y_train)),\n",
        "                                                  routings=args.routings,\n",
        "                                                  lam_regularize = args.lam_regularize)\n",
        "\n",
        "model.summary()\n",
        "plot_model(model, to_file=args.save_dir+'/model_fold.png', show_shapes=True)\n",
        "\n",
        "\n",
        "# train\n",
        "train_start_time = time.time()\n",
        "history = train(model, data=((X_train, y_train), (X_dev, y_dev)), args=args)\n",
        "train_used_time = time.time() - train_start_time\n",
        "\n",
        "results=pd.DataFrame(history.history)\n",
        "results.plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.xlabel (\"Epochs\")\n",
        "plt.ylabel (\"Accuracy - Mean Log Loss\")\n",
        "plt.gca().set_ylim(0, 2) # set the vertical range to [0-1]\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jun 30 18:58:56 2021\n",
            "Namespace(batch_size=20, debug=0, epochs=100, gpus=2, lam_recon=0.392, lam_regularize=0.0, lr=1e-05, routings=2, save_dir='./result_/sub_dependent_/', testing=False, weights=None)\n",
            "inputs_tiled (None, 2, 1792, 4, 1)\n",
            "c (None, 2, 1792, 1, 1)\n",
            "c (None, 2, 1792, 1, 1)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 5, 1)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 30, 3, 4)     40          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "primarycap_conv2d (Conv2D)      (None, 14, 1, 512)   18944       conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "primarycap_reshape (Reshape)    (None, 1792, 4)      0           primarycap_conv2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "primarycap_squash (Lambda)      (None, 1792, 4)      0           primarycap_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "micaps (CapsuleLayer)           (None, 2, 8)         114688      primarycap_squash[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "mask (Mask)                     (None, None)         0           micaps[0][0]                     \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "capsnet (Length)                (None, 2)            0           micaps[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Sequential)            (None, 32, 5, 1)     698016      mask[0][0]                       \n",
            "==================================================================================================\n",
            "Total params: 831,688\n",
            "Trainable params: 831,688\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer CapsuleLayer has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 1/100\n",
            "inputs_tiled (None, 2, 1792, 4, 1)\n",
            "c (None, 2, 1792, 1, 1)\n",
            "c (None, 2, 1792, 1, 1)\n",
            "inputs_tiled (None, 2, 1792, 4, 1)\n",
            "c (None, 2, 1792, 1, 1)\n",
            "c (None, 2, 1792, 1, 1)\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.8328 - capsnet_loss: 0.8094 - decoder_loss: 0.0596 - capsnet_accuracy: 0.6030inputs_tiled (None, 2, 1792, 4, 1)\n",
            "c (None, 2, 1792, 1, 1)\n",
            "c (None, 2, 1792, 1, 1)\n",
            "6/6 [==============================] - 15s 408ms/step - loss: 0.8329 - capsnet_loss: 0.8094 - decoder_loss: 0.0599 - capsnet_accuracy: 0.6034 - val_loss: 0.8325 - val_capsnet_loss: 0.8094 - val_decoder_loss: 0.0588 - val_capsnet_accuracy: 0.5769\n",
            "Epoch 2/100\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.8333 - capsnet_loss: 0.8094 - decoder_loss: 0.0610 - capsnet_accuracy: 0.6163 - val_loss: 0.8317 - val_capsnet_loss: 0.8094 - val_decoder_loss: 0.0567 - val_capsnet_accuracy: 0.5769\n",
            "Epoch 3/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.8324 - capsnet_loss: 0.8094 - decoder_loss: 0.0587 - capsnet_accuracy: 0.6328 - val_loss: 0.8308 - val_capsnet_loss: 0.8094 - val_decoder_loss: 0.0545 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 4/100\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.8320 - capsnet_loss: 0.8094 - decoder_loss: 0.0577 - capsnet_accuracy: 0.6090 - val_loss: 0.8299 - val_capsnet_loss: 0.8094 - val_decoder_loss: 0.0523 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 5/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.8315 - capsnet_loss: 0.8094 - decoder_loss: 0.0562 - capsnet_accuracy: 0.6064 - val_loss: 0.8290 - val_capsnet_loss: 0.8094 - val_decoder_loss: 0.0499 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 6/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.8291 - capsnet_loss: 0.8094 - decoder_loss: 0.0503 - capsnet_accuracy: 0.6249 - val_loss: 0.8280 - val_capsnet_loss: 0.8094 - val_decoder_loss: 0.0474 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 7/100\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.8279 - capsnet_loss: 0.8094 - decoder_loss: 0.0472 - capsnet_accuracy: 0.6245 - val_loss: 0.8269 - val_capsnet_loss: 0.8094 - val_decoder_loss: 0.0447 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 8/100\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.8260 - capsnet_loss: 0.8094 - decoder_loss: 0.0426 - capsnet_accuracy: 0.6117 - val_loss: 0.8257 - val_capsnet_loss: 0.8093 - val_decoder_loss: 0.0419 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 9/100\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.8280 - capsnet_loss: 0.8093 - decoder_loss: 0.0478 - capsnet_accuracy: 0.6284 - val_loss: 0.8245 - val_capsnet_loss: 0.8092 - val_decoder_loss: 0.0389 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 10/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.8244 - capsnet_loss: 0.8092 - decoder_loss: 0.0390 - capsnet_accuracy: 0.6296 - val_loss: 0.8230 - val_capsnet_loss: 0.8090 - val_decoder_loss: 0.0357 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 11/100\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.8211 - capsnet_loss: 0.8089 - decoder_loss: 0.0310 - capsnet_accuracy: 0.6652 - val_loss: 0.8215 - val_capsnet_loss: 0.8088 - val_decoder_loss: 0.0323 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 12/100\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.8228 - capsnet_loss: 0.8087 - decoder_loss: 0.0359 - capsnet_accuracy: 0.6581 - val_loss: 0.8197 - val_capsnet_loss: 0.8085 - val_decoder_loss: 0.0286 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 13/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.8189 - capsnet_loss: 0.8084 - decoder_loss: 0.0268 - capsnet_accuracy: 0.6219 - val_loss: 0.8177 - val_capsnet_loss: 0.8080 - val_decoder_loss: 0.0247 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 14/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.8188 - capsnet_loss: 0.8079 - decoder_loss: 0.0278 - capsnet_accuracy: 0.6390 - val_loss: 0.8154 - val_capsnet_loss: 0.8074 - val_decoder_loss: 0.0205 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 15/100\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.8173 - capsnet_loss: 0.8073 - decoder_loss: 0.0256 - capsnet_accuracy: 0.5924 - val_loss: 0.8128 - val_capsnet_loss: 0.8066 - val_decoder_loss: 0.0159 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 16/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.8131 - capsnet_loss: 0.8064 - decoder_loss: 0.0170 - capsnet_accuracy: 0.5868 - val_loss: 0.8099 - val_capsnet_loss: 0.8055 - val_decoder_loss: 0.0110 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 17/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.8110 - capsnet_loss: 0.8053 - decoder_loss: 0.0146 - capsnet_accuracy: 0.5871 - val_loss: 0.8064 - val_capsnet_loss: 0.8042 - val_decoder_loss: 0.0058 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 18/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.8065 - capsnet_loss: 0.8038 - decoder_loss: 0.0069 - capsnet_accuracy: 0.6055 - val_loss: 0.8025 - val_capsnet_loss: 0.8024 - val_decoder_loss: 1.0121e-04 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 19/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.8019 - capsnet_loss: 0.8018 - decoder_loss: 1.4964e-04 - capsnet_accuracy: 0.6177 - val_loss: 0.7979 - val_capsnet_loss: 0.8003 - val_decoder_loss: -0.0060 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 20/100\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.7975 - capsnet_loss: 0.7998 - decoder_loss: -0.0059 - capsnet_accuracy: 0.5963 - val_loss: 0.7927 - val_capsnet_loss: 0.7976 - val_decoder_loss: -0.0127 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 21/100\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.7900 - capsnet_loss: 0.7965 - decoder_loss: -0.0166 - capsnet_accuracy: 0.6243 - val_loss: 0.7866 - val_capsnet_loss: 0.7944 - val_decoder_loss: -0.0199 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 22/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.7857 - capsnet_loss: 0.7934 - decoder_loss: -0.0196 - capsnet_accuracy: 0.6186 - val_loss: 0.7795 - val_capsnet_loss: 0.7904 - val_decoder_loss: -0.0278 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 23/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.7775 - capsnet_loss: 0.7889 - decoder_loss: -0.0290 - capsnet_accuracy: 0.6420 - val_loss: 0.7713 - val_capsnet_loss: 0.7856 - val_decoder_loss: -0.0366 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 24/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.7689 - capsnet_loss: 0.7838 - decoder_loss: -0.0380 - capsnet_accuracy: 0.6408 - val_loss: 0.7618 - val_capsnet_loss: 0.7799 - val_decoder_loss: -0.0463 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 25/100\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.7601 - capsnet_loss: 0.7786 - decoder_loss: -0.0471 - capsnet_accuracy: 0.6073 - val_loss: 0.7507 - val_capsnet_loss: 0.7732 - val_decoder_loss: -0.0572 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 26/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.7477 - capsnet_loss: 0.7713 - decoder_loss: -0.0602 - capsnet_accuracy: 0.6121 - val_loss: 0.7378 - val_capsnet_loss: 0.7651 - val_decoder_loss: -0.0696 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 27/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.7354 - capsnet_loss: 0.7630 - decoder_loss: -0.0705 - capsnet_accuracy: 0.6067 - val_loss: 0.7227 - val_capsnet_loss: 0.7556 - val_decoder_loss: -0.0839 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 28/100\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.7207 - capsnet_loss: 0.7535 - decoder_loss: -0.0837 - capsnet_accuracy: 0.6144 - val_loss: 0.7051 - val_capsnet_loss: 0.7445 - val_decoder_loss: -0.1004 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 29/100\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.7021 - capsnet_loss: 0.7421 - decoder_loss: -0.1021 - capsnet_accuracy: 0.6174 - val_loss: 0.6846 - val_capsnet_loss: 0.7316 - val_decoder_loss: -0.1199 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 30/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.6815 - capsnet_loss: 0.7289 - decoder_loss: -0.1211 - capsnet_accuracy: 0.6200 - val_loss: 0.6607 - val_capsnet_loss: 0.7167 - val_decoder_loss: -0.1428 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 31/100\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.6574 - capsnet_loss: 0.7139 - decoder_loss: -0.1442 - capsnet_accuracy: 0.6130 - val_loss: 0.6333 - val_capsnet_loss: 0.6999 - val_decoder_loss: -0.1699 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 32/100\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.6303 - capsnet_loss: 0.6962 - decoder_loss: -0.1682 - capsnet_accuracy: 0.6252 - val_loss: 0.6015 - val_capsnet_loss: 0.6807 - val_decoder_loss: -0.2021 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 33/100\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.5968 - capsnet_loss: 0.6773 - decoder_loss: -0.2052 - capsnet_accuracy: 0.5993 - val_loss: 0.5651 - val_capsnet_loss: 0.6593 - val_decoder_loss: -0.2403 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 34/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.5597 - capsnet_loss: 0.6558 - decoder_loss: -0.2452 - capsnet_accuracy: 0.5975 - val_loss: 0.5237 - val_capsnet_loss: 0.6357 - val_decoder_loss: -0.2858 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 35/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.5191 - capsnet_loss: 0.6313 - decoder_loss: -0.2862 - capsnet_accuracy: 0.6287 - val_loss: 0.4769 - val_capsnet_loss: 0.6099 - val_decoder_loss: -0.3394 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 36/100\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.4610 - capsnet_loss: 0.6004 - decoder_loss: -0.3555 - capsnet_accuracy: 0.6380 - val_loss: 0.4247 - val_capsnet_loss: 0.5824 - val_decoder_loss: -0.4024 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 37/100\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.4208 - capsnet_loss: 0.5794 - decoder_loss: -0.4046 - capsnet_accuracy: 0.6064 - val_loss: 0.3671 - val_capsnet_loss: 0.5536 - val_decoder_loss: -0.4757 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 38/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3515 - capsnet_loss: 0.5468 - decoder_loss: -0.4982 - capsnet_accuracy: 0.6159 - val_loss: 0.3040 - val_capsnet_loss: 0.5238 - val_decoder_loss: -0.5607 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 39/100\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.2838 - capsnet_loss: 0.5149 - decoder_loss: -0.5894 - capsnet_accuracy: 0.6308 - val_loss: 0.2357 - val_capsnet_loss: 0.4936 - val_decoder_loss: -0.6580 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 40/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.2346 - capsnet_loss: 0.4939 - decoder_loss: -0.6614 - capsnet_accuracy: 0.5646 - val_loss: 0.1627 - val_capsnet_loss: 0.4636 - val_decoder_loss: -0.7676 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 41/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1541 - capsnet_loss: 0.4607 - decoder_loss: -0.7823 - capsnet_accuracy: 0.5834 - val_loss: 0.0853 - val_capsnet_loss: 0.4343 - val_decoder_loss: -0.8903 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 42/100\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0683 - capsnet_loss: 0.4284 - decoder_loss: -0.9187 - capsnet_accuracy: 0.6144 - val_loss: 0.0032 - val_capsnet_loss: 0.4059 - val_decoder_loss: -1.0274 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 43/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: -0.0021 - capsnet_loss: 0.4051 - decoder_loss: -1.0388 - capsnet_accuracy: 0.5653 - val_loss: -0.0833 - val_capsnet_loss: 0.3790 - val_decoder_loss: -1.1792 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 44/100\n",
            "6/6 [==============================] - 0s 36ms/step - loss: -0.0938 - capsnet_loss: 0.3758 - decoder_loss: -1.1980 - capsnet_accuracy: 0.6332 - val_loss: -0.1741 - val_capsnet_loss: 0.3537 - val_decoder_loss: -1.3464 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 45/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -0.1906 - capsnet_loss: 0.3495 - decoder_loss: -1.3777 - capsnet_accuracy: 0.6659 - val_loss: -0.2688 - val_capsnet_loss: 0.3304 - val_decoder_loss: -1.5286 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 46/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: -0.2895 - capsnet_loss: 0.3257 - decoder_loss: -1.5692 - capsnet_accuracy: 0.6290 - val_loss: -0.3670 - val_capsnet_loss: 0.3093 - val_decoder_loss: -1.7253 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 47/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: -0.3950 - capsnet_loss: 0.3041 - decoder_loss: -1.7835 - capsnet_accuracy: 0.6396 - val_loss: -0.4686 - val_capsnet_loss: 0.2905 - val_decoder_loss: -1.9364 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 48/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: -0.4809 - capsnet_loss: 0.2887 - decoder_loss: -1.9632 - capsnet_accuracy: 0.6227 - val_loss: -0.5726 - val_capsnet_loss: 0.2740 - val_decoder_loss: -2.1599 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 49/100\n",
            "6/6 [==============================] - 0s 45ms/step - loss: -0.5857 - capsnet_loss: 0.2735 - decoder_loss: -2.1918 - capsnet_accuracy: 0.6019 - val_loss: -0.6798 - val_capsnet_loss: 0.2598 - val_decoder_loss: -2.3971 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 50/100\n",
            "6/6 [==============================] - 0s 35ms/step - loss: -0.6931 - capsnet_loss: 0.2601 - decoder_loss: -2.4316 - capsnet_accuracy: 0.5957 - val_loss: -0.7903 - val_capsnet_loss: 0.2478 - val_decoder_loss: -2.6482 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 51/100\n",
            "6/6 [==============================] - 0s 43ms/step - loss: -0.8020 - capsnet_loss: 0.2490 - decoder_loss: -2.6812 - capsnet_accuracy: 0.5828 - val_loss: -0.9037 - val_capsnet_loss: 0.2377 - val_decoder_loss: -2.9119 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 52/100\n",
            "6/6 [==============================] - 0s 37ms/step - loss: -0.9195 - capsnet_loss: 0.2367 - decoder_loss: -2.9495 - capsnet_accuracy: 0.6382 - val_loss: -1.0203 - val_capsnet_loss: 0.2295 - val_decoder_loss: -3.1884 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 53/100\n",
            "6/6 [==============================] - 0s 38ms/step - loss: -1.0663 - capsnet_loss: 0.2259 - decoder_loss: -3.2964 - capsnet_accuracy: 0.6624 - val_loss: -1.1396 - val_capsnet_loss: 0.2231 - val_decoder_loss: -3.4761 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 54/100\n",
            "6/6 [==============================] - 0s 35ms/step - loss: -1.1541 - capsnet_loss: 0.2228 - decoder_loss: -3.5125 - capsnet_accuracy: 0.6132 - val_loss: -1.2614 - val_capsnet_loss: 0.2181 - val_decoder_loss: -3.7743 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 55/100\n",
            "6/6 [==============================] - 0s 35ms/step - loss: -1.2942 - capsnet_loss: 0.2165 - decoder_loss: -3.8537 - capsnet_accuracy: 0.6386 - val_loss: -1.3860 - val_capsnet_loss: 0.2145 - val_decoder_loss: -4.0828 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 56/100\n",
            "6/6 [==============================] - 0s 35ms/step - loss: -1.3978 - capsnet_loss: 0.2154 - decoder_loss: -4.1154 - capsnet_accuracy: 0.6005 - val_loss: -1.5130 - val_capsnet_loss: 0.2121 - val_decoder_loss: -4.4009 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 57/100\n",
            "6/6 [==============================] - 0s 41ms/step - loss: -1.5322 - capsnet_loss: 0.2116 - decoder_loss: -4.4486 - capsnet_accuracy: 0.6328 - val_loss: -1.6425 - val_capsnet_loss: 0.2108 - val_decoder_loss: -4.7277 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 58/100\n",
            "6/6 [==============================] - 0s 36ms/step - loss: -1.6543 - capsnet_loss: 0.2136 - decoder_loss: -4.7650 - capsnet_accuracy: 0.5627 - val_loss: -1.7740 - val_capsnet_loss: 0.2104 - val_decoder_loss: -5.0623 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 59/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -1.7743 - capsnet_loss: 0.2118 - decoder_loss: -5.0667 - capsnet_accuracy: 0.5892 - val_loss: -1.9077 - val_capsnet_loss: 0.2107 - val_decoder_loss: -5.4042 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 60/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -1.9078 - capsnet_loss: 0.2128 - decoder_loss: -5.4098 - capsnet_accuracy: 0.5639 - val_loss: -2.0432 - val_capsnet_loss: 0.2117 - val_decoder_loss: -5.7524 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 61/100\n",
            "6/6 [==============================] - 0s 32ms/step - loss: -2.0541 - capsnet_loss: 0.2137 - decoder_loss: -5.7851 - capsnet_accuracy: 0.5856 - val_loss: -2.1803 - val_capsnet_loss: 0.2133 - val_decoder_loss: -6.1059 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 62/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: -2.2246 - capsnet_loss: 0.2128 - decoder_loss: -6.2179 - capsnet_accuracy: 0.6573 - val_loss: -2.3188 - val_capsnet_loss: 0.2153 - val_decoder_loss: -6.4646 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 63/100\n",
            "6/6 [==============================] - 0s 36ms/step - loss: -2.3485 - capsnet_loss: 0.2155 - decoder_loss: -6.5408 - capsnet_accuracy: 0.6189 - val_loss: -2.4588 - val_capsnet_loss: 0.2178 - val_decoder_loss: -6.8280 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 64/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -2.4987 - capsnet_loss: 0.2174 - decoder_loss: -6.9288 - capsnet_accuracy: 0.6473 - val_loss: -2.5999 - val_capsnet_loss: 0.2206 - val_decoder_loss: -7.1952 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 65/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: -2.6076 - capsnet_loss: 0.2227 - decoder_loss: -7.2201 - capsnet_accuracy: 0.5767 - val_loss: -2.7418 - val_capsnet_loss: 0.2236 - val_decoder_loss: -7.5648 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 66/100\n",
            "6/6 [==============================] - 0s 36ms/step - loss: -2.7962 - capsnet_loss: 0.2242 - decoder_loss: -7.7052 - capsnet_accuracy: 0.6474 - val_loss: -2.8844 - val_capsnet_loss: 0.2269 - val_decoder_loss: -7.9369 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 67/100\n",
            "6/6 [==============================] - 0s 37ms/step - loss: -2.9242 - capsnet_loss: 0.2258 - decoder_loss: -8.0357 - capsnet_accuracy: 0.6614 - val_loss: -3.0274 - val_capsnet_loss: 0.2304 - val_decoder_loss: -8.3108 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 68/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -3.0528 - capsnet_loss: 0.2315 - decoder_loss: -8.3782 - capsnet_accuracy: 0.6096 - val_loss: -3.1709 - val_capsnet_loss: 0.2341 - val_decoder_loss: -8.6863 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 69/100\n",
            "6/6 [==============================] - 0s 35ms/step - loss: -3.2055 - capsnet_loss: 0.2333 - decoder_loss: -8.7726 - capsnet_accuracy: 0.6496 - val_loss: -3.3148 - val_capsnet_loss: 0.2378 - val_decoder_loss: -9.0627 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 70/100\n",
            "6/6 [==============================] - 0s 41ms/step - loss: -3.3372 - capsnet_loss: 0.2380 - decoder_loss: -9.1204 - capsnet_accuracy: 0.6182 - val_loss: -3.4584 - val_capsnet_loss: 0.2414 - val_decoder_loss: -9.4383 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 71/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: -3.4696 - capsnet_loss: 0.2408 - decoder_loss: -9.4651 - capsnet_accuracy: 0.6120 - val_loss: -3.6015 - val_capsnet_loss: 0.2452 - val_decoder_loss: -9.8130 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 72/100\n",
            "6/6 [==============================] - 0s 37ms/step - loss: -3.6243 - capsnet_loss: 0.2463 - decoder_loss: -9.8739 - capsnet_accuracy: 0.6046 - val_loss: -3.7442 - val_capsnet_loss: 0.2489 - val_decoder_loss: -10.1865 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 73/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -3.7462 - capsnet_loss: 0.2488 - decoder_loss: -10.1912 - capsnet_accuracy: 0.5953 - val_loss: -3.8863 - val_capsnet_loss: 0.2526 - val_decoder_loss: -10.5582 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 74/100\n",
            "6/6 [==============================] - 0s 40ms/step - loss: -3.9150 - capsnet_loss: 0.2529 - decoder_loss: -10.6324 - capsnet_accuracy: 0.6194 - val_loss: -4.0273 - val_capsnet_loss: 0.2562 - val_decoder_loss: -10.9272 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 75/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -4.0541 - capsnet_loss: 0.2567 - decoder_loss: -10.9968 - capsnet_accuracy: 0.6126 - val_loss: -4.1669 - val_capsnet_loss: 0.2598 - val_decoder_loss: -11.2927 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 76/100\n",
            "6/6 [==============================] - 0s 41ms/step - loss: -4.1689 - capsnet_loss: 0.2608 - decoder_loss: -11.3002 - capsnet_accuracy: 0.5669 - val_loss: -4.3052 - val_capsnet_loss: 0.2634 - val_decoder_loss: -11.6545 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 77/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -4.3337 - capsnet_loss: 0.2621 - decoder_loss: -11.7239 - capsnet_accuracy: 0.6412 - val_loss: -4.4420 - val_capsnet_loss: 0.2667 - val_decoder_loss: -12.0119 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 78/100\n",
            "6/6 [==============================] - 0s 35ms/step - loss: -4.4388 - capsnet_loss: 0.2658 - decoder_loss: -12.0015 - capsnet_accuracy: 0.5989 - val_loss: -4.5772 - val_capsnet_loss: 0.2700 - val_decoder_loss: -12.3652 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 79/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -4.5915 - capsnet_loss: 0.2688 - decoder_loss: -12.3988 - capsnet_accuracy: 0.6307 - val_loss: -4.7106 - val_capsnet_loss: 0.2733 - val_decoder_loss: -12.7139 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 80/100\n",
            "6/6 [==============================] - 0s 41ms/step - loss: -4.7305 - capsnet_loss: 0.2736 - decoder_loss: -12.7658 - capsnet_accuracy: 0.6233 - val_loss: -4.8419 - val_capsnet_loss: 0.2764 - val_decoder_loss: -13.0571 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 81/100\n",
            "6/6 [==============================] - 0s 43ms/step - loss: -4.8602 - capsnet_loss: 0.2754 - decoder_loss: -13.1009 - capsnet_accuracy: 0.6462 - val_loss: -4.9713 - val_capsnet_loss: 0.2795 - val_decoder_loss: -13.3949 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 82/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -4.9936 - capsnet_loss: 0.2791 - decoder_loss: -13.4507 - capsnet_accuracy: 0.6281 - val_loss: -5.0985 - val_capsnet_loss: 0.2825 - val_decoder_loss: -13.7270 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 83/100\n",
            "6/6 [==============================] - 0s 35ms/step - loss: -5.1298 - capsnet_loss: 0.2823 - decoder_loss: -13.8065 - capsnet_accuracy: 0.6456 - val_loss: -5.2234 - val_capsnet_loss: 0.2854 - val_decoder_loss: -14.0529 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 84/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -5.2359 - capsnet_loss: 0.2839 - decoder_loss: -14.0811 - capsnet_accuracy: 0.6477 - val_loss: -5.3458 - val_capsnet_loss: 0.2881 - val_decoder_loss: -14.3722 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 85/100\n",
            "6/6 [==============================] - 0s 38ms/step - loss: -5.3647 - capsnet_loss: 0.2870 - decoder_loss: -14.4175 - capsnet_accuracy: 0.6361 - val_loss: -5.4657 - val_capsnet_loss: 0.2908 - val_decoder_loss: -14.6849 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 86/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: -5.5111 - capsnet_loss: 0.2904 - decoder_loss: -14.7997 - capsnet_accuracy: 0.6695 - val_loss: -5.5828 - val_capsnet_loss: 0.2934 - val_decoder_loss: -14.9901 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 87/100\n",
            "6/6 [==============================] - 0s 41ms/step - loss: -5.6136 - capsnet_loss: 0.2935 - decoder_loss: -15.0691 - capsnet_accuracy: 0.6118 - val_loss: -5.6972 - val_capsnet_loss: 0.2958 - val_decoder_loss: -15.2883 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 88/100\n",
            "6/6 [==============================] - 0s 35ms/step - loss: -5.7249 - capsnet_loss: 0.2957 - decoder_loss: -15.3588 - capsnet_accuracy: 0.6284 - val_loss: -5.8089 - val_capsnet_loss: 0.2981 - val_decoder_loss: -15.5792 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 89/100\n",
            "6/6 [==============================] - 0s 41ms/step - loss: -5.8482 - capsnet_loss: 0.2988 - decoder_loss: -15.6812 - capsnet_accuracy: 0.6459 - val_loss: -5.9177 - val_capsnet_loss: 0.3003 - val_decoder_loss: -15.8624 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 90/100\n",
            "6/6 [==============================] - 0s 35ms/step - loss: -5.9337 - capsnet_loss: 0.2999 - decoder_loss: -15.9022 - capsnet_accuracy: 0.6108 - val_loss: -6.0239 - val_capsnet_loss: 0.3025 - val_decoder_loss: -16.1388 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 91/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -6.0480 - capsnet_loss: 0.3028 - decoder_loss: -16.2009 - capsnet_accuracy: 0.6133 - val_loss: -6.1271 - val_capsnet_loss: 0.3046 - val_decoder_loss: -16.4074 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 92/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -6.1592 - capsnet_loss: 0.3039 - decoder_loss: -16.4874 - capsnet_accuracy: 0.6459 - val_loss: -6.2273 - val_capsnet_loss: 0.3066 - val_decoder_loss: -16.6681 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 93/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -6.2485 - capsnet_loss: 0.3069 - decoder_loss: -16.7228 - capsnet_accuracy: 0.5969 - val_loss: -6.3248 - val_capsnet_loss: 0.3085 - val_decoder_loss: -16.9216 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 94/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -6.3395 - capsnet_loss: 0.3083 - decoder_loss: -16.9586 - capsnet_accuracy: 0.6273 - val_loss: -6.4194 - val_capsnet_loss: 0.3103 - val_decoder_loss: -17.1675 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 95/100\n",
            "6/6 [==============================] - 0s 43ms/step - loss: -6.4161 - capsnet_loss: 0.3101 - decoder_loss: -17.1586 - capsnet_accuracy: 0.5961 - val_loss: -6.5113 - val_capsnet_loss: 0.3120 - val_decoder_loss: -17.4065 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 96/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -6.5250 - capsnet_loss: 0.3117 - decoder_loss: -17.4407 - capsnet_accuracy: 0.6263 - val_loss: -6.6004 - val_capsnet_loss: 0.3137 - val_decoder_loss: -17.6380 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 97/100\n",
            "6/6 [==============================] - 0s 34ms/step - loss: -6.6186 - capsnet_loss: 0.3134 - decoder_loss: -17.6838 - capsnet_accuracy: 0.6246 - val_loss: -6.6870 - val_capsnet_loss: 0.3153 - val_decoder_loss: -17.8629 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 98/100\n",
            "6/6 [==============================] - 0s 41ms/step - loss: -6.6804 - capsnet_loss: 0.3141 - decoder_loss: -17.8431 - capsnet_accuracy: 0.6132 - val_loss: -6.7707 - val_capsnet_loss: 0.3168 - val_decoder_loss: -18.0804 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 99/100\n",
            "6/6 [==============================] - 0s 43ms/step - loss: -6.7809 - capsnet_loss: 0.3161 - decoder_loss: -18.1044 - capsnet_accuracy: 0.6118 - val_loss: -6.8520 - val_capsnet_loss: 0.3183 - val_decoder_loss: -18.2914 - val_capsnet_accuracy: 0.6154\n",
            "Epoch 100/100\n",
            "6/6 [==============================] - 0s 42ms/step - loss: -6.8600 - capsnet_loss: 0.3174 - decoder_loss: -18.3099 - capsnet_accuracy: 0.6319 - val_loss: -6.9306 - val_capsnet_loss: 0.3197 - val_decoder_loss: -18.4956 - val_capsnet_accuracy: 0.6154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nsummary = pd.DataFrame({'fold': range(1,fold+1), 'Test accuracy': test_accuracy_allfold, 'train time': train_used_time_allfold, 'test time': test_used_time_allfold})\\nhyperparam = pd.DataFrame({'average acc of 10 folds': np.mean(test_accuracy_allfold), 'average train time of 10 folds': np.mean(train_used_time_allfold), 'average test time of 10 folds': np.mean(test_used_time_allfold),'epochs': args.epochs, 'lr':args.lr, 'batch size': args.batch_size},index=['dimention/sub'])\\nwriter = pd.ExcelWriter(args.save_dir + '/'+'summary'+ '_'+subject+'.xlsx')\\nsummary.to_excel(writer, 'Result', index=False)\\nhyperparam.to_excel(writer, 'HyperParam', index=False)\\nwriter.save()\\nprint('10 fold average accuracy: ', np.mean(test_accuracy_allfold))\\nprint('10 fold average train time: ', np.mean(train_used_time_allfold))\\nprint('10 fold average test time: ', np.mean(test_used_time_allfold))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5mlzs4mNJmM",
        "outputId": "8dbd9642-550a-411a-9cc8-8ad6f4617356"
      },
      "source": [
        "history.params"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epochs': 100, 'steps': 6, 'verbose': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "r6ONyQF_y_CM",
        "outputId": "11ab1417-6501-48be-da3d-410c8cc5ab41"
      },
      "source": [
        "results[-1:]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>capsnet_loss</th>\n",
              "      <th>decoder_loss</th>\n",
              "      <th>capsnet_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_capsnet_loss</th>\n",
              "      <th>val_decoder_loss</th>\n",
              "      <th>val_capsnet_accuracy</th>\n",
              "      <th>lr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>-6.870526</td>\n",
              "      <td>0.318049</td>\n",
              "      <td>-18.3382</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>-6.930588</td>\n",
              "      <td>0.319699</td>\n",
              "      <td>-18.495632</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.00001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        loss  capsnet_loss  ...  val_capsnet_accuracy       lr\n",
              "99 -6.870526      0.318049  ...              0.615385  0.00001\n",
              "\n",
              "[1 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXlIdxcwzBHO",
        "outputId": "849decd0-43f3-4069-c943-d9cfcf42553f"
      },
      "source": [
        "print (\"Accuracy for the training set: \", results.values[-1:][0][1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for the training set:  0.31804943084716797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytHVOXIczBpy",
        "outputId": "035b7c7d-200a-4038-81ff-15833e4d5104"
      },
      "source": [
        "print (\"Accuracy for the development test set: \", results.values[-1:][0][3])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for the development test set:  0.6153846383094788\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}