{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Capsnet.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZkZiEF8Sv2p"
      },
      "source": [
        "# Parte 4: Clasificación con Capsnet\n",
        "\n",
        "La implementación de esta red Capsnet se ha basado en el código implementado por Aurélien Géron (https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2EWbUEHSv2u"
      },
      "source": [
        "### Resultados con la arquitectura anterior: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEUTC_l8Sv2u"
      },
      "source": [
        "\n",
        "\n",
        "Resultados:\n",
        "    \n",
        "    TRAIN                   DEV\n",
        "    loss       accuracy     val_loss    val_accuracy\n",
        "       0.870418\t0.807692\t0.951864\t0.730769\n",
        "    \n",
        "Por tanto: \n",
        "\n",
        "    E = 1 - Accuracy\n",
        "    Etrain = 1 - 0.807692 = 0.192308\n",
        "    Etest = 1 - 0.730769 = 0.269231\n",
        "    \n",
        "    Bias = Etrain - Ehuman = 0.192308\n",
        "    Variance = Etest - Etrain = 0.269231 - 0.192308 = 0.076923\n",
        "\n",
        "La varianza se ha reducido muchisimo pero el bias ha aumentado a un 19%. Se tratará de mejorar el bias. Para mejorar esto será necesario añadir más complejidad, elegir una mejor optimización, cambiando la arquitectura (más neuronas, más capas)... \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOoUk2UqSv2v"
      },
      "source": [
        "### Cambios realizados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLuz3FH2Sv2w"
      },
      "source": [
        "Se aumenta de 100 a 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqxGVi3HSv2w"
      },
      "source": [
        "### Nuevos resultados: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjAyi9cnSv2x"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de2SdzdOSv2x"
      },
      "source": [
        "### 1 - Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb2onSPqSv2y"
      },
      "source": [
        "# Tensorflow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#Helper libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Signal libraries\n",
        "from scipy import signal"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg_RVKkYSv2z"
      },
      "source": [
        "# Reset the default graph, in case you re-run this notebook without restarting the kernel:\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# Random seeds so that this notebook always produces the same output:\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(45)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhaKxbUYSv20"
      },
      "source": [
        "### 2 - Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR0I6XXoSv20"
      },
      "source": [
        "class ROutput:\n",
        "    def __init__(self, task, data):\n",
        "        self.task = task\n",
        "        self.data = data\n",
        "        \n",
        "class OutTaskData: \n",
        "    def __init__(self, task, data): \n",
        "        self.task = task\n",
        "        self.data = data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t7wiwCPSv20"
      },
      "source": [
        "import scipy.io as sio\n",
        "# Primero leemos los registros\n",
        "def read_outputs(rec):\n",
        "    '''read_outputs(\"userS0091f1.mat\")'''\n",
        "    mat = sio.loadmat(rec)\n",
        "    mdata = mat['session']\n",
        "    val = mdata[0,0]\n",
        "    #output = ROutput(np.array(val[\"task\"]), np.array(val[\"data\"]))\n",
        "    output = ROutput(np.array(val[\"task_EEG_p\"]), np.array(val[\"data_processed_EEG\"]))\n",
        "    return output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNIFgSqYSv21"
      },
      "source": [
        "### Cargamos los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpCWbdPrSv21"
      },
      "source": [
        "# Configuración\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import Perceptron\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "task1 = 402 # SE PUEDE CAMBIAR\n",
        "task2 = 404 # SE PUEDE CAMBIAR\n",
        "task_OneHotEnconding = {402: [1.,0.], 404: [0.,1.]}\n",
        "user = 'W29' # SE PUEDE CAMBIAR\n",
        "day = '0329'\n",
        "folder_day = 'W29-29_03_2021'\n",
        "total_records = 22 # CAMBIAR SI HAY MAS REGISTROS\n",
        "fm = 200\n",
        "electrodes_names_selected = ['F3', 'FZ', 'FC1','FCZ','C1','CZ','CP1','CPZ', 'FC5', 'FC3','C5','C3','CP5','CP3','P3',\n",
        "                             'PZ','F4','FC2','FC4','FC6','C2','C4','CP2','CP4','C6','CP6','P4','HR' ,'HL', 'VU', 'VD']\n",
        "number_channels = len(electrodes_names_selected)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2nUYikPSv22",
        "outputId": "033dbf7e-b509-4a05-b6b2-e566031d6832"
      },
      "source": [
        "lTaskData = []\n",
        "total_records_used = 0\n",
        "for i_rec in range(1,total_records+1):\n",
        "    i_rec_record = i_rec\n",
        "    if i_rec_record <10:\n",
        "        i_rec_record = \"0\"+str(i_rec_record)\n",
        "    if i_rec % 2 == 0: # Registros impares primero: USUARIO SIN MOVIMIENTO SOLO PENSANDO\n",
        "        record = \"./RegistrosProcesados2/\"+folder_day+\"/W29_2021\"+day+\"_openloop_\"+str(i_rec_record)+\"_processed.mat\"\n",
        "        output = read_outputs(record) # output.task será y, output.data será x\n",
        "\n",
        "\n",
        "        output.task = np.transpose(output.task)\n",
        "        output.data = output.data.reshape((np.shape(output.data)[0],np.shape(output.data)[1]))\n",
        "        output.data = np.transpose(output.data)\n",
        "        #output.data = output.data.reshape((np.shape(output.data)[0],np.shape(output.data)[1],1))\n",
        "\n",
        "        outT = (output.task == task1) | (output.task == task2)\n",
        "        outData = output.data[0:np.shape(output.data)[0], outT[0,:]]\n",
        "        outTask = output.task[0, outT[0,:]]\n",
        "        outTD = OutTaskData(outTask, outData)\n",
        "\n",
        "        lTaskData.append(outTD)\n",
        "        total_records_used+=1\n",
        "print(total_records_used, total_records)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MKV0t3gSv24",
        "outputId": "5ed4f966-fede-4b66-c11c-ec7a5042b372"
      },
      "source": [
        "# Vamos a coger 2 registros para el entrenamiento, 1 para el conjunto dev set, 1 para el test set\n",
        "X_train, y_train, X_dev, y_dev, X_test, y_test = [],[],[],[],[],[] \n",
        "for j in range(0,total_records_used-3): # Cogemos 18 registros para entrenamiento\n",
        "    X_train.append(lTaskData[j].data)\n",
        "    y_train.append(lTaskData[j].task)\n",
        "\n",
        "for j in range(total_records_used-3,total_records_used-1): # Cogemos 2 registros para el dev set\n",
        "    X_dev.append(lTaskData[j].data)\n",
        "    y_dev.append(lTaskData[j].task)\n",
        "for j in range(total_records_used-1,total_records_used): # Cogemos 2 registros para el test set\n",
        "    X_test.append(lTaskData[j].data)\n",
        "    y_test.append(lTaskData[j].task)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "#y_train = np.ravel(np.array(y_train))\n",
        "y_train = np.array(y_train)\n",
        "X_dev = np.array(X_dev)\n",
        "#y_dev = np.ravel(np.array(y_dev))\n",
        "y_dev = np.array(y_dev)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "#y_test = np.ravel(np.array(y_test))\n",
        "\n",
        "print (\"X_train:\",X_train.shape)\n",
        "print (\"y_train:\",y_train.shape)\n",
        "print (\"X_dev:\",X_dev.shape)\n",
        "print (\"y_dev:\",y_dev.shape)\n",
        "print (\"X_test:\",X_test.shape)\n",
        "print (\"y_test:\",y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# VENTANEO Y ONE HOT ENCODING \n",
        "window = 5\n",
        "samples_advance = 3\n",
        "\n",
        "# Ventaneo X_train\n",
        "\n",
        "X_train_l = []\n",
        "y_train_l = []\n",
        "for num_X_train in range(np.shape(X_train)[0]): # Para no mezclar registros\n",
        "    win_init = int(0)\n",
        "    window_position = 0\n",
        "    \n",
        "    for i in range(np.shape(X_train)[2]): # For each signal registered\n",
        "        win_end = int(win_init + window)\n",
        "        if win_end >= np.shape(X_train)[2]:\n",
        "            break\n",
        "\n",
        "        task = np.unique(y_train[num_X_train,win_init:win_end])\n",
        "\n",
        "        if len(task)==1:\n",
        "        #if task1 in task or task2 in task:\n",
        "            signal_window = X_train[num_X_train, :, win_init:win_end]\n",
        "            \n",
        "            #data_filtered = preprocessing(signal_window, fm, number_channels)\n",
        "            #X_train_l.append(data_filtered)\n",
        "            X_train_l.append(signal_window)\n",
        "            #taskOH = task_OneHotEnconding[task[0]]\n",
        "            #y_train_l.append(taskOH)\n",
        "            y_train_l.append(task)\n",
        "            \n",
        "            \n",
        "        win_init += int(samples_advance)\n",
        "\n",
        "X_train_l = np.array(X_train_l)\n",
        "y_train_l = np.array(y_train_l)\n",
        "\n",
        "\n",
        "# Ventaneo X_dev\n",
        "X_dev_l = []\n",
        "y_dev_l = []\n",
        "for num_X_dev in range(np.shape(X_dev)[0]):\n",
        "    win_init = int(0)\n",
        "    window_position = 0\n",
        "    \n",
        "    for i in range(np.shape(X_dev)[2]): # For each signal registered\n",
        "        win_end = int(win_init + window)\n",
        "        if win_end >= np.shape(X_dev)[2]:\n",
        "            break\n",
        "\n",
        "        task = np.unique(y_dev[num_X_dev,win_init:win_end])\n",
        "\n",
        "        if len(task)==1:\n",
        "        #if task1 in task or task2 in task:\n",
        "            signal_window = X_dev[num_X_dev, :, win_init:win_end]\n",
        "            \n",
        "            #data_filtered = preprocessing(signal_window, fm, number_channels)\n",
        "            #X_train_l.append(data_filtered)\n",
        "            X_dev_l.append(signal_window)\n",
        "            #taskOH = task_OneHotEnconding[task[0]]\n",
        "            #y_dev_l.append(taskOH)\n",
        "            y_dev_l.append(task)\n",
        "            \n",
        "        win_init += int(samples_advance)\n",
        "\n",
        "X_dev_l = np.array(X_dev_l)\n",
        "y_dev_l = np.array(y_dev_l)\n",
        "\n",
        "# Ventaneo X_test\n",
        "X_test_l = []\n",
        "y_test_l = []\n",
        "for num_X_test in range(np.shape(X_test)[0]): \n",
        "    win_init = int(0)\n",
        "    window_position = 0\n",
        "    \n",
        "    for i in range(np.shape(X_test)[2]): # For each signal registered\n",
        "        win_end = int(win_init + window)\n",
        "        if win_end >= np.shape(X_test)[2]:\n",
        "            break\n",
        "\n",
        "        task = np.unique(y_test[num_X_test,win_init:win_end])\n",
        "\n",
        "        if len(task)==1:\n",
        "        #if task1 in task or task2 in task:\n",
        "            signal_window = X_test[num_X_test, :, win_init:win_end]\n",
        "            \n",
        "            #data_filtered = preprocessing(signal_window, fm, number_channels)\n",
        "            #X_train_l.append(data_filtered)\n",
        "            X_test_l.append(signal_window)\n",
        "            #taskOH = task_OneHotEnconding[task[0]]\n",
        "            y_test_l.append(task)\n",
        "            #y_test_l.append(taskOH)\n",
        "            \n",
        "        win_init += int(samples_advance)\n",
        "\n",
        "X_test_l = np.array(X_test_l)\n",
        "y_test_l = np.array(y_test_l)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train_l = X_train_l.reshape((np.shape(X_train_l)[0],np.shape(X_train_l)[1],np.shape(X_train_l)[2], 1))\n",
        "X_dev_l = X_dev_l.reshape((np.shape(X_dev_l)[0],np.shape(X_dev_l)[1],np.shape(X_dev_l)[2], 1))\n",
        "X_test_l = X_test_l.reshape((np.shape(X_test_l)[0],np.shape(X_test_l)[1],np.shape(X_test_l)[2], 1))\n",
        "\n",
        "print()\n",
        "print(\"ONE HOT ENCODER & WINDOWING:\")\n",
        "print (\"X_train:\",X_train_l.shape)\n",
        "print (\"y_train:\",y_train_l.shape)\n",
        "print (\"X_dev:\",X_dev_l.shape)\n",
        "print (\"y_dev:\",y_dev_l.shape)\n",
        "print (\"X_test:\",X_test_l.shape)\n",
        "print (\"y_test:\",y_test_l.shape)\n",
        "\n",
        "X_train = X_train_l\n",
        "y_train = y_train_l.reshape((np.shape(y_train_l)[0]))\n",
        "X_dev = X_dev_l\n",
        "y_dev = y_dev_l.reshape((np.shape(y_dev_l)[0]))\n",
        "X_test = X_test_l\n",
        "y_test = y_test_l.reshape((np.shape(y_test_l)[0]))\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_dev = X_dev.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "y_train = y_train.astype('int64')\n",
        "y_dev = y_dev.astype('int64')\n",
        "y_test = y_test.astype('int64')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train: (8, 32, 49)\n",
            "y_train: (8, 49)\n",
            "X_dev: (2, 32, 49)\n",
            "y_dev: (2, 49)\n",
            "X_test: (1, 32, 49)\n",
            "y_test: (1, 49)\n",
            "\n",
            "ONE HOT ENCODER & WINDOWING:\n",
            "X_train: (104, 32, 5, 1)\n",
            "y_train: (104, 1)\n",
            "X_dev: (26, 32, 5, 1)\n",
            "y_dev: (26, 1)\n",
            "X_test: (13, 32, 5, 1)\n",
            "y_test: (13, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKC1lv_fSv29"
      },
      "source": [
        "### 4 - Construcción de la Capsnet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E82S17oOSv29"
      },
      "source": [
        "from keras.layers.convolutional import Conv2D"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx54iCjdSv29"
      },
      "source": [
        "#### Datos de entrada: \n",
        "Se comienza creando un placeholder para los datos de entrada. Un placeholder es una variable a la que se le asigna datos después. Se utiliza para alimentar ejemplos de entrenamiento reales. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyINtCZJSv29"
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "X = tf.compat.v1.placeholder(shape=[None, 32, 5, 1], dtype=tf.float32, name=\"X\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sRL8j6cSv2-"
      },
      "source": [
        "#### Capsulas primarias \n",
        "\n",
        "La primera capa estará compuesta de 128 mapas de características con 5x5 capsulas cada una, donde cada capsula tendrá como salida un vector de activación 4D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1HKWxr-Sv2-"
      },
      "source": [
        "caps1_n_maps = 128 \n",
        "caps1_n_caps = caps1_n_maps * 5 * 5 # 3200 capsulas primarias \n",
        "caps1_n_dims = 4"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AEAr5ERSv2-"
      },
      "source": [
        "Para calcular sus salidas, primero se aplican dos capas convolucionales regulares: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIXqZjL7Sv2-"
      },
      "source": [
        "conv1_params = { # PRIMERA CAPA CONVOLUCIONAL\n",
        "    \"filters\": 4, # 4,\n",
        "    \"kernel_size\": 3,\n",
        "    \"strides\": 1,\n",
        "    \"padding\": \"same\",\n",
        "    \"activation\": tf.nn.relu # Se utiliza una función ReLu en vez de la SeLu utilizada en el artículo de Capsnet - EEG\n",
        "}\n",
        "conv2_params = { # CÁPSULAS PRIMARIAS\n",
        "    \"filters\": caps1_n_maps * caps1_n_dims, # 512 filtros creados porque hay 128 cápsulas con una dimensión de 4 lo que hace un total de 512 filtros\n",
        "    \"kernel_size\": 3,\n",
        "    \"strides\": 2,\n",
        "    \"padding\": \"same\",\n",
        "    \"activation\": tf.nn.relu # Se utiliza una función ReLu en vez de la SeLu utilizada en el artículo de Capsnet - EEG\n",
        "}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu03zNlcSv2-",
        "outputId": "be7f473a-1e28-4139-d37e-d25076481563"
      },
      "source": [
        "conv1 = tf.keras.layers.Conv2D(name=\"conv1\", **conv1_params, input_shape=(32, 5, 1 ))(X)\n",
        "print(conv1.shape) # (32-3+0)/1 + 1 = 30; (5-3+0)/1 +1 = 3; 4 filtros => (30,3,4)\n",
        "conv2 = tf.keras.layers.Conv2D( name=\"conv2\", **conv2_params)(conv1)\n",
        "print(conv2.shape) # (30-3+0)/2 + 1 = 14; (3-3+0)/2 +1 = 1; 4 filtros => (30,3,512)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 32, 5, 4)\n",
            "(None, 16, 3, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMstdkL3Sv2_"
      },
      "source": [
        "A continuación, cambiamos la forma de la salida para obtener un grupo de vectores 8D que representan las salidas de las cápsulas primarias.\n",
        "\n",
        "La salida de la conv2 es un array que contiene 3200 (128x4) mapas de características para cada instancia, donde cada mapa de características es 5x5. Por tanto, el tamaño de la salida será (tamaño del batch,5,5,3200). Se busca dividir los 3200 en 128 vectores de 4 dimensiones cada uno. Eso se puede lograr haciendo reshape (cambiar la forma) a (tamaño del batch,5,5,128,4). Sin embargo, como esta primera capa de cápsula está completamente conectada a la siguiente capa de cápsula, se puede simplemente aplanar (flatten) con rejillas (grids) de 6x6. Esto significa con que sirve con hacer reshape a (tamaño del batch, 5x5x128,4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvQs2EDpSv2_",
        "outputId": "8d1f5cbb-306f-44b1-a910-57ed3bb01ff5"
      },
      "source": [
        "caps1_raw = tf.reshape(conv2, [-1, caps1_n_caps, caps1_n_dims],\n",
        "                       name=\"caps1_raw\")\n",
        "caps1_raw"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'caps1_raw:0' shape=(None, 3200, 4) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d_uj-npSv2_"
      },
      "source": [
        "Ahora, es necesario utilizar la función squash en todos estos vectores para normalizarlos. \n",
        "\n",
        "Es notorio que no se puede usar tf.norm() porque la derivada de ||s|| no está definida cuando ||s||=0, ya que si un vector es 0 explotará durante el entrenamiento (explosión de gradiente, lo hemos visto en Deep), es decir, los gradientes serán nan, por lo que cuando el optimizador actualice las variables, también se convertirán en nan, y de ahí en adelante quedará atrapado en nan. La solución es implementar la norma manualmente calculando la raíz cuadrada de la suma de cuadrados más un pequeño valor épsilon: $\\left \\| s \\right \\| \\approx \\sqrt{\\sum_{i}s_{i}^{2}+\\epsilon }$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP4rG6_7Sv3A"
      },
      "source": [
        "def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
        "    # Función actualizada extraída de Aurélien Géron\n",
        "    with tf.name_scope(name):\n",
        "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis, keepdims=True)\n",
        "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
        "        squash_factor = squared_norm / (1. + squared_norm)\n",
        "        unit_vector = s / safe_norm\n",
        "        return squash_factor * unit_vector"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPHfN_ttSv3A"
      },
      "source": [
        "Ahora se aplica esta función para obtener la salida $u_{i}$ para cada cápsula primaria i:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9-YNHuySv3A",
        "outputId": "1c2c4e16-ea82-476d-acd6-918180f775ed"
      },
      "source": [
        "caps1_output = squash(caps1_raw, name=\"caps1_output\")\n",
        "np.shape(caps1_output)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 3200, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcqvF7aLSv3A"
      },
      "source": [
        "Así ya se tiene calculada la salida de la primera capa de cápsula. La dificultad comienza ahora ya que habrá que utilizar el algoritmo de enrutamiento dinámico entre las cápsulas primarias y las cápsulas MI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icr8kUkQSv3A"
      },
      "source": [
        "#### Cápsulas MI\n",
        "Para calcular la salida de las cápsulas MI, primero se deben calcular los vectores de salida predichos (uno para cada par de cápsulas primaria/MI). Entonces, se podrá correr el algoritmo de enrutamiento dinámico por acuerdos.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAYciK1jSv3B"
      },
      "source": [
        "###### Calculando los vectores de salida predichos\n",
        "La capa de cápsula MI contendrá 2 cápsulas (uno por cada tarea) de 8 dimensiones cada una: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGct2TFqSv3B"
      },
      "source": [
        "caps2_n_caps = 2\n",
        "caps2_n_dims = 8"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKxtw3_PSv3B"
      },
      "source": [
        "Para cada cápsula i en la primera capa, se busca predecir la salida de cadapa cápsula j en la segunda capa. Para ello, se necesitará una matriz de transformación $W_{ij}$ (una por cada par de cápsulas (i,j)), entonces se podrá calcular la salida predicha $\\hat{u}_{j|i} = W_{ij}u_{i}$. Como se quiere transformar un vector 4D en un vector 8D, cada matriz de transformación $W_{ij}$ debe tener una dimensión (shape) de (8,4),\n",
        "\n",
        "Para calcular $\\hat{u}_{j|i}$ para cada par de cápsulas (i,j), se usará la función tf.matmul() que multiplica arrays de grandes dimensiones. \n",
        "\n",
        "El primer array tiene unas dimensiones de (tamaño del batch, 3200 capsulas primarias, 2, 8, 4) y la forma del segundo array es (tamaño del batch, 3200, 2, 4, 1).\n",
        "\n",
        "Las cápsulas de la primera capa en realidad ya generan predicciones para los datos de tamaño batch, por lo que la segunda matriz estará bien, pero para la primera matriz, necesitaremos usar tf.tile () para tener x (del tamaño del batch) copias de las matrices de transformación.\n",
        "\n",
        "Se va a comenzar creanto una variable de entrenamiento W de dimensión (1, 3200 , 2, 8, 4) que contendrá todas las matrices de transformación, la primera dimensión de tamaño 1 hará que sea fácil realizar las copias. Se iniciará esta variable aleatoriamente usando una distribución normal con una desviación estandar 0.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWLJxqbLSv3B"
      },
      "source": [
        "init_sigma = 0.1\n",
        "\n",
        "W_init = tf.random.normal(\n",
        "    shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\n",
        "    stddev=init_sigma, dtype=tf.float32, name=\"W_init\")\n",
        "W = tf.Variable(W_init, name=\"W\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnyjYh_6Sv3B"
      },
      "source": [
        "Ahora podemos crear la primera matriz repitiendo W una vez por instancia:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT1exSDmSv3B"
      },
      "source": [
        "batch_size = tf.shape(X)[0]\n",
        "W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Z-zPwPSv3B"
      },
      "source": [
        "Ahora vamos con la segunda matriz, que como se comentó tiene que ser de dimensión (tamaño del batch, 3200, 2, 4, 1) que contiene la salida de las cápsulas de la primera capa repetidas 2 veces (una vez por tarea, a lo largo de la tercera dimension, con axis=2). La salida de la capa 1 tiene una dimensión (tamaño del batch, 3200, 4) por lo que primero debemos expandirla dos veces, para obtener una matriz de forma (tamaño del batch, 3200, 1, 4, 1), entonces esto se podrá repetir 2 veces a lo largo de la tercera dimensión."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "096szW7qSv3C"
      },
      "source": [
        "caps1_output_expanded = tf.expand_dims(caps1_output, -1,\n",
        "                                       name=\"caps1_output_expanded\")\n",
        "caps1_output_tile = tf.expand_dims(caps1_output_expanded, 2,\n",
        "                                   name=\"caps1_output_tile\")\n",
        "caps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps, 1, 1],\n",
        "                             name=\"caps1_output_tiled\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfB7j0FjSv3C"
      },
      "source": [
        "Se comprueban las dimensiones del primer array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db_t8nODSv3C",
        "outputId": "5dd9df84-564b-409a-b237-437ca29a640e"
      },
      "source": [
        "W_tiled"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'W_tiled:0' shape=(None, 3200, 2, 8, 4) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EpVBm5ySv3C"
      },
      "source": [
        "Ya es posible predecir vectores de salida $\\hat{u}_{j|i}$. Para ello se multiplican estos dos arrays usando tf.matmul() como se explico anteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps9vqrOfSv3C",
        "outputId": "1c33b23f-b39a-44da-f743-17ec4d17143f"
      },
      "source": [
        "caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled,\n",
        "                            name=\"caps2_predicted\")\n",
        "caps2_predicted # Se comprueba la dimensión"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'caps2_predicted:0' shape=(None, 3200, 2, 8, 1) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_0CDniLSv3C"
      },
      "source": [
        "Por tanto, para cada instancia en el batch (que aún no se ha instanciado) y para cada par de primeras y segundas capas de cápsulas (3200x2) se tiene un vector de salida de predicción de 8D (8x1). \n",
        "\n",
        "Es el momento de aplicar el algoritmo de enrutamiento dinámico por acuerdos: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBI5_Ad-Sv3D"
      },
      "source": [
        "#### Routing by agreement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_XzAm4kSv3D"
      },
      "source": [
        "Primero, se inicializan los pesos inicialies $b_{ij}$ a 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP0Wu6igSv3D",
        "outputId": "5f410c0e-7957-4790-a65b-c25783693a58"
      },
      "source": [
        "raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1],\n",
        "                       dtype=np.float32, name=\"raw_weights\")\n",
        "# Las últimas 2 dimensiones tienen tamaño 1, ahora se explicará el por qué\n",
        "raw_weights"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'raw_weights:0' shape=(None, 3200, 2, 1, 1) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYwC_rtHSv3D"
      },
      "source": [
        "###### Ronda 1\n",
        "Se aplica la función softmax para calcular los pesos de routing $c_{i}=softmax(b_{i})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYjzc4kjSv3D",
        "outputId": "1e1a9fee-8f77-494c-8721-372b86fc206b"
      },
      "source": [
        "routing_weights = tf.nn.softmax(raw_weights, name=\"routing_weights\")\n",
        "routing_weights"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'routing_weights:0' shape=(None, 3200, 2, 1, 1) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UyS_8z6Sv3D"
      },
      "source": [
        "Ahora se va a calcular la net para todos las salidas predichas de los vectores para cada capsula de la segunda capa, $s_{j}= \\sum_{i} c_{ij}\\hat{u}_{j|i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M9vee_7Sv3E",
        "outputId": "c3967cd2-b56d-49ba-eef8-783c28eadaa4"
      },
      "source": [
        "weighted_predictions = tf.multiply(routing_weights, caps2_predicted,\n",
        "                                   name=\"weighted_predictions\")# realiza multiplicación de matrices por elementos\n",
        "\n",
        "weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keepdims=True,\n",
        "                             name=\"weighted_sum\")\n",
        "\n",
        "print(weighted_predictions)\n",
        "print(weighted_sum)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"weighted_predictions:0\", shape=(None, 3200, 2, 8, 1), dtype=float32)\n",
            "Tensor(\"weighted_sum:0\", shape=(None, 1, 2, 8, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c6qRocmSv3E"
      },
      "source": [
        "Finalmente, se aplica la función squash para obtener las salidas de las cápsulas de la segunda capa al final de la primera iteración del algoritmo de enrutamiento por acuerdo $v_{j} = \\frac{\\left \\| s_{j} \\right \\|^{2}}{1+\\left \\| s_{j} \\right \\|^{2}}\\frac{s_{j}}{\\left \\| s_{j} \\right \\|}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etIQe-wVSv3E",
        "outputId": "f1724c50-19f4-4cef-d9b7-b8cee313dcd0"
      },
      "source": [
        "caps2_output_round_1 = squash(weighted_sum, axis=-2,\n",
        "                              name=\"caps2_output_round_1\")\n",
        "caps2_output_round_1 # Se tienen que tener 8D vectores de salida para cada instancia (2)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'caps2_output_round_1/mul:0' shape=(None, 1, 2, 8, 1) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li8zuxKpSv3E"
      },
      "source": [
        "###### Ronda 2\n",
        "Primero, se medirá como de cerca está cada vector predicho $\\hat{u}_{j|i}$ de la actual vector de salida $v_{j}$ calculando su producto escalar  $\\hat{u}_{j|i} \\cdot  v_{j}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNi9kphMSv3E",
        "outputId": "1a2235de-ac14-44af-8b04-1387d8e95104"
      },
      "source": [
        "caps2_predicted"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'caps2_predicted:0' shape=(None, 3200, 2, 8, 1) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_mi6pZ8Sv3F",
        "outputId": "9991e053-1985-4d4e-a9b3-de2714d4909a"
      },
      "source": [
        "caps2_output_round_1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'caps2_output_round_1/mul:0' shape=(None, 1, 2, 8, 1) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1m5OMrlSv3F"
      },
      "source": [
        "caps2_output_round_1_tiled = tf.tile(\n",
        "    caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1],\n",
        "    name=\"caps2_output_round_1_tiled\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKdVud4MSv3F"
      },
      "source": [
        "agreement = tf.matmul(caps2_predicted, caps2_output_round_1_tiled,\n",
        "                      transpose_a=True, name=\"agreement\")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4hR0wcISv3F"
      },
      "source": [
        "Ahora se pueden actualizar los pesos de enrutamiento $b_{i,j}$: \n",
        "$b_{i,j} \\leftarrow b_{i,j} + \\hat{u}_{j|i} \\cdot  v_{j}$  (see Procedure 1, step 7, in the paper).\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j86KpGWISv3F"
      },
      "source": [
        "raw_weights_round_2 = tf.add(raw_weights, agreement,\n",
        "                             name=\"raw_weights_round_2\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBFw_PRHSv3F"
      },
      "source": [
        "El resto de la ronda 2 es la misma que la ronda 1: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK2cnHpSSv3F"
      },
      "source": [
        "routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,\n",
        "                                        name=\"routing_weights_round_2\")\n",
        "weighted_predictions_round_2 = tf.multiply(routing_weights_round_2,\n",
        "                                           caps2_predicted,\n",
        "                                           name=\"weighted_predictions_round_2\")\n",
        "weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2,\n",
        "                                     axis=1, keepdims=True,\n",
        "                                     name=\"weighted_sum_round_2\")\n",
        "caps2_output_round_2 = squash(weighted_sum_round_2,\n",
        "                              axis=-2,\n",
        "                              name=\"caps2_output_round_2\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zQTd4ooSv3G"
      },
      "source": [
        "Se pueden repetir todas las rondas que se quiera, para ello habría que hacer exactamente los mismos pasos que en la ronda 2. Solo se harán para este TFM 2 rondas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjyeMBmPSv3G"
      },
      "source": [
        "caps2_output = caps2_output_round_2"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzEXXhlRSv3G"
      },
      "source": [
        "#### Estimando las clases de probabilidad (longitud del vector)\n",
        "Las longitudes de los vectores de salida representan las probabilidades de clase, para ello se puede usar tf.norm pero se corre el riesgo con los valores 0 como se explicó anteriormente, por ello se crea una función propia con epsilon:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDi8N2sESv3G"
      },
      "source": [
        "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False, name=None):\n",
        "    with tf.name_scope(name):\n",
        "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
        "                                     keepdims=keep_dims)\n",
        "        return tf.sqrt(squared_norm + epsilon)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6CRCNmdSv3G"
      },
      "source": [
        "y_proba = safe_norm(caps2_output, axis=-2, name=\"y_proba\")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD8LuaHISv3G"
      },
      "source": [
        "Se predecirá la clase con mayor probabilidad estimada: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5omcg-WSv3G",
        "outputId": "d4af1743-b9a5-4d79-ff25-e404fbb7afab"
      },
      "source": [
        "y_proba_argmax = tf.argmax(y_proba, axis=2, name=\"y_proba\")\n",
        "y_proba_argmax"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'y_proba_1:0' shape=(None, 1, 1) dtype=int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pUkt0ETSv3H",
        "outputId": "c9c36997-3e77-483e-c6b2-35dbd25d5cd4"
      },
      "source": [
        "y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\") # squeeze elimina las dimensiones de tamaño 1 que ya no se necesitan\n",
        "y_pred"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'y_pred:0' shape=(None,) dtype=int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es3Mqdb2Sv3H"
      },
      "source": [
        "#### Operaciones de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shLl6O30Sv3H"
      },
      "source": [
        "###### Etiquetas (labels)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdKuxZFHSv3H",
        "outputId": "482ea888-ff48-47c8-ecb0-6cf37c53ed41"
      },
      "source": [
        "y = tf.compat.v1.placeholder(shape=[None], dtype=tf.int64, name=\"y\") # Placeholder para las etiquetas\n",
        "y"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'y:0' shape=(None,) dtype=int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2DNrirbSv3H"
      },
      "source": [
        "###### Margin loss\n",
        "\n",
        "Se usa una función de perdida especial de margen que hace posible detectar dos o más tareas en cada conjunto de datos: \n",
        "$L_{k} = T_{k}max(0,m^{+} - \\left \\| v_{k} \\right \\|)^{2} + \\lambda (1-T_{k})max(0,\\left \\| v_{k} \\right \\| -m^{-})^{2}$\n",
        "\n",
        "Donde $T_{k}$ será 1 si la clase k está presente o 0 en otro caso. Los hiperparámetros $m^{+}$ y $m^{-}$ se instancian a 0.9 y 0.1. Y $\\lambda$, que reduce la influencia de la perdida en las etiquetas que no pertecen a la clase correcta, se instancia a 0.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WLmVUTBSv3I"
      },
      "source": [
        "m_plus = 0.9\n",
        "m_minus = 0.1\n",
        "lambda_ = 0.5"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy6uxOY0Sv3I"
      },
      "source": [
        "T = tf.one_hot(y, depth=caps2_n_caps, name=\"T\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB2kcF9NSv3I",
        "outputId": "199f09c6-d462-4849-b87b-94ae03a87452"
      },
      "source": [
        "with tf.compat.v1.Session():\n",
        "    print(T.eval(feed_dict={y: np.array([0, 1, 2, 3, 9])}))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24pFnsRrSv3I"
      },
      "source": [
        "Ahora, se calculará la norma de los vectores salida para cada salida de cápsula y cada instancia:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgtmFIGxSv3I",
        "outputId": "6b6be93f-100b-4a91-b11f-54dff8a3888c"
      },
      "source": [
        "caps2_output"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'caps2_output_round_2/mul:0' shape=(None, 1, 2, 8, 1) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4J-gaQfSv3I"
      },
      "source": [
        "caps2_output_norm = safe_norm(caps2_output, axis=-2, keep_dims=True,\n",
        "                              name=\"caps2_output_norm\")"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sulmn17-Sv3J"
      },
      "source": [
        "Calculamos $max(0,m^{+} - \\left \\| v_{k} \\right \\|)^{2}$ y se cambia su dimension a (batch size, 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBIzozW-Sv3J"
      },
      "source": [
        "present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),\n",
        "                              name=\"present_error_raw\")\n",
        "present_error = tf.reshape(present_error_raw, shape=(-1, 2),\n",
        "                           name=\"present_error\")"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KibV2mzTSv3J"
      },
      "source": [
        "Calculamos $max(0,\\left \\| v_{k} \\right \\| -m^{-})^{2}$ y se cambia su dimension a (batch size, 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFU0JU_KSv3J"
      },
      "source": [
        "absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),\n",
        "                             name=\"absent_error_raw\")\n",
        "absent_error = tf.reshape(absent_error_raw, shape=(-1, 2),\n",
        "                          name=\"absent_error\")"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gru4IptZSv3J"
      },
      "source": [
        "Y ya calculamos el loss para cada instancia $L_{0}+L_{1}$ y calculamos la media. Lo que da un loss final: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzM2vmaISv3J"
      },
      "source": [
        "L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error,\n",
        "           name=\"L\")"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6ieyR4PSv3J"
      },
      "source": [
        "margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG7uxlIMSv3J"
      },
      "source": [
        "#### Reconstrucción "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzYcc9wgSv3K"
      },
      "source": [
        "Ahora es el momento de añadir la red decoder al final de la red capsular. Esta será una red de 3 capas completamente conectadas la cual aprenderá la reconstrucción de los datos de entrada basándose en la salida de la red capsular. Esto fuerza a la red capsular a mantener la información para reconstruir los datos. Además, regulariza el modelo, reduciendo el riesgo de overfitting y ayudando a generalizar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP_77c2iSv3K"
      },
      "source": [
        "###### Mask\n",
        "\n",
        "Durante el entrenamiento, en lugar de enviar todas las salidas de las redes capsulares a la red decoder, solo se deben enviar los vectores de salida de las capsulas que correspondan a la tarea objetivo. Todas los otros vectores de salida deben ser enmascarados. Vamos, habrá que enmascarar todos menos el que más largo que será el que corresponderá a la entrada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUH1w2DfSv3K"
      },
      "source": [
        "mask_with_labels = tf.compat.v1.placeholder_with_default(False, shape=(),\n",
        "                                               name=\"mask_with_labels\")"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GLmDnVXSv3K"
      },
      "source": [
        "reconstruction_targets = tf.cond(mask_with_labels, # condition\n",
        "                                 lambda: y,        # if True\n",
        "                                 lambda: y_pred,   # if False\n",
        "                                 name=\"reconstruction_targets\")"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAwQKZIeSv3K"
      },
      "source": [
        "reconstruction_mask = tf.one_hot(reconstruction_targets,\n",
        "                                 depth=caps2_n_caps,\n",
        "                                 name=\"reconstruction_mask\")"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxcwQsmdSv3K",
        "outputId": "42c52eb6-4d36-4153-b792-cbe1d375841a"
      },
      "source": [
        "reconstruction_mask"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'reconstruction_mask:0' shape=(None, 2) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKrBHvpVSv3K",
        "outputId": "538edd87-4981-4b5f-da07-b1d17e1f28d9"
      },
      "source": [
        "caps2_output"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'caps2_output_round_2/mul:0' shape=(None, 1, 2, 8, 1) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHraLgAjSv3L"
      },
      "source": [
        "reconstruction_mask_reshaped = tf.reshape(\n",
        "    reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1],\n",
        "    name=\"reconstruction_mask_reshaped\")"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybFSU7WWSv3L"
      },
      "source": [
        "Aplicamos la máscara: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W6APM0tSv3L"
      },
      "source": [
        "caps2_output_masked = tf.multiply(\n",
        "    caps2_output, reconstruction_mask_reshaped,\n",
        "    name=\"caps2_output_masked\")"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW5qD7TnSv3L",
        "outputId": "c60d8d67-3b37-4aa8-de0a-f8097262bb58"
      },
      "source": [
        "caps2_output_masked"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'caps2_output_masked:0' shape=(None, 1, 2, 8, 1) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNT6NnvCSv3L"
      },
      "source": [
        " #  reshape operation to flatten the decoder's inputs\n",
        "decoder_input = tf.reshape(caps2_output_masked,\n",
        "                       [-1, caps2_n_caps * caps2_n_dims],\n",
        "                       name=\"decoder_input\")"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga3VpVEASv3L"
      },
      "source": [
        "###### Decoder\n",
        "2 densas capas ReLu completamente conectadas seguidas de una capa de salida sigmoide"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH27jO-BSv3L"
      },
      "source": [
        "n_hidden1 = 512\n",
        "n_hidden2 = 1024\n",
        "n_output = 32 * 5 # 32 x 5"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJWpmsU5Sv3L"
      },
      "source": [
        "with tf.name_scope(\"decoder\"):\n",
        "    hidden1 = tf.keras.layers.Dense(n_hidden1, activation='relu', name=\"hidden1\")(decoder_input) \n",
        "    hidden2 = tf.keras.layers.Dense(n_hidden2, activation='relu', name=\"hidden2\")(hidden1)\n",
        "    decoder_output = tf.keras.layers.Dense(n_output, activation='sigmoid', name=\"decoder_output\")(hidden2)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_lQyeQSSv3M"
      },
      "source": [
        "###### Loss de la reconstrucción\n",
        "Solo es la diferencia al cuadrado entre los datos de entrada y los datos reconstruidos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EucfLGgSv3M"
      },
      "source": [
        "X_flat = tf.reshape(X, [-1, n_output], name=\"X_flat\")\n",
        "squared_difference = tf.square(X_flat - decoder_output,\n",
        "                               name=\"squared_difference\")\n",
        "reconstruction_loss = tf.reduce_mean(squared_difference,\n",
        "                                    name=\"reconstruction_loss\")"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNMP4D3iSv3M"
      },
      "source": [
        "###### Loss final\n",
        "Es la suma del margin loss y del loss de la reconstrucción"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6iv_7lFSv3M"
      },
      "source": [
        "alpha = 0.0005\n",
        "\n",
        "loss = tf.add(margin_loss, alpha * reconstruction_loss, name=\"loss\")"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU2E8P-LSv3M"
      },
      "source": [
        "#### Entrenamiento y evaluación\n",
        "Primero instanciamos el accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71TvYCrtSv3M"
      },
      "source": [
        "correct = tf.equal(y, y_pred, name=\"correct\")\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7dzG2ylSv3M"
      },
      "source": [
        "Los hiperparámetros de entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW7it4VfSv3N"
      },
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer()\n",
        "training_op = optimizer.minimize(loss, name=\"training_op\")"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0w7Ef9lSv3N"
      },
      "source": [
        "Se crea la variable inicializadora y de guardado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWkBMc4_Sv3N"
      },
      "source": [
        "init = tf.compat.v1.global_variables_initializer()\n",
        "saver = tf.compat.v1.train.Saver()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO7Z3SdMSv3N",
        "outputId": "a2326c30-fba1-4a3c-ac1e-9545fffc3051"
      },
      "source": [
        "X_train[0:1].dtype"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2FH__DCSv3N"
      },
      "source": [
        ""
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X29f2wt_Sv3N"
      },
      "source": [
        "### 5 - Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EMZxCMPTSv3N",
        "outputId": "84e591a4-6929-4a4b-d452-4c8ec9559d62"
      },
      "source": [
        "n_epochs = 500\n",
        "batch_size = 28\n",
        "#restore_checkpoint = False\n",
        "\n",
        "n_iterations_per_epoch = len(X_train) // batch_size\n",
        "n_iterations_validation = len(X_dev) // batch_size\n",
        "best_loss_val = np.infty\n",
        "#checkpoint_path = \"./my_capsule_network\"\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    #if restore_checkpoint and tf.compat.v1.train.checkpoint_exists(checkpoint_path):\n",
        "    #    saver.restore(sess, checkpoint_path)\n",
        "    #else:\n",
        "    init.run()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        b0 = 0\n",
        "        c0 = 0\n",
        "        for iteration in range(1, n_iterations_per_epoch + 1):\n",
        "            X_batch, y_batch = X_train[b0:b0+batch_size], y_train[b0:b0+batch_size]\n",
        "            # Run the training operation and measure the loss:\n",
        "            print(\"X_batch, y_batch\", np.shape(X_batch), np.shape(y_batch))\n",
        "            _, loss_train = sess.run(\n",
        "                [training_op, loss],\n",
        "                feed_dict={X: X_batch.reshape([-1, 32, 5, 1]),\n",
        "                           y: y_batch,\n",
        "                           mask_with_labels: True})\n",
        "            \n",
        "            print(\"HOLAAAAAAAAAAAAAAaa2\")\n",
        "            print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
        "                      iteration, n_iterations_per_epoch,\n",
        "                      iteration * 100 / n_iterations_per_epoch,\n",
        "                      loss_train),\n",
        "                  end=\"\")\n",
        "            b0+=batch_size\n",
        "\n",
        "        # At the end of each epoch,\n",
        "        # measure the validation loss and accuracy:\n",
        "        loss_vals = []\n",
        "        acc_vals = []\n",
        "        \n",
        "        for iteration in range(1, n_iterations_validation + 1):\n",
        "            X_batch, y_batch = X_dev[c0:c0+batch_size], y_dev[c0:c0+batch_size]\n",
        "            loss_val, acc_val = sess.run(\n",
        "                    [loss, accuracy],\n",
        "                    feed_dict={X: X_batch.reshape([-1, 32, 5, 1]),\n",
        "                               y: y_batch})\n",
        "            loss_vals.append(loss_val)\n",
        "            acc_vals.append(acc_val)\n",
        "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%) {}\".format(\n",
        "                      iteration, n_iterations_validation,\n",
        "                      iteration * 100 / n_iterations_validation, c0),\n",
        "                  end=\" \" * 10)\n",
        "            c0+=batch_size\n",
        "        loss_val = np.mean(loss_vals)\n",
        "        acc_val = np.mean(acc_vals)\n",
        "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
        "            epoch + 1, acc_val * 100, loss_val,\n",
        "            \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
        "\n",
        "        # And save the model if it improved:\n",
        "        if loss_val < best_loss_val:\n",
        "            save_path = saver.save(sess, checkpoint_path)\n",
        "            best_loss_val = loss_val"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_batch, y_batch (28, 32, 5, 1) (28,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1360\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: Input to reshape is a tensor with 688128 values, but the requested shape requires a multiple of 12800\n\t [[{{node caps1_raw}}]]\n  (1) Invalid argument: Input to reshape is a tensor with 688128 values, but the requested shape requires a multiple of 12800\n\t [[{{node caps1_raw}}]]\n\t [[loss/_39]]\n0 successful operations.\n0 derived errors ignored.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-ebaf7fa12545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                 feed_dict={X: X_batch.reshape([-1, 32, 5, 1]),\n\u001b[1;32m     26\u001b[0m                            \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                            mask_with_labels: True})\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HOLAAAAAAAAAAAAAAaa2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 968\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1191\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1369\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1392\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1394\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: Input to reshape is a tensor with 688128 values, but the requested shape requires a multiple of 12800\n\t [[node caps1_raw (defined at <ipython-input-13-24415626943c>:2) ]]\n  (1) Invalid argument: Input to reshape is a tensor with 688128 values, but the requested shape requires a multiple of 12800\n\t [[node caps1_raw (defined at <ipython-input-13-24415626943c>:2) ]]\n\t [[loss/_39]]\n0 successful operations.\n0 derived errors ignored.\n\nErrors may have originated from an input operation.\nInput Source operations connected to node caps1_raw:\n conv2/Relu (defined at <ipython-input-12-ea4146c3bbaa>:3)\n\nInput Source operations connected to node caps1_raw:\n conv2/Relu (defined at <ipython-input-12-ea4146c3bbaa>:3)\n\nOriginal stack trace for 'caps1_raw':\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 845, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 535, in <lambda>\n    self.io_loop.add_callback(lambda: self._handle_events(self.socket, 0))\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 451, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 434, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-24415626943c>\", line 2, in <module>\n    name=\"caps1_raw\")\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\n    return target(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 195, in reshape\n    result = gen_array_ops.reshape(tensor, shape, name)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 8398, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 750, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 3565, in _create_op_internal\n    op_def=op_def)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 2045, in __init__\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3ba-e_RSv3O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}