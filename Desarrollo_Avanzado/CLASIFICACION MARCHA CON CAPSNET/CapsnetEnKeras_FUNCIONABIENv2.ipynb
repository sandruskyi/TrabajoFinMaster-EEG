{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "CapsnetEnKeras.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oieoYd8_NJl4"
      },
      "source": [
        "# Parte 4: Clasificación con Capsnet\n",
        "\n",
        "La implementación de esta red Capsnet se ha basado en el código implementado por Aurélien Géron (https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOYsNIkfNJl7"
      },
      "source": [
        "### Resultados con la arquitectura anterior: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zeUMdGrNJl8"
      },
      "source": [
        "### Cambios realizados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvWOKZzaNJl8"
      },
      "source": [
        "Se aumenta de 100 a 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5eey6oGNJl8"
      },
      "source": [
        "### Nuevos resultados: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGdAPW3NNJl9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRy2Mth1NJl9"
      },
      "source": [
        "### 1 - Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cse2ICU7NJl-"
      },
      "source": [
        "# Tensorflow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#Helper libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Signal libraries\n",
        "from scipy import signal"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZdDJXPXNJl-"
      },
      "source": [
        "# Reset the default graph, in case you re-run this notebook without restarting the kernel:\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# Random seeds so that this notebook always produces the same output:\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(45)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4TwxUdJNJl_"
      },
      "source": [
        "### 2 - Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19rmC_7VNJl_"
      },
      "source": [
        "class ROutput:\n",
        "    def __init__(self, task, data):\n",
        "        self.task = task\n",
        "        self.data = data\n",
        "        \n",
        "class OutTaskData: \n",
        "    def __init__(self, task, data): \n",
        "        self.task = task\n",
        "        self.data = data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMzZss4DNJmA"
      },
      "source": [
        "import scipy.io as sio\n",
        "# Primero leemos los registros\n",
        "def read_outputs(rec):\n",
        "    '''read_outputs(\"userS0091f1.mat\")'''\n",
        "    mat = sio.loadmat(rec)\n",
        "    mdata = mat['session']\n",
        "    val = mdata[0,0]\n",
        "    #output = ROutput(np.array(val[\"task\"]), np.array(val[\"data\"]))\n",
        "    output = ROutput(np.array(val[\"task_EEG_p\"]), np.array(val[\"data_processed_EEG\"]))\n",
        "    return output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7EF2GDqNJmA"
      },
      "source": [
        "### Cargamos los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdNLHsqfNJmA"
      },
      "source": [
        "# Configuración\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import Perceptron\n",
        "#from keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "task1 = 402 # SE PUEDE CAMBIAR\n",
        "task2 = 404 # SE PUEDE CAMBIAR\n",
        "task_OneHotEnconding = {402: [1.,0.], 404: [0.,1.]}\n",
        "user = 'W29' # SE PUEDE CAMBIAR\n",
        "day = '0329'\n",
        "folder_day = 'W29-29_03_2021'\n",
        "total_records = 22 # CAMBIAR SI HAY MAS REGISTROS\n",
        "fm = 200\n",
        "electrodes_names_selected = ['F3', 'FZ', 'FC1','FCZ','C1','CZ','CP1','CPZ', 'FC5', 'FC3','C5','C3','CP5','CP3','P3',\n",
        "                             'PZ','F4','FC2','FC4','FC6','C2','C4','CP2','CP4','C6','CP6','P4','HR' ,'HL', 'VU', 'VD']\n",
        "number_channels = len(electrodes_names_selected)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW2fa7fbNJmB",
        "outputId": "fab928ce-b9d5-4dd0-dd62-0e5c0b934a52"
      },
      "source": [
        "lTaskData = []\n",
        "total_records_used = 0\n",
        "for i_rec in range(1,total_records+1):\n",
        "    i_rec_record = i_rec\n",
        "    if i_rec_record <10:\n",
        "        i_rec_record = \"0\"+str(i_rec_record)\n",
        "    if i_rec % 2 == 0: # Registros impares primero: USUARIO SIN MOVIMIENTO SOLO PENSANDO\n",
        "        record = \"./RegistrosProcesados2/W29_2021\"+day+\"_openloop_\"+str(i_rec_record)+\"_processed.mat\"\n",
        "        output = read_outputs(record) # output.task será y, output.data será x\n",
        "\n",
        "\n",
        "        output.task = np.transpose(output.task)\n",
        "        output.data = output.data.reshape((np.shape(output.data)[0],np.shape(output.data)[1]))\n",
        "        output.data = np.transpose(output.data)\n",
        "        #output.data = output.data.reshape((np.shape(output.data)[0],np.shape(output.data)[1],1))\n",
        "\n",
        "        outT = (output.task == task1) | (output.task == task2)\n",
        "        outData = output.data[0:np.shape(output.data)[0], outT[0,:]]\n",
        "        outTask = output.task[0, outT[0,:]]\n",
        "        outTD = OutTaskData(outTask, outData)\n",
        "\n",
        "        lTaskData.append(outTD)\n",
        "        total_records_used+=1\n",
        "print(total_records_used, total_records)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqrivmkzNJmC",
        "outputId": "9af45cd2-4229-417f-db8a-0f97060e6948"
      },
      "source": [
        "# Vamos a coger 2 registros para el entrenamiento, 1 para el conjunto dev set, 1 para el test set\n",
        "X_train, y_train, X_dev, y_dev, X_test, y_test = [],[],[],[],[],[] \n",
        "for j in range(0,total_records_used-3): # Cogemos 18 registros para entrenamiento\n",
        "    X_train.append(lTaskData[j].data)\n",
        "    y_train.append(lTaskData[j].task)\n",
        "\n",
        "for j in range(total_records_used-3,total_records_used-1): # Cogemos 2 registros para el dev set\n",
        "    X_dev.append(lTaskData[j].data)\n",
        "    y_dev.append(lTaskData[j].task)\n",
        "for j in range(total_records_used-1,total_records_used): # Cogemos 2 registros para el test set\n",
        "    X_test.append(lTaskData[j].data)\n",
        "    y_test.append(lTaskData[j].task)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "#y_train = np.ravel(np.array(y_train))\n",
        "y_train = np.array(y_train)\n",
        "X_dev = np.array(X_dev)\n",
        "#y_dev = np.ravel(np.array(y_dev))\n",
        "y_dev = np.array(y_dev)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "#y_test = np.ravel(np.array(y_test))\n",
        "\n",
        "print (\"X_train:\",X_train.shape)\n",
        "print (\"y_train:\",y_train.shape)\n",
        "print (\"X_dev:\",X_dev.shape)\n",
        "print (\"y_dev:\",y_dev.shape)\n",
        "print (\"X_test:\",X_test.shape)\n",
        "print (\"y_test:\",y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# VENTANEO Y ONE HOT ENCODING \n",
        "window = 5\n",
        "samples_advance = 3\n",
        "\n",
        "# Ventaneo X_train\n",
        "\n",
        "X_train_l = []\n",
        "y_train_l = []\n",
        "for num_X_train in range(np.shape(X_train)[0]): # Para no mezclar registros\n",
        "    win_init = int(0)\n",
        "    window_position = 0\n",
        "    \n",
        "    for i in range(np.shape(X_train)[2]): # For each signal registered\n",
        "        win_end = int(win_init + window)\n",
        "        if win_end >= np.shape(X_train)[2]:\n",
        "            break\n",
        "\n",
        "        task = np.unique(y_train[num_X_train,win_init:win_end])\n",
        "\n",
        "        if len(task)==1:\n",
        "        #if task1 in task or task2 in task:\n",
        "            signal_window = X_train[num_X_train, :, win_init:win_end]\n",
        "            \n",
        "            #data_filtered = preprocessing(signal_window, fm, number_channels)\n",
        "            #X_train_l.append(data_filtered)\n",
        "            X_train_l.append(signal_window)\n",
        "            taskOH = task_OneHotEnconding[task[0]]\n",
        "            y_train_l.append(taskOH)\n",
        "            #y_train_l.append(task)\n",
        "            \n",
        "            \n",
        "        win_init += int(samples_advance)\n",
        "\n",
        "X_train_l = np.array(X_train_l)\n",
        "y_train_l = np.array(y_train_l)\n",
        "\n",
        "\n",
        "# Ventaneo X_dev\n",
        "X_dev_l = []\n",
        "y_dev_l = []\n",
        "for num_X_dev in range(np.shape(X_dev)[0]):\n",
        "    win_init = int(0)\n",
        "    window_position = 0\n",
        "    \n",
        "    for i in range(np.shape(X_dev)[2]): # For each signal registered\n",
        "        win_end = int(win_init + window)\n",
        "        if win_end >= np.shape(X_dev)[2]:\n",
        "            break\n",
        "\n",
        "        task = np.unique(y_dev[num_X_dev,win_init:win_end])\n",
        "\n",
        "        if len(task)==1:\n",
        "        #if task1 in task or task2 in task:\n",
        "            signal_window = X_dev[num_X_dev, :, win_init:win_end]\n",
        "            \n",
        "            #data_filtered = preprocessing(signal_window, fm, number_channels)\n",
        "            #X_train_l.append(data_filtered)\n",
        "            X_dev_l.append(signal_window)\n",
        "            taskOH = task_OneHotEnconding[task[0]]\n",
        "            y_dev_l.append(taskOH)\n",
        "            #y_dev_l.append(task)\n",
        "            \n",
        "        win_init += int(samples_advance)\n",
        "\n",
        "X_dev_l = np.array(X_dev_l)\n",
        "y_dev_l = np.array(y_dev_l)\n",
        "\n",
        "# Ventaneo X_test\n",
        "X_test_l = []\n",
        "y_test_l = []\n",
        "for num_X_test in range(np.shape(X_test)[0]): \n",
        "    win_init = int(0)\n",
        "    window_position = 0\n",
        "    \n",
        "    for i in range(np.shape(X_test)[2]): # For each signal registered\n",
        "        win_end = int(win_init + window)\n",
        "        if win_end >= np.shape(X_test)[2]:\n",
        "            break\n",
        "\n",
        "        task = np.unique(y_test[num_X_test,win_init:win_end])\n",
        "\n",
        "        if len(task)==1:\n",
        "        #if task1 in task or task2 in task:\n",
        "            signal_window = X_test[num_X_test, :, win_init:win_end]\n",
        "            \n",
        "            #data_filtered = preprocessing(signal_window, fm, number_channels)\n",
        "            #X_train_l.append(data_filtered)\n",
        "            X_test_l.append(signal_window)\n",
        "            taskOH = task_OneHotEnconding[task[0]]\n",
        "            #y_test_l.append(task)\n",
        "            y_test_l.append(taskOH)\n",
        "            \n",
        "        win_init += int(samples_advance)\n",
        "\n",
        "X_test_l = np.array(X_test_l)\n",
        "y_test_l = np.array(y_test_l)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train_l = X_train_l.reshape((np.shape(X_train_l)[0],np.shape(X_train_l)[1],np.shape(X_train_l)[2], 1))\n",
        "X_dev_l = X_dev_l.reshape((np.shape(X_dev_l)[0],np.shape(X_dev_l)[1],np.shape(X_dev_l)[2], 1))\n",
        "X_test_l = X_test_l.reshape((np.shape(X_test_l)[0],np.shape(X_test_l)[1],np.shape(X_test_l)[2], 1))\n",
        "\n",
        "print()\n",
        "print(\"ONE HOT ENCODER & WINDOWING:\")\n",
        "print (\"X_train:\",X_train_l.shape)\n",
        "print (\"y_train:\",y_train_l.shape)\n",
        "print (\"X_dev:\",X_dev_l.shape)\n",
        "print (\"y_dev:\",y_dev_l.shape)\n",
        "print (\"X_test:\",X_test_l.shape)\n",
        "print (\"y_test:\",y_test_l.shape)\n",
        "\n",
        "X_train = X_train_l\n",
        "y_train = y_train_l\n",
        "X_dev = X_dev_l\n",
        "y_dev = y_dev_l\n",
        "X_test = X_test_l\n",
        "y_test = y_test_l\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_dev = X_dev.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "y_train = y_train.astype('float32')\n",
        "y_dev = y_dev.astype('float32')\n",
        "y_test = y_test.astype('float32')\n",
        "print()\n",
        "print(\"RESHAPE:\")\n",
        "print (\"X_train:\",X_train_l.shape)\n",
        "print (\"y_train:\",y_train_l.shape)\n",
        "print (\"X_dev:\",X_dev_l.shape)\n",
        "print (\"y_dev:\",y_dev_l.shape)\n",
        "print (\"X_test:\",X_test_l.shape)\n",
        "print (\"y_test:\",y_test_l.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train: (8, 32, 49)\n",
            "y_train: (8, 49)\n",
            "X_dev: (2, 32, 49)\n",
            "y_dev: (2, 49)\n",
            "X_test: (1, 32, 49)\n",
            "y_test: (1, 49)\n",
            "\n",
            "ONE HOT ENCODER & WINDOWING:\n",
            "X_train: (104, 32, 5, 1)\n",
            "y_train: (104, 2)\n",
            "X_dev: (26, 32, 5, 1)\n",
            "y_dev: (26, 2)\n",
            "X_test: (13, 32, 5, 1)\n",
            "y_test: (13, 2)\n",
            "\n",
            "RESHAPE:\n",
            "X_train: (104, 32, 5, 1)\n",
            "y_train: (104, 2)\n",
            "X_dev: (26, 32, 5, 1)\n",
            "y_dev: (26, 2)\n",
            "X_test: (13, 32, 5, 1)\n",
            "y_test: (13, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gQbqp_NJmG"
      },
      "source": [
        "### 4 - Construcción de la Capsnet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "tDzWsLaYNJmG",
        "outputId": "abf9dcb3-871e-4afb-f672-0374bacdb4f2"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import csv\n",
        "import math\n",
        "import pandas\n",
        "\"\"\"\n",
        "def plot_log(filename, show=True):\n",
        "\n",
        "    data = pandas.read_csv(filename)\n",
        "\n",
        "    fig = plt.figure(figsize=(4,6))\n",
        "    fig.subplots_adjust(top=0.95, bottom=0.05, right=0.95)\n",
        "    fig.add_subplot(211)\n",
        "    for key in data.keys():\n",
        "        if key.find('loss') >= 0 and not key.find('val') >= 0:  # training loss\n",
        "            plt.plot(data['epoch'].values, data[key].values, label=key)\n",
        "    plt.legend()\n",
        "    plt.title('Training loss')\n",
        "\n",
        "    fig.add_subplot(212)\n",
        "    for key in data.keys():\n",
        "        if key.find('acc') >= 0:  # acc\n",
        "            plt.plot(data['epoch'].values, data[key].values, label=key)\n",
        "    plt.legend()\n",
        "    plt.title('Training and validation accuracy')\n",
        "\n",
        "    # fig.savefig('result/log.png')\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def combine_images(generated_images, height=None, width=None):\n",
        "    num = generated_images.shape[0]\n",
        "    if width is None and height is None:\n",
        "        width = int(math.sqrt(num))\n",
        "        height = int(math.ceil(float(num)/width))\n",
        "    elif width is not None and height is None:  # height not given\n",
        "        height = int(math.ceil(float(num)/width))\n",
        "    elif height is not None and width is None:  # width not given\n",
        "        width = int(math.ceil(float(num)/height))\n",
        "\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    plot_log('result_/sub_dependent_/log_fold.csv')\n",
        "\"\"\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef plot_log(filename, show=True):\\n\\n    data = pandas.read_csv(filename)\\n\\n    fig = plt.figure(figsize=(4,6))\\n    fig.subplots_adjust(top=0.95, bottom=0.05, right=0.95)\\n    fig.add_subplot(211)\\n    for key in data.keys():\\n        if key.find(\\'loss\\') >= 0 and not key.find(\\'val\\') >= 0:  # training loss\\n            plt.plot(data[\\'epoch\\'].values, data[key].values, label=key)\\n    plt.legend()\\n    plt.title(\\'Training loss\\')\\n\\n    fig.add_subplot(212)\\n    for key in data.keys():\\n        if key.find(\\'acc\\') >= 0:  # acc\\n            plt.plot(data[\\'epoch\\'].values, data[key].values, label=key)\\n    plt.legend()\\n    plt.title(\\'Training and validation accuracy\\')\\n\\n    # fig.savefig(\\'result/log.png\\')\\n    if show:\\n        plt.show()\\n\\n\\ndef combine_images(generated_images, height=None, width=None):\\n    num = generated_images.shape[0]\\n    if width is None and height is None:\\n        width = int(math.sqrt(num))\\n        height = int(math.ceil(float(num)/width))\\n    elif width is not None and height is None:  # height not given\\n        height = int(math.ceil(float(num)/width))\\n    elif height is not None and width is None:  # width not given\\n        width = int(math.ceil(float(num)/height))\\n\\n    shape = generated_images.shape[1:3]\\n    image = np.zeros((height*shape[0], width*shape[1]),\\n                     dtype=generated_images.dtype)\\n    for index, img in enumerate(generated_images):\\n        i = int(index/width)\\n        j = index % width\\n        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] =             img[:, :, 0]\\n    return image\\n\\nif __name__==\"__main__\":\\n    plot_log(\\'result_/sub_dependent_/log_fold.csv\\')\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KvwXwu0cNJmH",
        "outputId": "047dc529-4754-40aa-f9be-ee5c3a2c454f"
      },
      "source": [
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras import initializers, layers, regularizers\n",
        "\n",
        "\n",
        "class Length(layers.Layer):\n",
        "    \"\"\"\n",
        "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
        "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
        "    inputs: shape=[None, num_vectors, dim_vector]\n",
        "    output: shape=[None, num_vectors]\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Length, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "class Mask(layers.Layer):\n",
        "    \"\"\"\n",
        "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
        "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
        "    masked Tensor.\n",
        "    For example:\n",
        "        ```\n",
        "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
        "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
        "        out = Mask()(x)  # out.shape=[8, 6]\n",
        "        # or\n",
        "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
        "        ```\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if isinstance(inputs, list):  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
        "            assert len(inputs) == 2\n",
        "            inputs, mask = inputs\n",
        "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
        "            # compute lengths of capsules\n",
        "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
        "            # generate the mask which is a one-hot code.\n",
        "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
        "            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n",
        "\n",
        "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
        "        # mask.shape=[None, num_capsule]\n",
        "        # masked.shape=[None, num_capsule * dim_capsule]\n",
        "        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
        "        return masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if isinstance(input_shape[0], tuple):  # true label provided\n",
        "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
        "        else:  # no true label provided\n",
        "            return tuple([None, input_shape[1] * input_shape[2]])\n",
        "\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param vectors: some vectors to be squashed, N-dim tensor\n",
        "    :param axis: the axis to squash\n",
        "    :return: a Tensor with same shape as input vectors\n",
        "    \"\"\"\n",
        "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
        "    return scale * vectors\n",
        "\n",
        "\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
        "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
        "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
        "    \n",
        "    :param num_capsule: number of capsules in this layer\n",
        "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
        "    :param routings: number of iterations for the routing algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self, num_capsule, dim_capsule, routings,lam_regularize,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.lam_regularize = lam_regularize\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "\n",
        "        # Transform matrix\n",
        "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
        "                                        self.dim_capsule, self.input_dim_capsule],\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 regularizer=regularizers.l2(self.lam_regularize),\n",
        "                                 name='W')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
        "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
        "        #inputs_expand = K.expand_dims(inputs, 1)   SANDRA\n",
        "        inputs_expand = tf.expand_dims(inputs, 1)\n",
        "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
        "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
        "        #inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1]) SANDRA\n",
        "        inputs_tiled  = tf.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
        "        inputs_tiled  = tf.expand_dims(inputs_tiled, 4)\n",
        "        print(\"inputs_tiled\",np.shape(inputs_tiled))\n",
        "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
        "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
        "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
        "        # Regard the first two dimensions as `batch` dimension,\n",
        "        # then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
        "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
        "        \n",
        "        #inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs_tiled) SANDRA\n",
        "        inputs_hat = tf.map_fn(lambda x: tf.matmul(self.W, x), elems=inputs_tiled)\n",
        "        \n",
        "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
        "        #b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule]) SANDRA\n",
        "        b = tf.zeros(shape=[tf.shape(inputs_hat)[0], self.num_capsule, \n",
        "                      self.input_num_capsule, 1, 1])\n",
        "        \n",
        "        assert self.routings > 0, 'The routings should be > 0.'\n",
        "        for i in range(self.routings):\n",
        "            # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
        "            #c = tf.nn.softmax(b, dim=1)\n",
        "            # c =tf.compat.v1.math.softmax(b, axis = 1) SANDRA\n",
        "            c = layers.Softmax(axis=1)(b)\n",
        "            print(\"c\",np.shape(c))\n",
        "            \n",
        "\n",
        "            # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
        "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "            # The first two dimensions as `batch` dimension,\n",
        "            # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
        "            # outputs.shape=[None, num_capsule, dim_capsule]\n",
        "            #dot_c = K.batch_dot(c, inputs_hat, [2,2]) sandra\n",
        "            #print(\"dot_c\",np.shape(dot_c)) sandra\n",
        "\n",
        "            #outputs = squash(dot_c)  # [None, 10, 16] # sandra\n",
        "            outputs = tf.multiply(c, inputs_hat)\n",
        "            outputs = tf.reduce_sum(outputs, axis=2, keepdims=True)\n",
        "            outputs = squash(outputs, axis=-2)  # [None, 10, 1, 16, 1]\n",
        "      \n",
        "            if i < self.routings - 1:\n",
        "                # outputs.shape =  [None, num_capsule, dim_capsule]\n",
        "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "                # The first two dimensions as `batch` dimension,\n",
        "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
        "                # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
        "                # b += K.batch_dot(outputs, inputs_hat, [2, 3]) SANDRA\n",
        "                outputs_tiled = tf.tile(outputs, [1, 1, self.input_num_capsule, 1, 1])\n",
        "                agreement = tf.matmul(inputs_hat, outputs_tiled, transpose_a=True)\n",
        "                b = tf.add(b, agreement)\n",
        "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
        "        # Squeeze the outputs to remove useless axis:\n",
        "        #  From  --> outputs.shape=[None, num_capsule, 1, dim_capsule, 1]\n",
        "        #  To    --> outputs.shape=[None, num_capsule,    dim_capsule]\n",
        "        outputs = tf.squeeze(outputs, [2, 4])\n",
        "        return outputs\n",
        "    \n",
        "    \"\"\"\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'num_capsule': self.num_capsule,\n",
        "            'dim_capsule': self.dim_capsule,\n",
        "            'routings': self.routings\n",
        "        }\n",
        "        base_config = super(CapsuleLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "    \"\"\"\n",
        "\"\"\"\n",
        "class channel_attention(layers.Layer):\n",
        "\n",
        "    def __init__(self, weight_decay=0.00000004, scope=\"\", reuse=None,**kwargs):  #deap H=120,W=24,C=256; dreamer H=123,W=9,C=256\n",
        "        super(channel_attention, self).__init__(**kwargs)\n",
        "\n",
        "        self.weight_decay = weight_decay\n",
        "        self.scope = scope\n",
        "        self.reuse = reuse\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.H = input_shape[1]\n",
        "        self.W = input_shape[2]\n",
        "        self.C = input_shape[3]\n",
        "        self.w_c = self.add_weight(name=\"w_c\",\n",
        "                                   shape=[self.C, self.C],\n",
        "                                   dtype=tf.float32,\n",
        "                                   initializer=tf.orthogonal_initializer(),\n",
        "                                   regularizer=tf.contrib.layers.l1_regularizer(self.weight_decay))\n",
        "\n",
        "        self.b_c = self.add_weight(name=\"b_c\",\n",
        "                                   shape=[self.C],\n",
        "                                   dtype=tf.float32,\n",
        "                                   initializer=tf.zeros_initializer())\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        feature_map = inputs\n",
        "\n",
        "        transpose_feature_map = tf.transpose(tf.reduce_mean(feature_map, [1, 2], keep_dims=True),\n",
        "                                             perm=[0, 3, 1, 2])\n",
        "        channel_wise_attention_fm = tf.matmul(tf.reshape(transpose_feature_map,\n",
        "                                                         [-1, self.C]), self.w_c) + self.b_c\n",
        "        channel_wise_attention_fm = tf.nn.sigmoid(channel_wise_attention_fm)\n",
        "        #         channel_wise_attention_fm = tf.clip_by_value(tf.nn.relu(channel_wise_attention_fm),\n",
        "        #                                                      clip_value_min = 0,\n",
        "        #                                                      clip_value_max = 1)\n",
        "        attention = tf.reshape(tf.concat([channel_wise_attention_fm] * (self.H * self.W),\n",
        "                                         axis=1), [-1, self.H, self.W, self.C])\n",
        "        attended_fm = attention * feature_map\n",
        "        return attended_fm\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "class spatial_attention(layers.Layer):\n",
        "\n",
        "    def __init__(self, weight_decay=0.4, scope=\"\", reuse=None,**kwargs):\n",
        "        super(spatial_attention, self).__init__(**kwargs)\n",
        "        self.weight_decay = weight_decay\n",
        "        self.scope = scope\n",
        "        self.reuse = reuse\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.H = input_shape[1]\n",
        "        self.W = input_shape[2]\n",
        "        self.C = input_shape[3]\n",
        "        self.w_s = self.add_weight(name=\"w_s\",\n",
        "                                   shape=[self.C, 1],\n",
        "                                   dtype=tf.float32,\n",
        "                                   initializer=tf.orthogonal_initializer(),\n",
        "                                   regularizer=tf.contrib.layers.l1_regularizer(self.weight_decay))\n",
        "\n",
        "        self.b_s = self.add_weight(name=\"b_s\",\n",
        "                                   shape=[1],\n",
        "                                   dtype=tf.float32,\n",
        "                                   initializer=tf.zeros_initializer())\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        feature_map = inputs\n",
        "\n",
        "        spatial_attention_fm = tf.matmul(tf.reshape(feature_map, [-1, self.C]), self.w_s) + self.b_s\n",
        "        spatial_attention_fm = tf.nn.sigmoid(tf.reshape(spatial_attention_fm, [-1, self.W * self.H]))\n",
        "        #         spatial_attention_fm = tf.clip_by_value(tf.nn.relu(tf.reshape(spatial_attention_fm,\n",
        "        #                                                                       [-1, W * H])),\n",
        "        #                                                 clip_value_min = 0,\n",
        "        #                                                 clip_value_max = 1)\n",
        "        attention = tf.reshape(tf.concat([spatial_attention_fm] * self.C, axis=1), [-1, self.H, self.W, self.C])\n",
        "        attended_fm = attention * feature_map\n",
        "        return attended_fm\n",
        "\"\"\"\n",
        "\n",
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding, lam_regularize):\n",
        "    \"\"\"\n",
        "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
        "    :param dim_capsule: the dim of the output vector of capsule\n",
        "    :param n_channels: the number of types of capsules\n",
        "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
        "    \"\"\"\n",
        "    outputs = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
        "                           name='primarycap_conv2d',kernel_regularizer= regularizers.l2(lam_regularize))(inputs)\n",
        "    #if model_version == 'v2':     # MLF-CapsNet\n",
        "    #    outputs = layers.concatenate([inputs,outputs],axis=3,\n",
        "    #                                    name='concatenate')\n",
        "\n",
        "    #   outputs = layers.Conv2D(filters=256, kernel_size=1, strides=1, padding='valid',\n",
        "    #                          name='bottleneck_layer')(outputs)\n",
        "    #elif model_version == 'v1':   # MLF-CapsNet without bottleneck layer\n",
        "    #outputs = layers.concatenate([inputs, outputs], axis=3, name='concatenate')\n",
        "\n",
        "\n",
        "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(outputs)#(conca_maps)#(output)\n",
        "    return layers.Lambda(squash, name='primarycap_squash')(outputs)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# The following is another way to implement primary capsule layer. This is much slower.\n",
        "# Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
        "    outputs = []\n",
        "    for _ in range(n_channels):\n",
        "        output = layers.Conv2D(filters=dim_capsule, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
        "        outputs.append(layers.Reshape([output.get_shape().as_list()[1] ** 2, dim_capsule])(output))\n",
        "    outputs = layers.Concatenate(axis=1)(outputs)\n",
        "    return layers.Lambda(squash)(outputs)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "def conca(inputs):\n",
        "    #[a , b] = inputs\n",
        "    conca_maps = K.concatenate(inputs, axis=3)\n",
        "    return conca_maps\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "def add(inputs):\n",
        "    [a, b] = inputs\n",
        "    out = a + b\n",
        "    return out\n",
        "\"\"\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef add(inputs):\\n    [a, b] = inputs\\n    out = a + b\\n    return out\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQvXFC9GNJmK"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras import layers, models, optimizers,regularizers\n",
        "from keras.layers import InputSpec, Dense\n",
        "#from capsulelayers import CapsuleLayer, PrimaryCap, Length\n",
        "\n",
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "def deap_load(data_file,dimention,debaseline):\n",
        "    rnn_suffix = \".mat_win_128_rnn_dataset.pkl\"\n",
        "    label_suffix = \".mat_win_128_labels.pkl\"\n",
        "    arousal_or_valence = dimention\n",
        "    with_or_without = debaseline # 'yes','not'\n",
        "    dataset_dir = \"/home/bsipl_5/experiment/ijcnn-master/deap_shuffled_data/\" + with_or_without + \"_\" + arousal_or_valence + \"/\"\n",
        "\n",
        "    ###load training set\n",
        "    with open(dataset_dir + data_file + rnn_suffix, \"rb\") as fp:\n",
        "        rnn_datasets = pickle.load(fp)\n",
        "    with open(dataset_dir + data_file + label_suffix, \"rb\") as fp:\n",
        "        labels = pickle.load(fp)\n",
        "        labels = np.transpose(labels)\n",
        "\n",
        "    labels = np.asarray(pd.get_dummies(labels), dtype=np.int8)\n",
        "\n",
        "\n",
        "    # shuffle data\n",
        "    index = np.array(range(0, len(labels)))\n",
        "    np.random.shuffle(index)\n",
        "    rnn_datasets = rnn_datasets[index]  # .transpose(0,2,1)\n",
        "    labels = labels[index]\n",
        "\n",
        "    datasets = rnn_datasets.reshape(-1, 128, 32, 1).astype('float32')\n",
        "    labels = labels.astype('float32')\n",
        "\n",
        "    return datasets , labels\n",
        "\n",
        "def dreamer_load(sub,dimention,debaseline):\n",
        "    if debaseline == 'yes':\n",
        "        dataset_suffix = \"f_dataset.pkl\"\n",
        "        label_suffix = \"_labels.pkl\"\n",
        "        dataset_dir = \"/home/bsipl_5/experiment/Data/data_pre(-base)/\" + dimention + \"/\"\n",
        "    else:\n",
        "        dataset_suffix = \"_rnn_dataset.pkl\"\n",
        "        label_suffix = \"_labels.pkl\"\n",
        "        dataset_dir = '/home/bsipl_5/experiment/ijcnn-master/dreamer_shuffled_data/' + 'no_' + dimention + '/'\n",
        "\n",
        "    ###load training set\n",
        "    with open(dataset_dir + sub + dataset_suffix, \"rb\") as fp:\n",
        "        datasets = pickle.load(fp)\n",
        "    with open(dataset_dir + sub + '_' + dimention + label_suffix, \"rb\") as fp:\n",
        "        labels = pickle.load(fp)\n",
        "        labels = np.transpose(labels)\n",
        "\n",
        "    labels = labels > 3\n",
        "    labels = np.asarray(pd.get_dummies(labels), dtype=np.int8)\n",
        "\n",
        "\n",
        "    # shuffle data\n",
        "    index = np.array(range(0, len(labels)))\n",
        "    np.random.shuffle(index)\n",
        "    datasets = datasets[index]  # .transpose(0,2,1)\n",
        "    labels = labels[index]\n",
        "\n",
        "    datasets = datasets.reshape(-1, 128, 14, 1).astype('float32')\n",
        "    labels = labels.astype('float32')\n",
        "\n",
        "    return datasets , labels\n",
        "\"\"\"\n",
        "\n",
        "class CapsToScalars(layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CapsToScalars, self).__init__(**kwargs)\n",
        "        self.input_spec = InputSpec(min_ndim=3)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return K.sqrt(K.sum(K.square(inputs + K.epsilon()), axis=-1))\n",
        "\n",
        "def CapsNet(input_shape, n_class, routings, lam_regularize):\n",
        "    \"\"\"\n",
        "    A Capsule Network .\n",
        "    :param input_shape: data shape, 3d, [width, height, channels]\n",
        "    :param n_class: number of classes\n",
        "    :param routings: number of routing iterations\n",
        "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
        "            `eval_model` can also be used for training.\n",
        "    \"\"\"\n",
        "    x = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Layer 1: Just a conventional Conv2D layer, 对DEAP，kernel_size=9；对DREAMER，kernel_size=6\n",
        "    conv1 = layers.Conv2D(filters=4, kernel_size=3, strides=1, padding='valid', activation='relu', name='conv1',kernel_regularizer=regularizers.l2(lam_regularize))(x) #kernel_size=9\n",
        "\n",
        "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
        "    # 对DEAP，kernel_size=9；对DREAMER，kernel_size=6\n",
        "    # 对CapsNet，strides=2，pading=‘valid’；对MLF-CapsNet，stides=1，padding='same'\n",
        "    primarycaps = PrimaryCap(conv1, dim_capsule=4, n_channels=128, kernel_size=3, strides=2, padding='valid',lam_regularize = lam_regularize)\n",
        "    \n",
        "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
        "    micaps = CapsuleLayer(num_capsule=n_class, dim_capsule=8, routings=routings,\n",
        "                             name='micaps', lam_regularize = lam_regularize)(primarycaps)\n",
        "\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    out_caps = Length(name='capsnet')(micaps)\n",
        "\n",
        "    # Decoder network.\n",
        "    y = layers.Input(shape=(n_class,))\n",
        "    masked_by_y = Mask()([micaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
        "    masked = Mask()(micaps)  # Mask using the capsule with maximal length. For prediction\n",
        "\n",
        "    # Shared Decoder model in training and prediction\n",
        "    decoder = models.Sequential(name='decoder')\n",
        "    decoder.add(layers.Dense(512, activation='relu', input_dim=8*n_class))\n",
        "    decoder.add(layers.Dense(1024, activation='relu'))\n",
        "    decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
        "    decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
        "\n",
        "    # Models for training and evaluation (prediction)\n",
        "    train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
        "    eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
        "    \n",
        "    return train_model, eval_model\n",
        "    \"\"\"\n",
        "    # manipulate model\n",
        "    \n",
        "    noise = layers.Input(shape=(n_class, 8))\n",
        "    noised_micaps = layers.Add()([micaps, noise])\n",
        "    masked_noised_y = Mask()([noised_micaps, y])\n",
        "    manipulate_model = models.Model([x, y, noise], decoder(masked_noised_y))\n",
        "    \n",
        "    return train_model, eval_model, manipulate_model\n",
        "    \"\"\"\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    \"\"\"\n",
        "    out_caps = Length(name='capsnet')(micaps)\n",
        "\n",
        "    \n",
        "\n",
        "    # Models for training and evaluation (prediction)\n",
        "    y = layers.Input(shape=(n_class,))\n",
        "    train_model = models.Model([x, y], out_caps)\n",
        "    eval_model = models.Model(x, out_caps)\n",
        "    \"\"\"\n",
        "    #l = K.sqrt(K.sum(K.square(micaps + K.epsilon()), axis=-1))\n",
        "    \n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    #masked_by_y = Mask_CID()([digits_caps, y])  \n",
        "    #masked = Mask_CID()(digits_caps)\n",
        "    \n",
        "    \n",
        "    #  Decoder Network\n",
        "    print(\"HOLA\")\n",
        "    #model.add(Flatten())\n",
        "    y = tf.keras.Input(shape=(n_class,))\n",
        "    masked_by_y = Mask()([micaps, y])  \n",
        "    masked = Mask()(micaps)\n",
        "    hidden1 = tf.keras.layers.Dense( 512, activation='relu', name=\"hidden1\") (masked_by_y)\n",
        "    hidden2 = tf.keras.layers.Dense(1024, activation='relu', name=\"hidden2\") (hidden1)\n",
        "    decoder_output = tf.keras.layers.Dense(32 * 5, activation='sigmoid', name=\"decoder_output\") (hidden2)\n",
        "    print(\"HOLAAAAAAAAa\")\n",
        "\n",
        "    \n",
        "\n",
        "    train_model = models.Model([x, y], decoder_output)\n",
        "    eval_model = models.Model(x, decoder_output)\n",
        "    train_model.summary()\n",
        "    \n",
        "    # Decoder Network\n",
        "    decoder = models.Sequential(name='decoder')\n",
        "    decoder.add(Dense(512, activation='relu', name=\"hidden1\", output_dim=1024))\n",
        "    decoder.add(Dense(1024, activation='relu', name=\"hidden2\"))\n",
        "    decoder.add(Dense(2 * 5, activation='sigmoid', name=\"decoder_output\"))\n",
        "    \n",
        "    train_model = models.Model([x, y], [m_capsnet.output, decoder(masked_by_y)])\n",
        "    eval_model = models.Model(x, [m_capsnet.output, decoder(masked)])\n",
        "    train_model.summary()\n",
        "    \n",
        "    return train_model, eval_model\n",
        "    \"\"\"\n",
        "def margin_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
        "    :param y_true: [None, n_classes]\n",
        "    :param y_pred: [None, num_capsule]\n",
        "    :return: a scalar loss value.\n",
        "    \"\"\"\n",
        "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
        "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
        "\n",
        "    return K.mean(K.sum(L, 1))\n",
        "\n",
        "def train(model, data, args):\n",
        "    \"\"\"\n",
        "    Training a CapsuleNet\n",
        "    :param model: the CapsuleNet model\n",
        "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
        "    :param args: arguments\n",
        "    :return: The trained model\n",
        "    \"\"\"\n",
        "    # unpacking the data\n",
        "    (X_train, y_train), (X_dev, y_dev) = data\n",
        "\n",
        "    # callbacks\n",
        "    log = callbacks.CSVLogger(args.save_dir + '/' + 'log_fold.csv')\n",
        "    tb = callbacks.TensorBoard(log_dir=args.save_dir + '/tensorboard-logs_fold',\n",
        "                               batch_size=args.batch_size, histogram_freq=args.debug)\n",
        "    checkpoint = callbacks.ModelCheckpoint(args.save_dir + '/weights-{epoch:02d}_fold.h5', monitor='val_acc',\n",
        "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
        "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: args.lr * (1.0 ** epoch))\n",
        "\n",
        "    #EarlyStop = callbacks.EarlyStopping(monitor='val_capsnet_acc', patience=5)\n",
        "    # compile the model\n",
        "    model.compile(optimizer= optimizers.Adam(lr=args.lr),\n",
        "                  loss=margin_loss,\n",
        "                  loss_weights=[1., args.lam_recon],\n",
        "                  metrics={'capsnet': 'accuracy'})\n",
        "\n",
        "    \"\"\"\n",
        "    # Training without data augmentation:\n",
        "    model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs,\n",
        "              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint, lr_decay, EarlyStop])\n",
        "    \"\"\"\n",
        "\n",
        "    '''\n",
        "    # Training with validation set\n",
        "    model.fit([x_train, y_train], y_train ,  batch_size=args.batch_size, epochs=args.epochs,verbose = 1,\n",
        "              validation_split= 0.1 , callbacks=[log, tb, checkpoint, lr_decay])\n",
        "    '''\n",
        "\n",
        "    # Training without validation set\n",
        "    history = model.fit([X_train, y_train],[y_train, X_train], batch_size=args.batch_size, epochs=args.epochs,\n",
        "                callbacks=[log, tb, lr_decay])\n",
        "\n",
        "\n",
        "    #from utils import plot_log\n",
        "    #plot_log(args.save_dir + '/log.csv', show=True)\n",
        "\n",
        "    return history\n",
        "\n",
        "time_start_whole = time.time()\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_5bwR72aNJmL",
        "outputId": "0eaa75b6-32ae-4356-cf93-ebf9e5ba1f5e"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from keras import callbacks\n",
        "from keras.utils.vis_utils import plot_model\n",
        "#from keras.utils import multi_gpu_model\n",
        "\n",
        "# setting the hyper parameters\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"Capsule Network on \" + folder_day)\n",
        "parser.add_argument('--epochs', default=100, type=int)  \n",
        "parser.add_argument('--batch_size', default=20, type=int)\n",
        "parser.add_argument('--lam_regularize', default=0.0, type=float,\n",
        "                    help=\"The coefficient for the regularizers\")\n",
        "parser.add_argument('--lam_recon', default=0.392, type=float,\n",
        "                        help=\"The coefficient for the loss of decoder\")\n",
        "parser.add_argument('-r', '--routings', default=2, type=int,\n",
        "                    help=\"Number of iterations used in routing algorithm. should > 0\")\n",
        "parser.add_argument('--debug', default=0, type=int,\n",
        "                    help=\"Save weights by TensorBoard\")\n",
        "parser.add_argument('--save_dir', default='./result_/sub_dependent_/') # other\n",
        "parser.add_argument('-t', '--testing', action='store_true',\n",
        "                    help=\"Test the trained model on testing dataset\")\n",
        "parser.add_argument('-w', '--weights', default=None,\n",
        "                    help=\"The path of the saved weights. Should be specified when testing\")\n",
        "parser.add_argument('--lr', default=0.00001, type=float,\n",
        "                    help=\"Initial learning rate\")  # v0:0.0001, v2:0.00001\n",
        "\n",
        "parser.add_argument('--gpus', default=2, type=int)\n",
        "\n",
        "#args = parser.parse_args()\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "\n",
        "print(time.asctime(time.localtime(time.time())))\n",
        "print(args)\n",
        "if not os.path.exists(args.save_dir):\n",
        "    os.makedirs(args.save_dir)\n",
        "\n",
        "#if dataset_name == 'dreamer':          # load dreamer data\n",
        "#    datasets,labels = dreamer_load(subject,dimention,debaseline)\n",
        "#else:  # load deap data\n",
        "#    datasets,labels = deap_load(subject,dimention,debaseline)\n",
        "\n",
        "args.save_dir = args.save_dir\n",
        "if not os.path.exists(args.save_dir):\n",
        "    os.makedirs(args.save_dir)\n",
        "\n",
        "#fold = 10\n",
        "#test_accuracy_allfold = np.zeros(shape=[0], dtype=float)\n",
        "#train_used_time_allfold = np.zeros(shape=[0], dtype=float)\n",
        "#test_used_time_allfold = np.zeros(shape=[0], dtype=float)\n",
        "#for curr_fold in range(fold):\n",
        "#fold_size = datasets.shape[0] // fold\n",
        "#indexes_list = [i for i in range(len(datasets))]\n",
        "#indexes = np.array(indexes_list)\n",
        "#split_list = [i for i in range(curr_fold * fold_size, (curr_fold + 1) * fold_size)]\n",
        "#split = np.array(split_list)\n",
        "#x_test = datasets[split]\n",
        "#y_test = labels[split]\n",
        "\n",
        "#split = np.array(list(set(indexes_list) ^ set(split_list)))\n",
        "#x_train = datasets[split]\n",
        "#y_train = labels[split]\n",
        "\n",
        "#train_sample = y_train.shape[0]\n",
        "#print(\"training examples:\", train_sample)\n",
        "#test_sample = y_test.shape[0]\n",
        "#print(\"test examples    :\", test_sample)\n",
        "\n",
        "# define model\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "    model, eval_model = CapsNet(input_shape=X_train.shape[1:],\n",
        "                                                  n_class=len(np.unique(y_train)),\n",
        "                                                  routings=args.routings,\n",
        "                                                  lam_regularize = args.lam_regularize)\n",
        "\n",
        "model.summary()\n",
        "plot_model(model, to_file=args.save_dir+'/model_fold.png', show_shapes=True)\n",
        "\n",
        "# define muti-gpu model\n",
        "#multi_model = multi_gpu_model(model, gpus=args.gpus)\n",
        "\n",
        "# train\n",
        "\n",
        "train_start_time = time.time()\n",
        "#train(model=multi_model, data=((X_train, y_train), (X_dev, y_dev)), args=args,fold=curr_fold)\n",
        "history = train(model, data=((X_train, y_train), (X_dev, y_dev)), args=args)\n",
        "train_used_time = time.time() - train_start_time\n",
        "\n",
        "results=pd.DataFrame(history.history)\n",
        "results.plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.xlabel (\"Epochs\")\n",
        "plt.ylabel (\"Accuracy - Mean Log Loss\")\n",
        "plt.gca().set_ylim(0, 2) # set the vertical range to [0-1]\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "model.save_weights(args.save_dir + '/trained_model_fold.h5')\n",
        "print('Trained model saved to \\'%s/trained_model_fold.h5\\'' % (args.save_dir))\n",
        "print('Train time: ', train_used_time)\n",
        "\n",
        "#test\n",
        "#print('-' * 30 + 'fold  ' + str(curr_fold) + '  Begin: test' + '-' * 30)\n",
        "print('  Begin: test')\n",
        "\n",
        "test_start_time = time.time()\n",
        "y_pred = eval_model.predict(X_dev, batch_size=20)  # batch_size = 100\n",
        "dev_used_time = time.time() - test_start_time\n",
        "dev_acc = np.sum(np.argmax(y_pred, 1) == np.argmax(y_dev, 1)) / y_dev.shape[0]\n",
        "#print('shape of y_pred: ',y_pred.shape[0])\n",
        "#print('y_pred: ', y_pred)\n",
        "#print('y_test: ', y_test)\n",
        "print('(' + time.asctime(time.localtime(time.time())) + ') Dev set acc:', dev_acc, 'Dev set time: ',dev_used_time)\n",
        "#print('-' * 30 + 'fold  ' + str(curr_fold) + '  End: test' + '-' * 30)\n",
        "#test_accuracy_allfold = np.append(test_accuracy_allfold, test_acc_fold)\n",
        "#train_used_time_allfold = np.append(train_used_time_allfold, train_used_time_fold)\n",
        "#test_used_time_allfold = np.append(test_used_time_allfold, test_used_time_fold)\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "###\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "summary = pd.DataFrame({'fold': range(1,fold+1), 'Test accuracy': test_accuracy_allfold, 'train time': train_used_time_allfold, 'test time': test_used_time_allfold})\n",
        "hyperparam = pd.DataFrame({'average acc of 10 folds': np.mean(test_accuracy_allfold), 'average train time of 10 folds': np.mean(train_used_time_allfold), 'average test time of 10 folds': np.mean(test_used_time_allfold),'epochs': args.epochs, 'lr':args.lr, 'batch size': args.batch_size},index=['dimention/sub'])\n",
        "writer = pd.ExcelWriter(args.save_dir + '/'+'summary'+ '_'+subject+'.xlsx')\n",
        "summary.to_excel(writer, 'Result', index=False)\n",
        "hyperparam.to_excel(writer, 'HyperParam', index=False)\n",
        "writer.save()\n",
        "print('10 fold average accuracy: ', np.mean(test_accuracy_allfold))\n",
        "print('10 fold average train time: ', np.mean(train_used_time_allfold))\n",
        "print('10 fold average test time: ', np.mean(test_used_time_allfold))\n",
        "\"\"\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jun 30 18:51:21 2021\n",
            "Namespace(batch_size=20, debug=0, epochs=100, gpus=2, lam_recon=0.392, lam_regularize=0.0, lr=1e-05, routings=2, save_dir='./result_/sub_dependent_/', testing=False, weights=None)\n",
            "inputs_tiled (None, 2, 1792, 4, 1)\n",
            "c (None, 2, 1792, 1, 1)\n",
            "c (None, 2, 1792, 1, 1)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 5, 1)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 30, 3, 4)     40          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "primarycap_conv2d (Conv2D)      (None, 14, 1, 512)   18944       conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "primarycap_reshape (Reshape)    (None, 1792, 4)      0           primarycap_conv2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "primarycap_squash (Lambda)      (None, 1792, 4)      0           primarycap_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "micaps (CapsuleLayer)           (None, 2, 8)         114688      primarycap_squash[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "mask (Mask)                     (None, None)         0           micaps[0][0]                     \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "capsnet (Length)                (None, 2)            0           micaps[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Sequential)            (None, 32, 5, 1)     698016      mask[0][0]                       \n",
            "==================================================================================================\n",
            "Total params: 831,688\n",
            "Trainable params: 831,688\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer CapsuleLayer has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 1/100\n",
            "inputs_tiled (None, 2, 1792, 4, 1)\n",
            "c (None, 2, 1792, 1, 1)\n",
            "c (None, 2, 1792, 1, 1)\n",
            "inputs_tiled (None, 2, 1792, 4, 1)\n",
            "c (None, 2, 1792, 1, 1)\n",
            "c (None, 2, 1792, 1, 1)\n",
            "6/6 [==============================] - 14s 195ms/step - loss: 0.8329 - capsnet_loss: 0.8094 - decoder_loss: 0.0599 - capsnet_accuracy: 0.6034\n",
            "Epoch 2/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.8333 - capsnet_loss: 0.8094 - decoder_loss: 0.0610 - capsnet_accuracy: 0.6163\n",
            "Epoch 3/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.8324 - capsnet_loss: 0.8094 - decoder_loss: 0.0587 - capsnet_accuracy: 0.6328\n",
            "Epoch 4/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 0.8320 - capsnet_loss: 0.8094 - decoder_loss: 0.0577 - capsnet_accuracy: 0.6090\n",
            "Epoch 5/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.8315 - capsnet_loss: 0.8094 - decoder_loss: 0.0562 - capsnet_accuracy: 0.6064\n",
            "Epoch 6/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.8291 - capsnet_loss: 0.8094 - decoder_loss: 0.0503 - capsnet_accuracy: 0.6249\n",
            "Epoch 7/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.8279 - capsnet_loss: 0.8094 - decoder_loss: 0.0472 - capsnet_accuracy: 0.6245\n",
            "Epoch 8/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.8260 - capsnet_loss: 0.8094 - decoder_loss: 0.0426 - capsnet_accuracy: 0.6117\n",
            "Epoch 9/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.8280 - capsnet_loss: 0.8093 - decoder_loss: 0.0478 - capsnet_accuracy: 0.6284\n",
            "Epoch 10/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.8244 - capsnet_loss: 0.8092 - decoder_loss: 0.0390 - capsnet_accuracy: 0.6296\n",
            "Epoch 11/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.8211 - capsnet_loss: 0.8089 - decoder_loss: 0.0310 - capsnet_accuracy: 0.6652\n",
            "Epoch 12/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 0.8228 - capsnet_loss: 0.8087 - decoder_loss: 0.0359 - capsnet_accuracy: 0.6581\n",
            "Epoch 13/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.8189 - capsnet_loss: 0.8084 - decoder_loss: 0.0268 - capsnet_accuracy: 0.6219\n",
            "Epoch 14/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.8188 - capsnet_loss: 0.8079 - decoder_loss: 0.0278 - capsnet_accuracy: 0.6390\n",
            "Epoch 15/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.8173 - capsnet_loss: 0.8073 - decoder_loss: 0.0256 - capsnet_accuracy: 0.5924\n",
            "Epoch 16/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.8131 - capsnet_loss: 0.8064 - decoder_loss: 0.0170 - capsnet_accuracy: 0.5868\n",
            "Epoch 17/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.8110 - capsnet_loss: 0.8053 - decoder_loss: 0.0146 - capsnet_accuracy: 0.5871\n",
            "Epoch 18/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 0.8065 - capsnet_loss: 0.8038 - decoder_loss: 0.0069 - capsnet_accuracy: 0.6055\n",
            "Epoch 19/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.8019 - capsnet_loss: 0.8018 - decoder_loss: 1.4964e-04 - capsnet_accuracy: 0.6177\n",
            "Epoch 20/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.7975 - capsnet_loss: 0.7998 - decoder_loss: -0.0059 - capsnet_accuracy: 0.5963\n",
            "Epoch 21/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.7900 - capsnet_loss: 0.7965 - decoder_loss: -0.0166 - capsnet_accuracy: 0.6243\n",
            "Epoch 22/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.7857 - capsnet_loss: 0.7934 - decoder_loss: -0.0196 - capsnet_accuracy: 0.6186\n",
            "Epoch 23/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 0.7775 - capsnet_loss: 0.7889 - decoder_loss: -0.0290 - capsnet_accuracy: 0.6420\n",
            "Epoch 24/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 0.7689 - capsnet_loss: 0.7838 - decoder_loss: -0.0380 - capsnet_accuracy: 0.6408\n",
            "Epoch 25/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.7601 - capsnet_loss: 0.7786 - decoder_loss: -0.0471 - capsnet_accuracy: 0.6073\n",
            "Epoch 26/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.7477 - capsnet_loss: 0.7713 - decoder_loss: -0.0602 - capsnet_accuracy: 0.6121\n",
            "Epoch 27/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 0.7354 - capsnet_loss: 0.7630 - decoder_loss: -0.0705 - capsnet_accuracy: 0.6067\n",
            "Epoch 28/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 0.7207 - capsnet_loss: 0.7535 - decoder_loss: -0.0837 - capsnet_accuracy: 0.6144\n",
            "Epoch 29/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.7021 - capsnet_loss: 0.7421 - decoder_loss: -0.1021 - capsnet_accuracy: 0.6174\n",
            "Epoch 30/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.6815 - capsnet_loss: 0.7289 - decoder_loss: -0.1211 - capsnet_accuracy: 0.6200\n",
            "Epoch 31/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.6574 - capsnet_loss: 0.7139 - decoder_loss: -0.1442 - capsnet_accuracy: 0.6130\n",
            "Epoch 32/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.6303 - capsnet_loss: 0.6962 - decoder_loss: -0.1682 - capsnet_accuracy: 0.6252\n",
            "Epoch 33/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.5968 - capsnet_loss: 0.6773 - decoder_loss: -0.2052 - capsnet_accuracy: 0.5993\n",
            "Epoch 34/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.5597 - capsnet_loss: 0.6558 - decoder_loss: -0.2452 - capsnet_accuracy: 0.5975\n",
            "Epoch 35/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 0.5191 - capsnet_loss: 0.6313 - decoder_loss: -0.2862 - capsnet_accuracy: 0.6287\n",
            "Epoch 36/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 0.4610 - capsnet_loss: 0.6004 - decoder_loss: -0.3555 - capsnet_accuracy: 0.6380\n",
            "Epoch 37/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.4208 - capsnet_loss: 0.5794 - decoder_loss: -0.4046 - capsnet_accuracy: 0.6064\n",
            "Epoch 38/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.3515 - capsnet_loss: 0.5468 - decoder_loss: -0.4982 - capsnet_accuracy: 0.6159\n",
            "Epoch 39/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 0.2838 - capsnet_loss: 0.5149 - decoder_loss: -0.5894 - capsnet_accuracy: 0.6308\n",
            "Epoch 40/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.2346 - capsnet_loss: 0.4939 - decoder_loss: -0.6614 - capsnet_accuracy: 0.5646\n",
            "Epoch 41/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 0.1541 - capsnet_loss: 0.4607 - decoder_loss: -0.7823 - capsnet_accuracy: 0.5834\n",
            "Epoch 42/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.0683 - capsnet_loss: 0.4284 - decoder_loss: -0.9187 - capsnet_accuracy: 0.6144\n",
            "Epoch 43/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -0.0021 - capsnet_loss: 0.4051 - decoder_loss: -1.0388 - capsnet_accuracy: 0.5653\n",
            "Epoch 44/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -0.0938 - capsnet_loss: 0.3758 - decoder_loss: -1.1980 - capsnet_accuracy: 0.6332\n",
            "Epoch 45/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -0.1906 - capsnet_loss: 0.3495 - decoder_loss: -1.3777 - capsnet_accuracy: 0.6659\n",
            "Epoch 46/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: -0.2895 - capsnet_loss: 0.3257 - decoder_loss: -1.5692 - capsnet_accuracy: 0.6290\n",
            "Epoch 47/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -0.3950 - capsnet_loss: 0.3041 - decoder_loss: -1.7835 - capsnet_accuracy: 0.6396\n",
            "Epoch 48/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -0.4809 - capsnet_loss: 0.2887 - decoder_loss: -1.9632 - capsnet_accuracy: 0.6227\n",
            "Epoch 49/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -0.5857 - capsnet_loss: 0.2735 - decoder_loss: -2.1918 - capsnet_accuracy: 0.6019\n",
            "Epoch 50/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -0.6931 - capsnet_loss: 0.2601 - decoder_loss: -2.4316 - capsnet_accuracy: 0.5957\n",
            "Epoch 51/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -0.8020 - capsnet_loss: 0.2490 - decoder_loss: -2.6812 - capsnet_accuracy: 0.5828\n",
            "Epoch 52/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -0.9195 - capsnet_loss: 0.2367 - decoder_loss: -2.9495 - capsnet_accuracy: 0.6382\n",
            "Epoch 53/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -1.0663 - capsnet_loss: 0.2259 - decoder_loss: -3.2964 - capsnet_accuracy: 0.6624\n",
            "Epoch 54/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: -1.1541 - capsnet_loss: 0.2228 - decoder_loss: -3.5125 - capsnet_accuracy: 0.6132\n",
            "Epoch 55/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -1.2942 - capsnet_loss: 0.2165 - decoder_loss: -3.8537 - capsnet_accuracy: 0.6386\n",
            "Epoch 56/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -1.3978 - capsnet_loss: 0.2154 - decoder_loss: -4.1154 - capsnet_accuracy: 0.6005\n",
            "Epoch 57/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -1.5322 - capsnet_loss: 0.2116 - decoder_loss: -4.4486 - capsnet_accuracy: 0.6328\n",
            "Epoch 58/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -1.6542 - capsnet_loss: 0.2136 - decoder_loss: -4.7649 - capsnet_accuracy: 0.5627\n",
            "Epoch 59/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -1.7743 - capsnet_loss: 0.2118 - decoder_loss: -5.0666 - capsnet_accuracy: 0.5892\n",
            "Epoch 60/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -1.9078 - capsnet_loss: 0.2128 - decoder_loss: -5.4096 - capsnet_accuracy: 0.5639\n",
            "Epoch 61/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -2.0540 - capsnet_loss: 0.2137 - decoder_loss: -5.7849 - capsnet_accuracy: 0.5856\n",
            "Epoch 62/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -2.2245 - capsnet_loss: 0.2128 - decoder_loss: -6.2177 - capsnet_accuracy: 0.6573\n",
            "Epoch 63/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -2.3483 - capsnet_loss: 0.2155 - decoder_loss: -6.5404 - capsnet_accuracy: 0.6189\n",
            "Epoch 64/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -2.4985 - capsnet_loss: 0.2174 - decoder_loss: -6.9282 - capsnet_accuracy: 0.6473\n",
            "Epoch 65/100\n",
            "6/6 [==============================] - 0s 28ms/step - loss: -2.6073 - capsnet_loss: 0.2227 - decoder_loss: -7.2193 - capsnet_accuracy: 0.5767\n",
            "Epoch 66/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -2.7958 - capsnet_loss: 0.2242 - decoder_loss: -7.7042 - capsnet_accuracy: 0.6474\n",
            "Epoch 67/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -2.9238 - capsnet_loss: 0.2258 - decoder_loss: -8.0346 - capsnet_accuracy: 0.6614\n",
            "Epoch 68/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: -3.0522 - capsnet_loss: 0.2315 - decoder_loss: -8.3767 - capsnet_accuracy: 0.6096\n",
            "Epoch 69/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -3.2049 - capsnet_loss: 0.2333 - decoder_loss: -8.7709 - capsnet_accuracy: 0.6496\n",
            "Epoch 70/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -3.3364 - capsnet_loss: 0.2380 - decoder_loss: -9.1183 - capsnet_accuracy: 0.6182\n",
            "Epoch 71/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: -3.4686 - capsnet_loss: 0.2408 - decoder_loss: -9.4627 - capsnet_accuracy: 0.6120\n",
            "Epoch 72/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -3.6232 - capsnet_loss: 0.2463 - decoder_loss: -9.8712 - capsnet_accuracy: 0.6046\n",
            "Epoch 73/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -3.7450 - capsnet_loss: 0.2488 - decoder_loss: -10.1881 - capsnet_accuracy: 0.5953\n",
            "Epoch 74/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: -3.9137 - capsnet_loss: 0.2529 - decoder_loss: -10.6290 - capsnet_accuracy: 0.6194\n",
            "Epoch 75/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -4.0526 - capsnet_loss: 0.2567 - decoder_loss: -10.9931 - capsnet_accuracy: 0.6126\n",
            "Epoch 76/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -4.1672 - capsnet_loss: 0.2608 - decoder_loss: -11.2959 - capsnet_accuracy: 0.5669\n",
            "Epoch 77/100\n",
            "6/6 [==============================] - 0s 28ms/step - loss: -4.3320 - capsnet_loss: 0.2621 - decoder_loss: -11.7196 - capsnet_accuracy: 0.6412\n",
            "Epoch 78/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -4.4369 - capsnet_loss: 0.2658 - decoder_loss: -11.9967 - capsnet_accuracy: 0.5989\n",
            "Epoch 79/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -4.5896 - capsnet_loss: 0.2688 - decoder_loss: -12.3939 - capsnet_accuracy: 0.6307\n",
            "Epoch 80/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -4.7285 - capsnet_loss: 0.2736 - decoder_loss: -12.7607 - capsnet_accuracy: 0.6233\n",
            "Epoch 81/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -4.8581 - capsnet_loss: 0.2754 - decoder_loss: -13.0957 - capsnet_accuracy: 0.6462\n",
            "Epoch 82/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: -4.9914 - capsnet_loss: 0.2791 - decoder_loss: -13.4451 - capsnet_accuracy: 0.6281\n",
            "Epoch 83/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -5.1276 - capsnet_loss: 0.2823 - decoder_loss: -13.8008 - capsnet_accuracy: 0.6456\n",
            "Epoch 84/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -5.2336 - capsnet_loss: 0.2839 - decoder_loss: -14.0752 - capsnet_accuracy: 0.6477\n",
            "Epoch 85/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -5.3622 - capsnet_loss: 0.2870 - decoder_loss: -14.4112 - capsnet_accuracy: 0.6361\n",
            "Epoch 86/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -5.5086 - capsnet_loss: 0.2904 - decoder_loss: -14.7934 - capsnet_accuracy: 0.6695\n",
            "Epoch 87/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -5.6109 - capsnet_loss: 0.2935 - decoder_loss: -15.0621 - capsnet_accuracy: 0.6118\n",
            "Epoch 88/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -5.7222 - capsnet_loss: 0.2957 - decoder_loss: -15.3518 - capsnet_accuracy: 0.6284\n",
            "Epoch 89/100\n",
            "6/6 [==============================] - 0s 28ms/step - loss: -5.8454 - capsnet_loss: 0.2988 - decoder_loss: -15.6741 - capsnet_accuracy: 0.6459\n",
            "Epoch 90/100\n",
            "6/6 [==============================] - 0s 28ms/step - loss: -5.9308 - capsnet_loss: 0.2999 - decoder_loss: -15.8946 - capsnet_accuracy: 0.6108\n",
            "Epoch 91/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -6.0449 - capsnet_loss: 0.3028 - decoder_loss: -16.1931 - capsnet_accuracy: 0.6133\n",
            "Epoch 92/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: -6.1560 - capsnet_loss: 0.3039 - decoder_loss: -16.4794 - capsnet_accuracy: 0.6459\n",
            "Epoch 93/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -6.2451 - capsnet_loss: 0.3069 - decoder_loss: -16.7143 - capsnet_accuracy: 0.5969\n",
            "Epoch 94/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -6.3361 - capsnet_loss: 0.3082 - decoder_loss: -16.9499 - capsnet_accuracy: 0.6273\n",
            "Epoch 95/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -6.4125 - capsnet_loss: 0.3101 - decoder_loss: -17.1496 - capsnet_accuracy: 0.5961\n",
            "Epoch 96/100\n",
            "6/6 [==============================] - 0s 29ms/step - loss: -6.5214 - capsnet_loss: 0.3117 - decoder_loss: -17.4314 - capsnet_accuracy: 0.6263\n",
            "Epoch 97/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: -6.6149 - capsnet_loss: 0.3134 - decoder_loss: -17.6744 - capsnet_accuracy: 0.6246\n",
            "Epoch 98/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: -6.6767 - capsnet_loss: 0.3141 - decoder_loss: -17.8337 - capsnet_accuracy: 0.6132\n",
            "Epoch 99/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: -6.7771 - capsnet_loss: 0.3161 - decoder_loss: -18.0947 - capsnet_accuracy: 0.6118\n",
            "Epoch 100/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: -6.8563 - capsnet_loss: 0.3174 - decoder_loss: -18.3003 - capsnet_accuracy: 0.6319\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nsummary = pd.DataFrame({'fold': range(1,fold+1), 'Test accuracy': test_accuracy_allfold, 'train time': train_used_time_allfold, 'test time': test_used_time_allfold})\\nhyperparam = pd.DataFrame({'average acc of 10 folds': np.mean(test_accuracy_allfold), 'average train time of 10 folds': np.mean(train_used_time_allfold), 'average test time of 10 folds': np.mean(test_used_time_allfold),'epochs': args.epochs, 'lr':args.lr, 'batch size': args.batch_size},index=['dimention/sub'])\\nwriter = pd.ExcelWriter(args.save_dir + '/'+'summary'+ '_'+subject+'.xlsx')\\nsummary.to_excel(writer, 'Result', index=False)\\nhyperparam.to_excel(writer, 'HyperParam', index=False)\\nwriter.save()\\nprint('10 fold average accuracy: ', np.mean(test_accuracy_allfold))\\nprint('10 fold average train time: ', np.mean(train_used_time_allfold))\\nprint('10 fold average test time: ', np.mean(test_used_time_allfold))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5mlzs4mNJmM",
        "outputId": "0fb4def6-5b0a-4062-e551-aac98df5f74b"
      },
      "source": [
        "history.params"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epochs': 100, 'steps': 6, 'verbose': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "r6ONyQF_y_CM",
        "outputId": "7d1a64f3-0a2d-4dad-b40b-73aa9a5179df"
      },
      "source": [
        "results[-1:]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>capsnet_loss</th>\n",
              "      <th>decoder_loss</th>\n",
              "      <th>capsnet_accuracy</th>\n",
              "      <th>lr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>-6.866737</td>\n",
              "      <td>0.318054</td>\n",
              "      <td>-18.328548</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.00001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        loss  capsnet_loss  decoder_loss  capsnet_accuracy       lr\n",
              "99 -6.866737      0.318054    -18.328548          0.615385  0.00001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXlIdxcwzBHO",
        "outputId": "d2c71d27-8d8d-4d42-b240-bfe146753044"
      },
      "source": [
        "print (\"Accuracy for the training set: \", results.values[-1:][0][1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for the training set:  0.3180536925792694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytHVOXIczBpy",
        "outputId": "e7ad6833-7b4d-412f-f7ee-8934f1e8267f"
      },
      "source": [
        "print (\"Accuracy for the development test set: \", results.values[-1:][0][3])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for the development test set:  0.6153846383094788\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}