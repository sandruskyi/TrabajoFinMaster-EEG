{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 4: Clasificación con Capsnet\n",
    "\n",
    "La implementación de esta red Capsnet se ha basado en el código implementado por Aurélien Géron (https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados con la arquitectura anterior: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Resultados:\n",
    "    \n",
    "    TRAIN                   DEV\n",
    "    loss       accuracy     val_loss    val_accuracy\n",
    "       0.870418\t0.807692\t0.951864\t0.730769\n",
    "    \n",
    "Por tanto: \n",
    "\n",
    "    E = 1 - Accuracy\n",
    "    Etrain = 1 - 0.807692 = 0.192308\n",
    "    Etest = 1 - 0.730769 = 0.269231\n",
    "    \n",
    "    Bias = Etrain - Ehuman = 0.192308\n",
    "    Variance = Etest - Etrain = 0.269231 - 0.192308 = 0.076923\n",
    "\n",
    "La varianza se ha reducido muchisimo pero el bias ha aumentado a un 19%. Se tratará de mejorar el bias. Para mejorar esto será necesario añadir más complejidad, elegir una mejor optimización, cambiando la arquitectura (más neuronas, más capas)... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cambios realizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aumenta de 100 a 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuevos resultados: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#Helper libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Signal libraries\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the default graph, in case you re-run this notebook without restarting the kernel:\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Random seeds so that this notebook always produces the same output:\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROutput:\n",
    "    def __init__(self, task, data):\n",
    "        self.task = task\n",
    "        self.data = data\n",
    "        \n",
    "class OutTaskData: \n",
    "    def __init__(self, task, data): \n",
    "        self.task = task\n",
    "        self.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "# Primero leemos los registros\n",
    "def read_outputs(rec):\n",
    "    '''read_outputs(\"userS0091f1.mat\")'''\n",
    "    mat = sio.loadmat(rec)\n",
    "    mdata = mat['session']\n",
    "    val = mdata[0,0]\n",
    "    #output = ROutput(np.array(val[\"task\"]), np.array(val[\"data\"]))\n",
    "    output = ROutput(np.array(val[\"task_EEG_p\"]), np.array(val[\"data_processed_EEG\"]))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import Perceptron\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "task1 = 402 # SE PUEDE CAMBIAR\n",
    "task2 = 404 # SE PUEDE CAMBIAR\n",
    "task_OneHotEnconding = {402: [1.,0.], 404: [0.,1.]}\n",
    "user = 'W29' # SE PUEDE CAMBIAR\n",
    "day = '0329'\n",
    "folder_day = 'W29-29_03_2021'\n",
    "total_records = 22 # CAMBIAR SI HAY MAS REGISTROS\n",
    "fm = 200\n",
    "electrodes_names_selected = ['F3', 'FZ', 'FC1','FCZ','C1','CZ','CP1','CPZ', 'FC5', 'FC3','C5','C3','CP5','CP3','P3',\n",
    "                             'PZ','F4','FC2','FC4','FC6','C2','C4','CP2','CP4','C6','CP6','P4','HR' ,'HL', 'VU', 'VD']\n",
    "number_channels = len(electrodes_names_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 22\n"
     ]
    }
   ],
   "source": [
    "lTaskData = []\n",
    "total_records_used = 0\n",
    "for i_rec in range(1,total_records+1):\n",
    "    i_rec_record = i_rec\n",
    "    if i_rec_record <10:\n",
    "        i_rec_record = \"0\"+str(i_rec_record)\n",
    "    if i_rec % 2 == 0: # Registros impares primero: USUARIO SIN MOVIMIENTO SOLO PENSANDO\n",
    "        record = \"./RegistrosProcesados2/\"+folder_day+\"/W29_2021\"+day+\"_openloop_\"+str(i_rec_record)+\"_processed.mat\"\n",
    "        output = read_outputs(record) # output.task será y, output.data será x\n",
    "\n",
    "\n",
    "        output.task = np.transpose(output.task)\n",
    "        output.data = output.data.reshape((np.shape(output.data)[0],np.shape(output.data)[1]))\n",
    "        output.data = np.transpose(output.data)\n",
    "        #output.data = output.data.reshape((np.shape(output.data)[0],np.shape(output.data)[1],1))\n",
    "\n",
    "        outT = (output.task == task1) | (output.task == task2)\n",
    "        outData = output.data[0:np.shape(output.data)[0], outT[0,:]]\n",
    "        outTask = output.task[0, outT[0,:]]\n",
    "        outTD = OutTaskData(outTask, outData)\n",
    "\n",
    "        lTaskData.append(outTD)\n",
    "        total_records_used+=1\n",
    "print(total_records_used, total_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (8, 32, 49)\n",
      "y_train: (8, 49)\n",
      "X_dev: (2, 32, 49)\n",
      "y_dev: (2, 49)\n",
      "X_test: (1, 32, 49)\n",
      "y_test: (1, 49)\n",
      "\n",
      "ONE HOT ENCODER & WINDOWING:\n",
      "X_train: (104, 32, 5, 1)\n",
      "y_train: (104, 2)\n",
      "X_dev: (26, 32, 5, 1)\n",
      "y_dev: (26, 2)\n",
      "X_test: (13, 32, 5, 1)\n",
      "y_test: (13, 2)\n"
     ]
    }
   ],
   "source": [
    "# Vamos a coger 2 registros para el entrenamiento, 1 para el conjunto dev set, 1 para el test set\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test = [],[],[],[],[],[] \n",
    "for j in range(0,total_records_used-3): # Cogemos 18 registros para entrenamiento\n",
    "    X_train.append(lTaskData[j].data)\n",
    "    y_train.append(lTaskData[j].task)\n",
    "\n",
    "for j in range(total_records_used-3,total_records_used-1): # Cogemos 2 registros para el dev set\n",
    "    X_dev.append(lTaskData[j].data)\n",
    "    y_dev.append(lTaskData[j].task)\n",
    "for j in range(total_records_used-1,total_records_used): # Cogemos 2 registros para el test set\n",
    "    X_test.append(lTaskData[j].data)\n",
    "    y_test.append(lTaskData[j].task)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "#y_train = np.ravel(np.array(y_train))\n",
    "y_train = np.array(y_train)\n",
    "X_dev = np.array(X_dev)\n",
    "#y_dev = np.ravel(np.array(y_dev))\n",
    "y_dev = np.array(y_dev)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "#y_test = np.ravel(np.array(y_test))\n",
    "\n",
    "print (\"X_train:\",X_train.shape)\n",
    "print (\"y_train:\",y_train.shape)\n",
    "print (\"X_dev:\",X_dev.shape)\n",
    "print (\"y_dev:\",y_dev.shape)\n",
    "print (\"X_test:\",X_test.shape)\n",
    "print (\"y_test:\",y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# VENTANEO Y ONE HOT ENCODING \n",
    "window = 5\n",
    "samples_advance = 3\n",
    "\n",
    "# Ventaneo X_train\n",
    "\n",
    "X_train_l = []\n",
    "y_train_l = []\n",
    "for num_X_train in range(np.shape(X_train)[0]): # Para no mezclar registros\n",
    "    win_init = int(0)\n",
    "    window_position = 0\n",
    "    \n",
    "    for i in range(np.shape(X_train)[2]): # For each signal registered\n",
    "        win_end = int(win_init + window)\n",
    "        if win_end >= np.shape(X_train)[2]:\n",
    "            break\n",
    "\n",
    "        task = np.unique(y_train[num_X_train,win_init:win_end])\n",
    "\n",
    "        if len(task)==1:\n",
    "        #if task1 in task or task2 in task:\n",
    "            signal_window = X_train[num_X_train, :, win_init:win_end]\n",
    "            \n",
    "            #data_filtered = preprocessing(signal_window, fm, number_channels)\n",
    "            #X_train_l.append(data_filtered)\n",
    "            X_train_l.append(signal_window)\n",
    "            taskOH = task_OneHotEnconding[task[0]]\n",
    "            y_train_l.append(taskOH)\n",
    "            \n",
    "        win_init += int(samples_advance)\n",
    "\n",
    "X_train_l = np.array(X_train_l)\n",
    "y_train_l = np.array(y_train_l)\n",
    "\n",
    "\n",
    "# Ventaneo X_dev\n",
    "X_dev_l = []\n",
    "y_dev_l = []\n",
    "for num_X_dev in range(np.shape(X_dev)[0]):\n",
    "    win_init = int(0)\n",
    "    window_position = 0\n",
    "    \n",
    "    for i in range(np.shape(X_dev)[2]): # For each signal registered\n",
    "        win_end = int(win_init + window)\n",
    "        if win_end >= np.shape(X_dev)[2]:\n",
    "            break\n",
    "\n",
    "        task = np.unique(y_dev[num_X_dev,win_init:win_end])\n",
    "\n",
    "        if len(task)==1:\n",
    "        #if task1 in task or task2 in task:\n",
    "            signal_window = X_dev[num_X_dev, :, win_init:win_end]\n",
    "            \n",
    "            #data_filtered = preprocessing(signal_window, fm, number_channels)\n",
    "            #X_train_l.append(data_filtered)\n",
    "            X_dev_l.append(signal_window)\n",
    "            taskOH = task_OneHotEnconding[task[0]]\n",
    "            y_dev_l.append(taskOH)\n",
    "            \n",
    "        win_init += int(samples_advance)\n",
    "\n",
    "X_dev_l = np.array(X_dev_l)\n",
    "y_dev_l = np.array(y_dev_l)\n",
    "\n",
    "# Ventaneo X_test\n",
    "X_test_l = []\n",
    "y_test_l = []\n",
    "for num_X_test in range(np.shape(X_test)[0]): \n",
    "    win_init = int(0)\n",
    "    window_position = 0\n",
    "    \n",
    "    for i in range(np.shape(X_test)[2]): # For each signal registered\n",
    "        win_end = int(win_init + window)\n",
    "        if win_end >= np.shape(X_test)[2]:\n",
    "            break\n",
    "\n",
    "        task = np.unique(y_test[num_X_test,win_init:win_end])\n",
    "\n",
    "        if len(task)==1:\n",
    "        #if task1 in task or task2 in task:\n",
    "            signal_window = X_test[num_X_test, :, win_init:win_end]\n",
    "            \n",
    "            #data_filtered = preprocessing(signal_window, fm, number_channels)\n",
    "            #X_train_l.append(data_filtered)\n",
    "            X_test_l.append(signal_window)\n",
    "            taskOH = task_OneHotEnconding[task[0]]\n",
    "            y_test_l.append(taskOH)\n",
    "            \n",
    "        win_init += int(samples_advance)\n",
    "\n",
    "X_test_l = np.array(X_test_l)\n",
    "y_test_l = np.array(y_test_l)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_l = X_train_l.reshape((np.shape(X_train_l)[0],np.shape(X_train_l)[1],np.shape(X_train_l)[2], 1))\n",
    "X_dev_l = X_dev_l.reshape((np.shape(X_dev_l)[0],np.shape(X_dev_l)[1],np.shape(X_dev_l)[2], 1))\n",
    "X_test_l = X_test_l.reshape((np.shape(X_test_l)[0],np.shape(X_test_l)[1],np.shape(X_test_l)[2], 1))\n",
    "\n",
    "print()\n",
    "print(\"ONE HOT ENCODER & WINDOWING:\")\n",
    "print (\"X_train:\",X_train_l.shape)\n",
    "print (\"y_train:\",y_train_l.shape)\n",
    "print (\"X_dev:\",X_dev_l.shape)\n",
    "print (\"y_dev:\",y_dev_l.shape)\n",
    "print (\"X_test:\",X_test_l.shape)\n",
    "print (\"y_test:\",y_test_l.shape)\n",
    "\n",
    "X_train = X_train_l\n",
    "y_train = y_train_l\n",
    "X_dev = X_dev_l\n",
    "y_dev = y_dev_l\n",
    "X_test = X_test_l\n",
    "y_test = y_test_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTS = X_train.shape[1]\n",
    "OUTPUTS = y_train.shape[1]\n",
    "NUM_TRAINING_EXAMPLES = int(round(X_train.shape[0]/1))\n",
    "NUM_DEV_EXAMPLES = int(round(y_train.shape[0]/1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data is displayed to test correctness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-1.22062442],\n",
       "         [-1.13443752],\n",
       "         [-1.04225704],\n",
       "         [-0.96348194],\n",
       "         [-1.20483816]],\n",
       "\n",
       "        [[-1.08529571],\n",
       "         [-1.04785683],\n",
       "         [-0.95398982],\n",
       "         [-0.95791722],\n",
       "         [-1.07689992]],\n",
       "\n",
       "        [[-1.07181227],\n",
       "         [-1.04773374],\n",
       "         [-1.20150893],\n",
       "         [-1.0816844 ],\n",
       "         [-1.08110163]],\n",
       "\n",
       "        [[-0.8616019 ],\n",
       "         [-0.80683011],\n",
       "         [-0.77630309],\n",
       "         [-0.89358306],\n",
       "         [-0.93379216]],\n",
       "\n",
       "        [[-0.69940496],\n",
       "         [-0.75200047],\n",
       "         [-1.05191295],\n",
       "         [-1.05801051],\n",
       "         [-0.93926883]],\n",
       "\n",
       "        [[-1.06729164],\n",
       "         [-1.01225542],\n",
       "         [-0.91299989],\n",
       "         [-0.81540926],\n",
       "         [-0.64559539]],\n",
       "\n",
       "        [[-0.62225851],\n",
       "         [-0.69906135],\n",
       "         [-0.65192006],\n",
       "         [-0.7120623 ],\n",
       "         [-0.72689519]],\n",
       "\n",
       "        [[-0.95355303],\n",
       "         [-0.93083994],\n",
       "         [-0.8727538 ],\n",
       "         [-0.86674729],\n",
       "         [-0.90070152]],\n",
       "\n",
       "        [[-1.05570906],\n",
       "         [-1.02544838],\n",
       "         [-1.10623477],\n",
       "         [-1.3027781 ],\n",
       "         [-1.17957064]],\n",
       "\n",
       "        [[-1.15144562],\n",
       "         [-1.01064887],\n",
       "         [-0.88852596],\n",
       "         [-0.9067138 ],\n",
       "         [-0.92107661]],\n",
       "\n",
       "        [[-0.71754111],\n",
       "         [-0.73749487],\n",
       "         [-0.7816279 ],\n",
       "         [-1.00409651],\n",
       "         [-0.76296037]],\n",
       "\n",
       "        [[-1.0727575 ],\n",
       "         [-1.18557342],\n",
       "         [-1.02925874],\n",
       "         [-0.98951278],\n",
       "         [-0.96775952]],\n",
       "\n",
       "        [[-0.99039215],\n",
       "         [-0.87317272],\n",
       "         [-1.02246264],\n",
       "         [-0.87994235],\n",
       "         [-0.98963304]],\n",
       "\n",
       "        [[-0.88702193],\n",
       "         [-0.81145557],\n",
       "         [-0.75644764],\n",
       "         [-0.76746794],\n",
       "         [-0.90811346]],\n",
       "\n",
       "        [[-0.66392395],\n",
       "         [-0.79755624],\n",
       "         [-0.96060278],\n",
       "         [-0.86635718],\n",
       "         [-0.86437086]],\n",
       "\n",
       "        [[-0.93360039],\n",
       "         [-0.94999795],\n",
       "         [-0.81261629],\n",
       "         [-0.73039113],\n",
       "         [-0.76701288]],\n",
       "\n",
       "        [[-1.54139556],\n",
       "         [-1.48428852],\n",
       "         [-1.32644842],\n",
       "         [-1.29926702],\n",
       "         [-1.23371182]],\n",
       "\n",
       "        [[-0.94242877],\n",
       "         [-0.87841878],\n",
       "         [-0.96263723],\n",
       "         [-1.17609166],\n",
       "         [-1.16055095]],\n",
       "\n",
       "        [[-1.23328413],\n",
       "         [-1.06825993],\n",
       "         [-1.24357214],\n",
       "         [-1.06955074],\n",
       "         [-1.08705069]],\n",
       "\n",
       "        [[-0.87995879],\n",
       "         [-0.79611184],\n",
       "         [-0.66276538],\n",
       "         [-0.65053224],\n",
       "         [-0.75625933]],\n",
       "\n",
       "        [[-0.62383046],\n",
       "         [-0.77132464],\n",
       "         [-0.7649822 ],\n",
       "         [-0.77612347],\n",
       "         [-0.80821833]],\n",
       "\n",
       "        [[-0.95729906],\n",
       "         [-0.93664828],\n",
       "         [-0.98093978],\n",
       "         [-0.96345992],\n",
       "         [-0.90303264]],\n",
       "\n",
       "        [[-0.80893243],\n",
       "         [-0.82594647],\n",
       "         [-0.76177586],\n",
       "         [-0.81719246],\n",
       "         [-0.83694876]],\n",
       "\n",
       "        [[-0.7866771 ],\n",
       "         [-0.80903634],\n",
       "         [-0.92066342],\n",
       "         [-0.83633775],\n",
       "         [-0.72284217]],\n",
       "\n",
       "        [[-1.90994253],\n",
       "         [-2.06683308],\n",
       "         [-1.76193349],\n",
       "         [-1.57557423],\n",
       "         [-1.11906956]],\n",
       "\n",
       "        [[-1.39779103],\n",
       "         [-1.3455917 ],\n",
       "         [-0.98308543],\n",
       "         [-0.85829335],\n",
       "         [-0.90517654]],\n",
       "\n",
       "        [[-0.96370865],\n",
       "         [-0.90155605],\n",
       "         [-0.97942268],\n",
       "         [-1.12881224],\n",
       "         [-1.08717176]],\n",
       "\n",
       "        [[-0.90584384],\n",
       "         [-0.79491286],\n",
       "         [-0.80561248],\n",
       "         [-0.87028736],\n",
       "         [-1.01351653]],\n",
       "\n",
       "        [[-0.90778656],\n",
       "         [-1.08094121],\n",
       "         [-1.10409299],\n",
       "         [-0.95426058],\n",
       "         [-1.01487691]],\n",
       "\n",
       "        [[-0.73971229],\n",
       "         [-0.73744025],\n",
       "         [-0.79842845],\n",
       "         [-0.78158215],\n",
       "         [-0.8580075 ]],\n",
       "\n",
       "        [[-0.76061098],\n",
       "         [-0.68655137],\n",
       "         [-0.65540761],\n",
       "         [-0.70678363],\n",
       "         [-0.71237712]],\n",
       "\n",
       "        [[-0.62805583],\n",
       "         [-0.72432107],\n",
       "         [-0.79978522],\n",
       "         [-0.8158502 ],\n",
       "         [-0.7175974 ]]],\n",
       "\n",
       "\n",
       "       [[[-0.96348194],\n",
       "         [-1.20483816],\n",
       "         [-1.55336699],\n",
       "         [-1.54060503],\n",
       "         [-1.42135303]],\n",
       "\n",
       "        [[-0.95791722],\n",
       "         [-1.07689992],\n",
       "         [-1.1134107 ],\n",
       "         [-1.27264675],\n",
       "         [-1.41075407]],\n",
       "\n",
       "        [[-1.0816844 ],\n",
       "         [-1.08110163],\n",
       "         [-1.14815693],\n",
       "         [-1.10213952],\n",
       "         [-0.97304161]],\n",
       "\n",
       "        [[-0.89358306],\n",
       "         [-0.93379216],\n",
       "         [-0.83497805],\n",
       "         [-0.95430357],\n",
       "         [-1.03352613]],\n",
       "\n",
       "        [[-1.05801051],\n",
       "         [-0.93926883],\n",
       "         [-0.99873709],\n",
       "         [-0.89453909],\n",
       "         [-0.9492    ]],\n",
       "\n",
       "        [[-0.81540926],\n",
       "         [-0.64559539],\n",
       "         [-0.52618603],\n",
       "         [-0.58445837],\n",
       "         [-0.71824995]],\n",
       "\n",
       "        [[-0.7120623 ],\n",
       "         [-0.72689519],\n",
       "         [-0.80466184],\n",
       "         [-0.90070243],\n",
       "         [-0.74488911]],\n",
       "\n",
       "        [[-0.86674729],\n",
       "         [-0.90070152],\n",
       "         [-0.91050772],\n",
       "         [-0.6692415 ],\n",
       "         [-0.61864983]],\n",
       "\n",
       "        [[-1.3027781 ],\n",
       "         [-1.17957064],\n",
       "         [-1.13598004],\n",
       "         [-1.1249172 ],\n",
       "         [-1.15283892]],\n",
       "\n",
       "        [[-0.9067138 ],\n",
       "         [-0.92107661],\n",
       "         [-1.08115488],\n",
       "         [-0.97619098],\n",
       "         [-1.0060431 ]],\n",
       "\n",
       "        [[-1.00409651],\n",
       "         [-0.76296037],\n",
       "         [-0.82606097],\n",
       "         [-0.82935298],\n",
       "         [-1.0635344 ]],\n",
       "\n",
       "        [[-0.98951278],\n",
       "         [-0.96775952],\n",
       "         [-1.00887979],\n",
       "         [-1.0275082 ],\n",
       "         [-1.05358019]],\n",
       "\n",
       "        [[-0.87994235],\n",
       "         [-0.98963304],\n",
       "         [-0.924628  ],\n",
       "         [-0.7977289 ],\n",
       "         [-0.72992629]],\n",
       "\n",
       "        [[-0.76746794],\n",
       "         [-0.90811346],\n",
       "         [-0.88545901],\n",
       "         [-0.86144587],\n",
       "         [-0.81966466]],\n",
       "\n",
       "        [[-0.86635718],\n",
       "         [-0.86437086],\n",
       "         [-0.75119977],\n",
       "         [-0.91487578],\n",
       "         [-0.89629611]],\n",
       "\n",
       "        [[-0.73039113],\n",
       "         [-0.76701288],\n",
       "         [-0.76897869],\n",
       "         [-0.79911154],\n",
       "         [-0.71764861]],\n",
       "\n",
       "        [[-1.29926702],\n",
       "         [-1.23371182],\n",
       "         [-1.33783496],\n",
       "         [-1.29954671],\n",
       "         [-1.60492618]],\n",
       "\n",
       "        [[-1.17609166],\n",
       "         [-1.16055095],\n",
       "         [-1.20758885],\n",
       "         [-1.20940792],\n",
       "         [-1.16331053]],\n",
       "\n",
       "        [[-1.06955074],\n",
       "         [-1.08705069],\n",
       "         [-0.85006583],\n",
       "         [-0.98842855],\n",
       "         [-1.05102849]],\n",
       "\n",
       "        [[-0.65053224],\n",
       "         [-0.75625933],\n",
       "         [-1.11151713],\n",
       "         [-0.98623744],\n",
       "         [-0.86390762]],\n",
       "\n",
       "        [[-0.77612347],\n",
       "         [-0.80821833],\n",
       "         [-0.80325158],\n",
       "         [-0.85383606],\n",
       "         [-0.76038829]],\n",
       "\n",
       "        [[-0.96345992],\n",
       "         [-0.90303264],\n",
       "         [-0.88598305],\n",
       "         [-0.99838988],\n",
       "         [-1.10289912]],\n",
       "\n",
       "        [[-0.81719246],\n",
       "         [-0.83694876],\n",
       "         [-0.75925017],\n",
       "         [-0.800656  ],\n",
       "         [-0.63302448]],\n",
       "\n",
       "        [[-0.83633775],\n",
       "         [-0.72284217],\n",
       "         [-0.67353293],\n",
       "         [-0.54739749],\n",
       "         [-0.70893026]],\n",
       "\n",
       "        [[-1.57557423],\n",
       "         [-1.11906956],\n",
       "         [-1.11121031],\n",
       "         [-1.07953673],\n",
       "         [-1.70387922]],\n",
       "\n",
       "        [[-0.85829335],\n",
       "         [-0.90517654],\n",
       "         [-1.27972531],\n",
       "         [-1.29321664],\n",
       "         [-1.19717878]],\n",
       "\n",
       "        [[-1.12881224],\n",
       "         [-1.08717176],\n",
       "         [-0.88936256],\n",
       "         [-0.93789375],\n",
       "         [-0.94790944]],\n",
       "\n",
       "        [[-0.87028736],\n",
       "         [-1.01351653],\n",
       "         [-1.19473902],\n",
       "         [-0.91888407],\n",
       "         [-0.88785104]],\n",
       "\n",
       "        [[-0.95426058],\n",
       "         [-1.01487691],\n",
       "         [-0.80398295],\n",
       "         [-0.79554915],\n",
       "         [-0.81273939]],\n",
       "\n",
       "        [[-0.78158215],\n",
       "         [-0.8580075 ],\n",
       "         [-0.82216864],\n",
       "         [-1.0728803 ],\n",
       "         [-0.90311081]],\n",
       "\n",
       "        [[-0.70678363],\n",
       "         [-0.71237712],\n",
       "         [-0.69767884],\n",
       "         [-0.65558245],\n",
       "         [-0.64265704]],\n",
       "\n",
       "        [[-0.8158502 ],\n",
       "         [-0.7175974 ],\n",
       "         [-0.77222041],\n",
       "         [-0.7845426 ],\n",
       "         [-0.77489926]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-1.49931066],\n",
       "         [-1.17881168],\n",
       "         [-0.7481329 ],\n",
       "         [-0.92900456],\n",
       "         [-0.86233418]],\n",
       "\n",
       "        [[-0.7937873 ],\n",
       "         [-0.86892127],\n",
       "         [-0.83811115],\n",
       "         [-1.05663028],\n",
       "         [-0.97505223]],\n",
       "\n",
       "        [[-0.7795319 ],\n",
       "         [-0.88476203],\n",
       "         [-0.99346662],\n",
       "         [-0.73128738],\n",
       "         [-0.66127226]],\n",
       "\n",
       "        [[-1.04920283],\n",
       "         [-1.165143  ],\n",
       "         [-1.22817714],\n",
       "         [-0.87871933],\n",
       "         [-0.94598019]],\n",
       "\n",
       "        [[-0.92036718],\n",
       "         [-0.80490745],\n",
       "         [-1.05470504],\n",
       "         [-1.01537872],\n",
       "         [-1.17000671]],\n",
       "\n",
       "        [[-0.71426932],\n",
       "         [-0.6851344 ],\n",
       "         [-0.88889921],\n",
       "         [-0.81372052],\n",
       "         [-0.89704652]],\n",
       "\n",
       "        [[-1.11143019],\n",
       "         [-0.88752681],\n",
       "         [-0.76862219],\n",
       "         [-0.82601847],\n",
       "         [-0.94780885]],\n",
       "\n",
       "        [[-0.79167627],\n",
       "         [-0.97054387],\n",
       "         [-0.89311763],\n",
       "         [-1.11216407],\n",
       "         [-0.92709152]],\n",
       "\n",
       "        [[-0.79253592],\n",
       "         [-0.90243483],\n",
       "         [-0.98485232],\n",
       "         [-0.8410287 ],\n",
       "         [-0.85626481]],\n",
       "\n",
       "        [[-1.03561367],\n",
       "         [-0.95087449],\n",
       "         [-0.98072145],\n",
       "         [-0.44067014],\n",
       "         [-0.50006768]],\n",
       "\n",
       "        [[-1.19850981],\n",
       "         [-1.19937085],\n",
       "         [-1.15026047],\n",
       "         [-1.18696108],\n",
       "         [-1.12487771]],\n",
       "\n",
       "        [[-1.30290173],\n",
       "         [-1.15297553],\n",
       "         [-1.42060093],\n",
       "         [-1.50134265],\n",
       "         [-1.2012265 ]],\n",
       "\n",
       "        [[-0.92205132],\n",
       "         [-0.78397057],\n",
       "         [-0.56837666],\n",
       "         [-0.89630175],\n",
       "         [-1.02563072]],\n",
       "\n",
       "        [[-0.69536468],\n",
       "         [-0.83732409],\n",
       "         [-0.79670403],\n",
       "         [-1.03468131],\n",
       "         [-1.11591944]],\n",
       "\n",
       "        [[-0.77310959],\n",
       "         [-0.77763074],\n",
       "         [-0.86272853],\n",
       "         [-1.01781293],\n",
       "         [-0.96843473]],\n",
       "\n",
       "        [[-0.8428489 ],\n",
       "         [-0.81648944],\n",
       "         [-0.93493967],\n",
       "         [-1.08905359],\n",
       "         [-0.89183887]],\n",
       "\n",
       "        [[-1.25085863],\n",
       "         [-1.15385812],\n",
       "         [-1.06998089],\n",
       "         [-1.36531543],\n",
       "         [-1.52525622]],\n",
       "\n",
       "        [[-0.97031617],\n",
       "         [-1.10079753],\n",
       "         [-1.18788651],\n",
       "         [-1.11051192],\n",
       "         [-0.9985508 ]],\n",
       "\n",
       "        [[-0.63832282],\n",
       "         [-0.9771851 ],\n",
       "         [-0.82764791],\n",
       "         [-0.33883768],\n",
       "         [-0.43393966]],\n",
       "\n",
       "        [[-0.6421506 ],\n",
       "         [-0.60158445],\n",
       "         [-0.64393435],\n",
       "         [-0.87643953],\n",
       "         [-0.88204194]],\n",
       "\n",
       "        [[-1.19794859],\n",
       "         [-0.93350822],\n",
       "         [-0.84755774],\n",
       "         [-1.12346219],\n",
       "         [-0.98653279]],\n",
       "\n",
       "        [[-0.91130167],\n",
       "         [-0.83570481],\n",
       "         [-0.85835613],\n",
       "         [-1.11725558],\n",
       "         [-1.17390608]],\n",
       "\n",
       "        [[-1.07041849],\n",
       "         [-0.94206123],\n",
       "         [-0.98139263],\n",
       "         [-1.18392288],\n",
       "         [-1.05951704]],\n",
       "\n",
       "        [[-0.96742287],\n",
       "         [-0.92782376],\n",
       "         [-1.04925888],\n",
       "         [-1.14841856],\n",
       "         [-0.9465514 ]],\n",
       "\n",
       "        [[-1.48719994],\n",
       "         [-1.3270159 ],\n",
       "         [-1.389408  ],\n",
       "         [-1.66236812],\n",
       "         [-1.62016452]],\n",
       "\n",
       "        [[-1.00729438],\n",
       "         [-1.12306619],\n",
       "         [-1.25474526],\n",
       "         [-1.44223824],\n",
       "         [-1.09674765]],\n",
       "\n",
       "        [[-1.20265188],\n",
       "         [-1.17359568],\n",
       "         [-1.1777232 ],\n",
       "         [-0.39294327],\n",
       "         [-0.45107124]],\n",
       "\n",
       "        [[-0.76570843],\n",
       "         [-0.83361683],\n",
       "         [-0.74594924],\n",
       "         [-0.75822366],\n",
       "         [-0.6939608 ]],\n",
       "\n",
       "        [[-0.70738726],\n",
       "         [-0.67273742],\n",
       "         [-0.71815482],\n",
       "         [-1.09980026],\n",
       "         [-1.13763449]],\n",
       "\n",
       "        [[-0.86712845],\n",
       "         [-0.89853515],\n",
       "         [-0.98481201],\n",
       "         [-1.17714538],\n",
       "         [-1.04752493]],\n",
       "\n",
       "        [[-0.97608837],\n",
       "         [-0.95990871],\n",
       "         [-0.71836067],\n",
       "         [-0.83294598],\n",
       "         [-0.91822618]],\n",
       "\n",
       "        [[-0.70495666],\n",
       "         [-0.66674764],\n",
       "         [-0.76576295],\n",
       "         [-1.1541965 ],\n",
       "         [-1.24765067]]],\n",
       "\n",
       "\n",
       "       [[[-0.92900456],\n",
       "         [-0.86233418],\n",
       "         [-1.00022266],\n",
       "         [-1.276643  ],\n",
       "         [-1.60825159]],\n",
       "\n",
       "        [[-1.05663028],\n",
       "         [-0.97505223],\n",
       "         [-0.94760183],\n",
       "         [-0.68963918],\n",
       "         [-0.80484349]],\n",
       "\n",
       "        [[-0.73128738],\n",
       "         [-0.66127226],\n",
       "         [-0.67500228],\n",
       "         [-0.76741232],\n",
       "         [-0.7601851 ]],\n",
       "\n",
       "        [[-0.87871933],\n",
       "         [-0.94598019],\n",
       "         [-0.85436284],\n",
       "         [-0.94939681],\n",
       "         [-0.97394952]],\n",
       "\n",
       "        [[-1.01537872],\n",
       "         [-1.17000671],\n",
       "         [-1.07323172],\n",
       "         [-1.07711681],\n",
       "         [-0.90089316]],\n",
       "\n",
       "        [[-0.81372052],\n",
       "         [-0.89704652],\n",
       "         [-0.8937559 ],\n",
       "         [-0.90925215],\n",
       "         [-0.81547323]],\n",
       "\n",
       "        [[-0.82601847],\n",
       "         [-0.94780885],\n",
       "         [-0.96790375],\n",
       "         [-0.95816892],\n",
       "         [-0.75622829]],\n",
       "\n",
       "        [[-1.11216407],\n",
       "         [-0.92709152],\n",
       "         [-0.93490729],\n",
       "         [-0.84621236],\n",
       "         [-1.07128365]],\n",
       "\n",
       "        [[-0.8410287 ],\n",
       "         [-0.85626481],\n",
       "         [-0.79193763],\n",
       "         [-0.58454946],\n",
       "         [-0.56136456]],\n",
       "\n",
       "        [[-0.44067014],\n",
       "         [-0.50006768],\n",
       "         [-0.4694947 ],\n",
       "         [-0.81789775],\n",
       "         [-0.71245204]],\n",
       "\n",
       "        [[-1.18696108],\n",
       "         [-1.12487771],\n",
       "         [-1.34137423],\n",
       "         [-1.27482404],\n",
       "         [-1.09321898]],\n",
       "\n",
       "        [[-1.50134265],\n",
       "         [-1.2012265 ],\n",
       "         [-1.58185399],\n",
       "         [-1.4468842 ],\n",
       "         [-1.11837062]],\n",
       "\n",
       "        [[-0.89630175],\n",
       "         [-1.02563072],\n",
       "         [-0.99081351],\n",
       "         [-1.01980646],\n",
       "         [-1.07517941]],\n",
       "\n",
       "        [[-1.03468131],\n",
       "         [-1.11591944],\n",
       "         [-1.08670329],\n",
       "         [-0.92273583],\n",
       "         [-0.8797988 ]],\n",
       "\n",
       "        [[-1.01781293],\n",
       "         [-0.96843473],\n",
       "         [-0.88282717],\n",
       "         [-0.76951255],\n",
       "         [-1.07266158]],\n",
       "\n",
       "        [[-1.08905359],\n",
       "         [-0.89183887],\n",
       "         [-0.94870557],\n",
       "         [-0.94405444],\n",
       "         [-1.12992706]],\n",
       "\n",
       "        [[-1.36531543],\n",
       "         [-1.52525622],\n",
       "         [-1.81948155],\n",
       "         [-1.5002576 ],\n",
       "         [-1.5159406 ]],\n",
       "\n",
       "        [[-1.11051192],\n",
       "         [-0.9985508 ],\n",
       "         [-1.24363702],\n",
       "         [-1.24310007],\n",
       "         [-1.16705479]],\n",
       "\n",
       "        [[-0.33883768],\n",
       "         [-0.43393966],\n",
       "         [-0.39991012],\n",
       "         [-0.64380129],\n",
       "         [-0.61082382]],\n",
       "\n",
       "        [[-0.87643953],\n",
       "         [-0.88204194],\n",
       "         [-0.90777405],\n",
       "         [-0.7503693 ],\n",
       "         [-0.61398005]],\n",
       "\n",
       "        [[-1.12346219],\n",
       "         [-0.98653279],\n",
       "         [-0.9728504 ],\n",
       "         [-0.90442331],\n",
       "         [-0.99517388]],\n",
       "\n",
       "        [[-1.11725558],\n",
       "         [-1.17390608],\n",
       "         [-1.16271372],\n",
       "         [-0.95001093],\n",
       "         [-0.91698787]],\n",
       "\n",
       "        [[-1.18392288],\n",
       "         [-1.05951704],\n",
       "         [-0.96463505],\n",
       "         [-0.86546983],\n",
       "         [-0.96450317]],\n",
       "\n",
       "        [[-1.14841856],\n",
       "         [-0.9465514 ],\n",
       "         [-0.91276303],\n",
       "         [-0.87494997],\n",
       "         [-1.08347314]],\n",
       "\n",
       "        [[-1.66236812],\n",
       "         [-1.62016452],\n",
       "         [-1.60773615],\n",
       "         [-1.26553149],\n",
       "         [-1.25119788]],\n",
       "\n",
       "        [[-1.44223824],\n",
       "         [-1.09674765],\n",
       "         [-1.50444261],\n",
       "         [-1.27880747],\n",
       "         [-1.08511938]],\n",
       "\n",
       "        [[-0.39294327],\n",
       "         [-0.45107124],\n",
       "         [-0.43141971],\n",
       "         [-0.91865176],\n",
       "         [-0.91566346]],\n",
       "\n",
       "        [[-0.75822366],\n",
       "         [-0.6939608 ],\n",
       "         [-0.74196536],\n",
       "         [-0.64824445],\n",
       "         [-0.65385569]],\n",
       "\n",
       "        [[-1.09980026],\n",
       "         [-1.13763449],\n",
       "         [-1.07173048],\n",
       "         [-0.91532838],\n",
       "         [-0.97775868]],\n",
       "\n",
       "        [[-1.17714538],\n",
       "         [-1.04752493],\n",
       "         [-0.90773616],\n",
       "         [-0.81695948],\n",
       "         [-0.90915572]],\n",
       "\n",
       "        [[-0.83294598],\n",
       "         [-0.91822618],\n",
       "         [-0.98380479],\n",
       "         [-0.91381014],\n",
       "         [-0.90937567]],\n",
       "\n",
       "        [[-1.1541965 ],\n",
       "         [-1.24765067],\n",
       "         [-1.09524181],\n",
       "         [-0.81861735],\n",
       "         [-0.77814782]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dev[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 5, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 32, 5, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 32, 5, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Construcción de la Capsnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It only has two dependencies numpy and tensorflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from config import cfg\n",
    "\n",
    "\n",
    "# Class defining a Convolutional Capsule\n",
    "# consisting of multiple neuron layers\n",
    "#\n",
    "class CapsConv(object):\n",
    "    ''' Capsule layer.\n",
    "    Args:\n",
    "        input: A 4-D tensor.\n",
    "        num_units: integer, the length of the output vector of a capsule.\n",
    "        with_routing: boolean, this capsule is routing with the\n",
    "                      lower-level layer capsule.\n",
    "        num_outputs: the number of capsule in this layer.\n",
    "    Returns:\n",
    "        A 4-D tensor.\n",
    "    '''\n",
    "    def __init__(self, num_units, with_routing=True):\n",
    "        self.num_units = num_units\n",
    "        self.with_routing = with_routing\n",
    "\n",
    "    def __call__(self, input, num_outputs, kernel_size=None, stride=None):\n",
    "        self.num_outputs = num_outputs\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        if not self.with_routing:\n",
    "            # the PrimaryCaps layer\n",
    "            # input: [batch_size, 20, 20, 256]\n",
    "            assert input.get_shape() == [cfg.batch_size, 20, 20, 256]\n",
    "\n",
    "            capsules = []\n",
    "            for i in range(self.num_units):\n",
    "                # each capsule i: [batch_size, 6, 6, 32]\n",
    "                with tf.variable_scope('ConvUnit_' + str(i)):\n",
    "                    caps_i = tf.contrib.layers.conv2d(input,\n",
    "                                                      self.num_outputs,\n",
    "                                                      self.kernel_size,\n",
    "                                                      self.stride,\n",
    "                                                      padding=\"VALID\")\n",
    "                    caps_i = tf.reshape(caps_i, shape=(cfg.batch_size, -1, 1, 1))\n",
    "                    capsules.append(caps_i)\n",
    "\n",
    "            assert capsules[0].get_shape() == [cfg.batch_size, 1152, 1, 1]\n",
    "\n",
    "            # [batch_size, 1152, 8, 1]\n",
    "            capsules = tf.concat(capsules, axis=2)\n",
    "            capsules = squash(capsules)\n",
    "            assert capsules.get_shape() == [cfg.batch_size, 1152, 8, 1]\n",
    "\n",
    "        else:\n",
    "            # the DigitCaps layer\n",
    "            # Reshape the input into shape [batch_size, 1152, 8, 1]\n",
    "            self.input = tf.reshape(input, shape=(cfg.batch_size, 1152, 8, 1))\n",
    "\n",
    "            # b_IJ: [1, num_caps_l, num_caps_l_plus_1, 1]\n",
    "            b_IJ = tf.zeros(shape=[1, 1152, 10, 1], dtype=np.float32)\n",
    "            capsules = []\n",
    "            for j in range(self.num_outputs):\n",
    "                with tf.variable_scope('caps_' + str(j)):\n",
    "                    caps_j, b_IJ = capsule(input, b_IJ, j)\n",
    "                    capsules.append(caps_j)\n",
    "\n",
    "            # Return a tensor with shape [batch_size, 10, 16, 1]\n",
    "            capsules = tf.concat(capsules, axis=1)\n",
    "            assert capsules.get_shape() == [cfg.batch_size, 10, 16, 1]\n",
    "\n",
    "        return(capsules)\n",
    "\n",
    "\n",
    "def capsule(input, b_IJ, idx_j):\n",
    "    ''' The routing algorithm for one capsule in the layer l+1.\n",
    "    Args:\n",
    "        input: A Tensor with [batch_size, num_caps_l=1152, length(u_i)=8, 1]\n",
    "               shape, num_caps_l meaning the number of capsule in the layer l.\n",
    "    Returns:\n",
    "        A Tensor of shape [batch_size, 1, length(v_j)=16, 1] representing the\n",
    "        vector output `v_j` of capsule j in the layer l+1\n",
    "    Notes:\n",
    "        u_i represents the vector output of capsule i in the layer l, and\n",
    "        v_j the vector output of capsule j in the layer l+1.\n",
    "     '''\n",
    "\n",
    "    with tf.variable_scope('routing'):\n",
    "        w_initializer = np.random.normal(size=[1, 1152, 8, 16], scale=0.01)\n",
    "        W_Ij = tf.Variable(w_initializer, dtype=tf.float32)\n",
    "        # repeat W_Ij with batch_size times to shape [batch_size, 1152, 8, 16]\n",
    "        W_Ij = tf.tile(W_Ij, [cfg.batch_size, 1, 1, 1])\n",
    "\n",
    "        # calc u_hat\n",
    "        # [8, 16].T x [8, 1] => [16, 1] => [batch_size, 1152, 16, 1]\n",
    "        u_hat = tf.matmul(W_Ij, input, transpose_a=True)\n",
    "        assert u_hat.get_shape() == [cfg.batch_size, 1152, 16, 1]\n",
    "\n",
    "        shape = b_IJ.get_shape().as_list()\n",
    "        size_splits = [idx_j, 1, shape[2] - idx_j - 1]\n",
    "        for r_iter in range(cfg.iter_routing):\n",
    "            # line 4:\n",
    "            # [1, 1152, 10, 1]\n",
    "            c_IJ = tf.nn.softmax(b_IJ, dim=2)\n",
    "            assert c_IJ.get_shape() == [1, 1152, 10, 1]\n",
    "\n",
    "            # line 5:\n",
    "            # weighting u_hat with c_I in the third dim,\n",
    "            # then sum in the second dim, resulting in [batch_size, 1, 16, 1]\n",
    "            b_Il, b_Ij, b_Ir = tf.split(b_IJ, size_splits, axis=2)\n",
    "            c_Il, c_Ij, b_Ir = tf.split(c_IJ, size_splits, axis=2)\n",
    "            assert c_Ij.get_shape() == [1, 1152, 1, 1]\n",
    "\n",
    "            s_j = tf.multiply(c_Ij, u_hat)\n",
    "            s_j = tf.reduce_sum(tf.multiply(c_Ij, u_hat),\n",
    "                                axis=1, keep_dims=True)\n",
    "            assert s_j.get_shape() == [cfg.batch_size, 1, 16, 1]\n",
    "\n",
    "            # line 6:\n",
    "            # squash using Eq.1, resulting in [batch_size, 1, 16, 1]\n",
    "            v_j = squash(s_j)\n",
    "            assert s_j.get_shape() == [cfg.batch_size, 1, 16, 1]\n",
    "\n",
    "            # line 7:\n",
    "            # tile v_j from [batch_size ,1, 16, 1] to [batch_size, 1152, 16, 1]\n",
    "            # [16, 1].T x [16, 1] => [1, 1], then reduce mean in the\n",
    "            # batch_size dim, resulting in [1, 1152, 1, 1]\n",
    "            v_j_tiled = tf.tile(v_j, [1, 1152, 1, 1])\n",
    "            u_produce_v = tf.matmul(u_hat, v_j_tiled, transpose_a=True)\n",
    "            assert u_produce_v.get_shape() == [cfg.batch_size, 1152, 1, 1]\n",
    "            b_Ij += tf.reduce_sum(u_produce_v, axis=0, keep_dims=True)\n",
    "            b_IJ = tf.concat([b_Il, b_Ij, b_Ir], axis=2)\n",
    "\n",
    "        return(v_j, b_IJ)\n",
    "\n",
    "\n",
    "def squash(vector):\n",
    "    '''Squashing function.\n",
    "    Args:\n",
    "        vector: A 4-D tensor with shape [batch_size, num_caps, vec_len, 1],\n",
    "    Returns:\n",
    "        A 4-D tensor with the same shape as vector but\n",
    "        squashed in 3rd and 4th dimensions.\n",
    "    '''\n",
    "    vec_abs = tf.sqrt(tf.reduce_sum(tf.square(vector)))  # a scalar\n",
    "    scalar_factor = tf.square(vec_abs) / (1 + tf.square(vec_abs))\n",
    "    vec_squashed = scalar_factor * tf.divide(vector, vec_abs)  # element-wise\n",
    "    return(vec_squashed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from config import cfg\n",
    "from utils import get_batch_data\n",
    "from capsLayer import CapsConv\n",
    "\n",
    "\n",
    "class CapsNet(object):\n",
    "    def __init__(self, is_training=True):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            if is_training:\n",
    "                self.X, self.Y = get_batch_data()\n",
    "\n",
    "                self.build_arch()\n",
    "                self.loss()\n",
    "\n",
    "                # t_vars = tf.trainable_variables()\n",
    "                self.optimizer = tf.train.AdamOptimizer()\n",
    "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                self.train_op = self.optimizer.minimize(self.total_loss, global_step=self.global_step)  # var_list=t_vars)\n",
    "            else:\n",
    "                self.X = tf.placeholder(tf.float32,\n",
    "                                        shape=(cfg.batch_size, 28, 28, 1))\n",
    "                self.build_arch()\n",
    "\n",
    "        tf.logging.info('Seting up the main structure')\n",
    "\n",
    "    def build_arch(self):\n",
    "        with tf.variable_scope('Conv1_layer'):\n",
    "            # Conv1, [batch_size, 20, 20, 256]\n",
    "            conv1 = tf.contrib.layers.conv2d(self.X, num_outputs=256,\n",
    "                                             kernel_size=9, stride=1,\n",
    "                                             padding='VALID')\n",
    "            assert conv1.get_shape() == [cfg.batch_size, 20, 20, 256]\n",
    "\n",
    "        # TODO: Rewrite the 'CapsConv' class as a function, the capsLay\n",
    "        # function should be encapsulated into tow function, one like conv2d\n",
    "        # and another is fully_connected in Tensorflow.\n",
    "        # Primary Capsules, [batch_size, 1152, 8, 1]\n",
    "        with tf.variable_scope('PrimaryCaps_layer'):\n",
    "            primaryCaps = CapsConv(num_units=8, with_routing=False)\n",
    "            caps1 = primaryCaps(conv1, num_outputs=32, kernel_size=9, stride=2)\n",
    "            assert caps1.get_shape() == [cfg.batch_size, 1152, 8, 1]\n",
    "\n",
    "        # DigitCaps layer, [batch_size, 10, 16, 1]\n",
    "        with tf.variable_scope('DigitCaps_layer'):\n",
    "            digitCaps = CapsConv(num_units=16, with_routing=True)\n",
    "            self.caps2 = digitCaps(caps1, num_outputs=10)\n",
    "\n",
    "        # Decoder structure in Fig. 2\n",
    "        # 1. Do masking, how:\n",
    "        with tf.variable_scope('Masking'):\n",
    "            # a). calc ||v_c||, then do softmax(||v_c||)\n",
    "            # [batch_size, 10, 16, 1] => [batch_size, 10, 1, 1]\n",
    "            self.v_length = tf.sqrt(tf.reduce_sum(tf.square(self.caps2),\n",
    "                                                  axis=2, keep_dims=True))\n",
    "            self.softmax_v = tf.nn.softmax(self.v_length, dim=1)\n",
    "            assert self.softmax_v.get_shape() == [cfg.batch_size, 10, 1, 1]\n",
    "\n",
    "            # b). pick out the index of max softmax val of the 10 caps\n",
    "            # [batch_size, 10, 1, 1] => [batch_size] (index)\n",
    "            argmax_idx = tf.argmax(self.softmax_v, axis=1, output_type=tf.int32)\n",
    "            assert argmax_idx.get_shape() == [cfg.batch_size, 1, 1]\n",
    "\n",
    "            # c). indexing\n",
    "            # It's not easy to understand the indexing process with argmax_idx\n",
    "            # as we are 3-dim animal\n",
    "            masked_v = []\n",
    "            argmax_idx = tf.reshape(argmax_idx, shape=(cfg.batch_size, ))\n",
    "            for batch_size in range(cfg.batch_size):\n",
    "                v = self.caps2[batch_size][argmax_idx[batch_size], :]\n",
    "                masked_v.append(tf.reshape(v, shape=(1, 1, 16, 1)))\n",
    "\n",
    "            self.masked_v = tf.concat(masked_v, axis=0)\n",
    "            assert self.masked_v.get_shape() == [cfg.batch_size, 1, 16, 1]\n",
    "\n",
    "        # 2. Reconstructe the MNIST images with 3 FC layers\n",
    "        # [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]\n",
    "        with tf.variable_scope('Decoder'):\n",
    "            vector_j = tf.reshape(self.masked_v, shape=(cfg.batch_size, -1))\n",
    "            fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n",
    "            assert fc1.get_shape() == [cfg.batch_size, 512]\n",
    "            fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n",
    "            assert fc2.get_shape() == [cfg.batch_size, 1024]\n",
    "            self.decoded = tf.contrib.layers.fully_connected(fc2, num_outputs=784, activation_fn=tf.sigmoid)\n",
    "\n",
    "    def loss(self):\n",
    "        # 1. The margin loss\n",
    "\n",
    "        # [batch_size, 10, 1, 1]\n",
    "        # max_l = max(0, m_plus-||v_c||)^2\n",
    "        max_l = tf.square(tf.maximum(0., cfg.m_plus - self.v_length))\n",
    "        # max_r = max(0, ||v_c||-m_minus)^2\n",
    "        max_r = tf.square(tf.maximum(0., self.v_length - cfg.m_minus))\n",
    "        assert max_l.get_shape() == [cfg.batch_size, 10, 1, 1]\n",
    "\n",
    "        # reshape: [batch_size, 10, 1, 1] => [batch_size, 10]\n",
    "        max_l = tf.reshape(max_l, shape=(cfg.batch_size, -1))\n",
    "        max_r = tf.reshape(max_r, shape=(cfg.batch_size, -1))\n",
    "\n",
    "        # calc T_c: [batch_size, 10]\n",
    "        # T_c = Y, is my understanding correct? Try it.\n",
    "        T_c = self.Y\n",
    "        # [batch_size, 10], element-wise multiply\n",
    "        L_c = T_c * max_l + cfg.lambda_val * (1 - T_c) * max_r\n",
    "\n",
    "        self.margin_loss = tf.reduce_mean(tf.reduce_sum(L_c, axis=1))\n",
    "\n",
    "        # 2. The reconstruction loss\n",
    "        orgin = tf.reshape(self.X, shape=(cfg.batch_size, -1))\n",
    "        squared = tf.square(self.decoded - orgin)\n",
    "        self.reconstruction_err = tf.reduce_mean(squared)\n",
    "\n",
    "        # 3. Total loss\n",
    "        self.total_loss = self.margin_loss + 0.0005 * self.reconstruction_err\n",
    "\n",
    "        # Summary\n",
    "        tf.summary.scalar('margin_loss', self.margin_loss)\n",
    "        tf.summary.scalar('reconstruction_loss', self.reconstruction_err)\n",
    "        tf.summary.scalar('total_loss', self.total_loss)\n",
    "        recon_img = tf.reshape(self.decoded, shape=(cfg.batch_size, 28, 28, 1))\n",
    "        tf.summary.image('reconstruction_img', recon_img)\n",
    "        self.merged_sum = tf.summary.merge_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
