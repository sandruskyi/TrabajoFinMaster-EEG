{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 4: Clasificación con Capsnet\n",
    "\n",
    "La implementación de esta red Capsnet se ha basado en el código implementado por Aurélien Géron (https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados con la arquitectura anterior: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cambios realizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aumenta de 100 a 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuevos resultados: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#Helper libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Signal libraries\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the default graph, in case you re-run this notebook without restarting the kernel:\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Random seeds so that this notebook always produces the same output:\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROutput:\n",
    "    def __init__(self, task, data):\n",
    "        self.task = task\n",
    "        self.data = data\n",
    "        \n",
    "class OutTaskData: \n",
    "    def __init__(self, task, data): \n",
    "        self.task = task\n",
    "        self.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "# Primero leemos los registros\n",
    "def read_outputs(rec):\n",
    "    '''read_outputs(\"userS0091f1.mat\")'''\n",
    "    mat = sio.loadmat(rec)\n",
    "    mdata = mat['session']\n",
    "    val = mdata[0,0]\n",
    "    #output = ROutput(np.array(val[\"task\"]), np.array(val[\"data\"]))\n",
    "    output = ROutput(np.array(val[\"task_EEG_p\"]), np.array(val[\"data_processed_EEG\"]))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import Perceptron\n",
    "#from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "task1 = 402 # SE PUEDE CAMBIAR\n",
    "task2 = 404 # SE PUEDE CAMBIAR\n",
    "task_OneHotEnconding = {402: [1.,0.], 404: [0.,1.]}\n",
    "user = 'W29' # SE PUEDE CAMBIAR\n",
    "day = '0329'\n",
    "folder_day = 'W29-29_03_2021'\n",
    "total_records = 22 # CAMBIAR SI HAY MAS REGISTROS\n",
    "fm = 200\n",
    "electrodes_names_selected = ['F3', 'FZ', 'FC1','FCZ','C1','CZ','CP1','CPZ', 'FC5', 'FC3','C5','C3','CP5','CP3','P3',\n",
    "                             'PZ','F4','FC2','FC4','FC6','C2','C4','CP2','CP4','C6','CP6','P4','HR' ,'HL', 'VU', 'VD']\n",
    "number_channels = len(electrodes_names_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 22\n"
     ]
    }
   ],
   "source": [
    "lTaskData = []\n",
    "total_records_used = 0\n",
    "for i_rec in range(1,total_records+1):\n",
    "    i_rec_record = i_rec\n",
    "    if i_rec_record <10:\n",
    "        i_rec_record = \"0\"+str(i_rec_record)\n",
    "    if i_rec % 2 == 0: # Registros impares primero: USUARIO SIN MOVIMIENTO SOLO PENSANDO\n",
    "        record = \"./RegistrosProcesados2/\"+folder_day+\"/W29_2021\"+day+\"_openloop_\"+str(i_rec_record)+\"_processed.mat\"\n",
    "        output = read_outputs(record) # output.task será y, output.data será x\n",
    "\n",
    "\n",
    "        output.task = np.transpose(output.task)\n",
    "        output.data = output.data.reshape((np.shape(output.data)[0],np.shape(output.data)[1]))\n",
    "        output.data = np.transpose(output.data)\n",
    "        #output.data = output.data.reshape((np.shape(output.data)[0],np.shape(output.data)[1],1))\n",
    "\n",
    "        outT = (output.task == task1) | (output.task == task2)\n",
    "        outData = output.data[0:np.shape(output.data)[0], outT[0,:]]\n",
    "        outTask = output.task[0, outT[0,:]]\n",
    "        outTD = OutTaskData(outTask, outData)\n",
    "\n",
    "        lTaskData.append(outTD)\n",
    "        total_records_used+=1\n",
    "print(total_records_used, total_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (8, 32, 49)\n",
      "y_train: (8, 49)\n",
      "X_dev: (2, 32, 49)\n",
      "y_dev: (2, 49)\n",
      "X_test: (1, 32, 49)\n",
      "y_test: (1, 49)\n",
      "\n",
      "ONE HOT ENCODER & WINDOWING:\n",
      "X_train: (104, 32, 5, 1)\n",
      "y_train: (104, 2)\n",
      "X_dev: (26, 32, 5, 1)\n",
      "y_dev: (26, 2)\n",
      "X_test: (13, 32, 5, 1)\n",
      "y_test: (13, 2)\n",
      "\n",
      "RESHAPE:\n",
      "X_train: (104, 32, 5, 1)\n",
      "y_train: (104, 2)\n",
      "X_dev: (26, 32, 5, 1)\n",
      "y_dev: (26, 2)\n",
      "X_test: (13, 32, 5, 1)\n",
      "y_test: (13, 2)\n"
     ]
    }
   ],
   "source": [
    "# Vamos a coger 2 registros para el entrenamiento, 1 para el conjunto dev set, 1 para el test set\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test = [],[],[],[],[],[] \n",
    "for j in range(0,total_records_used-3): # Cogemos 18 registros para entrenamiento\n",
    "    X_train.append(lTaskData[j].data)\n",
    "    y_train.append(lTaskData[j].task)\n",
    "\n",
    "for j in range(total_records_used-3,total_records_used-1): # Cogemos 2 registros para el dev set\n",
    "    X_dev.append(lTaskData[j].data)\n",
    "    y_dev.append(lTaskData[j].task)\n",
    "for j in range(total_records_used-1,total_records_used): # Cogemos 2 registros para el test set\n",
    "    X_test.append(lTaskData[j].data)\n",
    "    y_test.append(lTaskData[j].task)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "#y_train = np.ravel(np.array(y_train))\n",
    "y_train = np.array(y_train)\n",
    "X_dev = np.array(X_dev)\n",
    "#y_dev = np.ravel(np.array(y_dev))\n",
    "y_dev = np.array(y_dev)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "#y_test = np.ravel(np.array(y_test))\n",
    "\n",
    "print (\"X_train:\",X_train.shape)\n",
    "print (\"y_train:\",y_train.shape)\n",
    "print (\"X_dev:\",X_dev.shape)\n",
    "print (\"y_dev:\",y_dev.shape)\n",
    "print (\"X_test:\",X_test.shape)\n",
    "print (\"y_test:\",y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# VENTANEO Y ONE HOT ENCODING \n",
    "window = 5\n",
    "samples_advance = 3\n",
    "\n",
    "# Ventaneo X_train\n",
    "\n",
    "X_train_l = []\n",
    "y_train_l = []\n",
    "for num_X_train in range(np.shape(X_train)[0]): # Para no mezclar registros\n",
    "    win_init = int(0)\n",
    "    window_position = 0\n",
    "    \n",
    "    for i in range(np.shape(X_train)[2]): # For each signal registered\n",
    "        win_end = int(win_init + window)\n",
    "        if win_end >= np.shape(X_train)[2]:\n",
    "            break\n",
    "\n",
    "        task = np.unique(y_train[num_X_train,win_init:win_end])\n",
    "\n",
    "        if len(task)==1:\n",
    "        #if task1 in task or task2 in task:\n",
    "            signal_window = X_train[num_X_train, :, win_init:win_end]\n",
    "            \n",
    "            #data_filtered = preprocessing(signal_window, fm, number_channels)\n",
    "            #X_train_l.append(data_filtered)\n",
    "            X_train_l.append(signal_window)\n",
    "            taskOH = task_OneHotEnconding[task[0]]\n",
    "            y_train_l.append(taskOH)\n",
    "            #y_train_l.append(task)\n",
    "            \n",
    "            \n",
    "        win_init += int(samples_advance)\n",
    "\n",
    "X_train_l = np.array(X_train_l)\n",
    "y_train_l = np.array(y_train_l)\n",
    "\n",
    "\n",
    "# Ventaneo X_dev\n",
    "X_dev_l = []\n",
    "y_dev_l = []\n",
    "for num_X_dev in range(np.shape(X_dev)[0]):\n",
    "    win_init = int(0)\n",
    "    window_position = 0\n",
    "    \n",
    "    for i in range(np.shape(X_dev)[2]): # For each signal registered\n",
    "        win_end = int(win_init + window)\n",
    "        if win_end >= np.shape(X_dev)[2]:\n",
    "            break\n",
    "\n",
    "        task = np.unique(y_dev[num_X_dev,win_init:win_end])\n",
    "\n",
    "        if len(task)==1:\n",
    "        #if task1 in task or task2 in task:\n",
    "            signal_window = X_dev[num_X_dev, :, win_init:win_end]\n",
    "            \n",
    "            #data_filtered = preprocessing(signal_window, fm, number_channels)\n",
    "            #X_train_l.append(data_filtered)\n",
    "            X_dev_l.append(signal_window)\n",
    "            taskOH = task_OneHotEnconding[task[0]]\n",
    "            y_dev_l.append(taskOH)\n",
    "            #y_dev_l.append(task)\n",
    "            \n",
    "        win_init += int(samples_advance)\n",
    "\n",
    "X_dev_l = np.array(X_dev_l)\n",
    "y_dev_l = np.array(y_dev_l)\n",
    "\n",
    "# Ventaneo X_test\n",
    "X_test_l = []\n",
    "y_test_l = []\n",
    "for num_X_test in range(np.shape(X_test)[0]): \n",
    "    win_init = int(0)\n",
    "    window_position = 0\n",
    "    \n",
    "    for i in range(np.shape(X_test)[2]): # For each signal registered\n",
    "        win_end = int(win_init + window)\n",
    "        if win_end >= np.shape(X_test)[2]:\n",
    "            break\n",
    "\n",
    "        task = np.unique(y_test[num_X_test,win_init:win_end])\n",
    "\n",
    "        if len(task)==1:\n",
    "        #if task1 in task or task2 in task:\n",
    "            signal_window = X_test[num_X_test, :, win_init:win_end]\n",
    "            \n",
    "            #data_filtered = preprocessing(signal_window, fm, number_channels)\n",
    "            #X_train_l.append(data_filtered)\n",
    "            X_test_l.append(signal_window)\n",
    "            taskOH = task_OneHotEnconding[task[0]]\n",
    "            #y_test_l.append(task)\n",
    "            y_test_l.append(taskOH)\n",
    "            \n",
    "        win_init += int(samples_advance)\n",
    "\n",
    "X_test_l = np.array(X_test_l)\n",
    "y_test_l = np.array(y_test_l)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_l = X_train_l.reshape((np.shape(X_train_l)[0],np.shape(X_train_l)[1],np.shape(X_train_l)[2], 1))\n",
    "X_dev_l = X_dev_l.reshape((np.shape(X_dev_l)[0],np.shape(X_dev_l)[1],np.shape(X_dev_l)[2], 1))\n",
    "X_test_l = X_test_l.reshape((np.shape(X_test_l)[0],np.shape(X_test_l)[1],np.shape(X_test_l)[2], 1))\n",
    "\n",
    "print()\n",
    "print(\"ONE HOT ENCODER & WINDOWING:\")\n",
    "print (\"X_train:\",X_train_l.shape)\n",
    "print (\"y_train:\",y_train_l.shape)\n",
    "print (\"X_dev:\",X_dev_l.shape)\n",
    "print (\"y_dev:\",y_dev_l.shape)\n",
    "print (\"X_test:\",X_test_l.shape)\n",
    "print (\"y_test:\",y_test_l.shape)\n",
    "\n",
    "X_train = X_train_l\n",
    "y_train = y_train_l\n",
    "X_dev = X_dev_l\n",
    "y_dev = y_dev_l\n",
    "X_test = X_test_l\n",
    "y_test = y_test_l\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_dev = X_dev.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('int64')\n",
    "y_dev = y_dev.astype('int64')\n",
    "y_test = y_test.astype('int64')\n",
    "print()\n",
    "print(\"RESHAPE:\")\n",
    "print (\"X_train:\",X_train_l.shape)\n",
    "print (\"y_train:\",y_train_l.shape)\n",
    "print (\"X_dev:\",X_dev_l.shape)\n",
    "print (\"y_dev:\",y_dev_l.shape)\n",
    "print (\"X_test:\",X_test_l.shape)\n",
    "print (\"y_test:\",y_test_l.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Construcción de la Capsnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef plot_log(filename, show=True):\\n\\n    data = pandas.read_csv(filename)\\n\\n    fig = plt.figure(figsize=(4,6))\\n    fig.subplots_adjust(top=0.95, bottom=0.05, right=0.95)\\n    fig.add_subplot(211)\\n    for key in data.keys():\\n        if key.find(\\'loss\\') >= 0 and not key.find(\\'val\\') >= 0:  # training loss\\n            plt.plot(data[\\'epoch\\'].values, data[key].values, label=key)\\n    plt.legend()\\n    plt.title(\\'Training loss\\')\\n\\n    fig.add_subplot(212)\\n    for key in data.keys():\\n        if key.find(\\'acc\\') >= 0:  # acc\\n            plt.plot(data[\\'epoch\\'].values, data[key].values, label=key)\\n    plt.legend()\\n    plt.title(\\'Training and validation accuracy\\')\\n\\n    # fig.savefig(\\'result/log.png\\')\\n    if show:\\n        plt.show()\\n\\n\\ndef combine_images(generated_images, height=None, width=None):\\n    num = generated_images.shape[0]\\n    if width is None and height is None:\\n        width = int(math.sqrt(num))\\n        height = int(math.ceil(float(num)/width))\\n    elif width is not None and height is None:  # height not given\\n        height = int(math.ceil(float(num)/width))\\n    elif height is not None and width is None:  # width not given\\n        width = int(math.ceil(float(num)/height))\\n\\n    shape = generated_images.shape[1:3]\\n    image = np.zeros((height*shape[0], width*shape[1]),\\n                     dtype=generated_images.dtype)\\n    for index, img in enumerate(generated_images):\\n        i = int(index/width)\\n        j = index % width\\n        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] =             img[:, :, 0]\\n    return image\\n\\nif __name__==\"__main__\":\\n    plot_log(\\'result/log.csv\\')\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "import math\n",
    "import pandas\n",
    "\"\"\"\n",
    "def plot_log(filename, show=True):\n",
    "\n",
    "    data = pandas.read_csv(filename)\n",
    "\n",
    "    fig = plt.figure(figsize=(4,6))\n",
    "    fig.subplots_adjust(top=0.95, bottom=0.05, right=0.95)\n",
    "    fig.add_subplot(211)\n",
    "    for key in data.keys():\n",
    "        if key.find('loss') >= 0 and not key.find('val') >= 0:  # training loss\n",
    "            plt.plot(data['epoch'].values, data[key].values, label=key)\n",
    "    plt.legend()\n",
    "    plt.title('Training loss')\n",
    "\n",
    "    fig.add_subplot(212)\n",
    "    for key in data.keys():\n",
    "        if key.find('acc') >= 0:  # acc\n",
    "            plt.plot(data['epoch'].values, data[key].values, label=key)\n",
    "    plt.legend()\n",
    "    plt.title('Training and validation accuracy')\n",
    "\n",
    "    # fig.savefig('result/log.png')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def combine_images(generated_images, height=None, width=None):\n",
    "    num = generated_images.shape[0]\n",
    "    if width is None and height is None:\n",
    "        width = int(math.sqrt(num))\n",
    "        height = int(math.ceil(float(num)/width))\n",
    "    elif width is not None and height is None:  # height not given\n",
    "        height = int(math.ceil(float(num)/width))\n",
    "    elif height is not None and width is None:  # width not given\n",
    "        width = int(math.ceil(float(num)/height))\n",
    "\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
    "            img[:, :, 0]\n",
    "    return image\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    plot_log('result/log.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef add(inputs):\\n    [a, b] = inputs\\n    out = a + b\\n    return out\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras import initializers, layers, regularizers\n",
    "\n",
    "\n",
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
    "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
    "    inputs: shape=[None, num_vectors, dim_vector]\n",
    "    output: shape=[None, num_vectors]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n",
    "    \n",
    "    \"\"\"\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Length, self).get_config()\n",
    "        return config\n",
    "    \"\"\"\n",
    "\n",
    "class Mask(layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
    "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
    "    masked Tensor.\n",
    "    For example:\n",
    "        ```\n",
    "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
    "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
    "        out = Mask()(x)  # out.shape=[8, 6]\n",
    "        # or\n",
    "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
    "        ```\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
    "            assert len(inputs) == 2\n",
    "            inputs, mask = inputs\n",
    "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
    "            # compute lengths of capsules\n",
    "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
    "            # generate the mask which is a one-hot code.\n",
    "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
    "            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n",
    "\n",
    "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
    "        # mask.shape=[None, num_capsule]\n",
    "        # masked.shape=[None, num_capsule * dim_capsule]\n",
    "        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
    "        return masked\n",
    "    \"\"\"\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  # true label provided\n",
    "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
    "        else:  # no true label provided\n",
    "            return tuple([None, input_shape[1] * input_shape[2]])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Mask, self).get_config()\n",
    "        return config\n",
    "    \"\"\"\n",
    "\n",
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
    "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
    "    :param routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_capsule, routings,lam_regularize,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.lam_regularize = lam_regularize\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
    "                                        self.dim_capsule, self.input_dim_capsule],\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 regularizer=regularizers.l2(self.lam_regularize),\n",
    "                                 name='W')\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
    "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
    "        inputs_expand = K.expand_dims(inputs, 1)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
    "\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
    "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
    "        # Regard the first two dimensions as `batch` dimension,\n",
    "        # then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
    "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "        inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs_tiled)\n",
    "\n",
    "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
    "        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n",
    "\n",
    "        assert self.routings > 0, 'The routings should be > 0.'\n",
    "        for i in range(self.routings):\n",
    "            # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "            c = tf.nn.softmax(b, dim=1)\n",
    "\n",
    "            # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
    "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "            # The first two dimensions as `batch` dimension,\n",
    "            # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
    "            # outputs.shape=[None, num_capsule, dim_capsule]\n",
    "            outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))  # [None, 10, 16]\n",
    "\n",
    "            if i < self.routings - 1:\n",
    "                # outputs.shape =  [None, num_capsule, dim_capsule]\n",
    "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # The first two dimensions as `batch` dimension,\n",
    "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
    "                # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "                b += K.batch_dot(outputs, inputs_hat, [2, 3])\n",
    "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    \"\"\"\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_capsule': self.num_capsule,\n",
    "            'dim_capsule': self.dim_capsule,\n",
    "            'routings': self.routings\n",
    "        }\n",
    "        base_config = super(CapsuleLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \"\"\"\n",
    "\"\"\"\n",
    "class channel_attention(layers.Layer):\n",
    "\n",
    "    def __init__(self, weight_decay=0.00000004, scope=\"\", reuse=None,**kwargs):  #deap H=120,W=24,C=256; dreamer H=123,W=9,C=256\n",
    "        super(channel_attention, self).__init__(**kwargs)\n",
    "\n",
    "        self.weight_decay = weight_decay\n",
    "        self.scope = scope\n",
    "        self.reuse = reuse\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.H = input_shape[1]\n",
    "        self.W = input_shape[2]\n",
    "        self.C = input_shape[3]\n",
    "        self.w_c = self.add_weight(name=\"w_c\",\n",
    "                                   shape=[self.C, self.C],\n",
    "                                   dtype=tf.float32,\n",
    "                                   initializer=tf.orthogonal_initializer(),\n",
    "                                   regularizer=tf.contrib.layers.l1_regularizer(self.weight_decay))\n",
    "\n",
    "        self.b_c = self.add_weight(name=\"b_c\",\n",
    "                                   shape=[self.C],\n",
    "                                   dtype=tf.float32,\n",
    "                                   initializer=tf.zeros_initializer())\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        feature_map = inputs\n",
    "\n",
    "        transpose_feature_map = tf.transpose(tf.reduce_mean(feature_map, [1, 2], keep_dims=True),\n",
    "                                             perm=[0, 3, 1, 2])\n",
    "        channel_wise_attention_fm = tf.matmul(tf.reshape(transpose_feature_map,\n",
    "                                                         [-1, self.C]), self.w_c) + self.b_c\n",
    "        channel_wise_attention_fm = tf.nn.sigmoid(channel_wise_attention_fm)\n",
    "        #         channel_wise_attention_fm = tf.clip_by_value(tf.nn.relu(channel_wise_attention_fm),\n",
    "        #                                                      clip_value_min = 0,\n",
    "        #                                                      clip_value_max = 1)\n",
    "        attention = tf.reshape(tf.concat([channel_wise_attention_fm] * (self.H * self.W),\n",
    "                                         axis=1), [-1, self.H, self.W, self.C])\n",
    "        attended_fm = attention * feature_map\n",
    "        return attended_fm\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "class spatial_attention(layers.Layer):\n",
    "\n",
    "    def __init__(self, weight_decay=0.4, scope=\"\", reuse=None,**kwargs):\n",
    "        super(spatial_attention, self).__init__(**kwargs)\n",
    "        self.weight_decay = weight_decay\n",
    "        self.scope = scope\n",
    "        self.reuse = reuse\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.H = input_shape[1]\n",
    "        self.W = input_shape[2]\n",
    "        self.C = input_shape[3]\n",
    "        self.w_s = self.add_weight(name=\"w_s\",\n",
    "                                   shape=[self.C, 1],\n",
    "                                   dtype=tf.float32,\n",
    "                                   initializer=tf.orthogonal_initializer(),\n",
    "                                   regularizer=tf.contrib.layers.l1_regularizer(self.weight_decay))\n",
    "\n",
    "        self.b_s = self.add_weight(name=\"b_s\",\n",
    "                                   shape=[1],\n",
    "                                   dtype=tf.float32,\n",
    "                                   initializer=tf.zeros_initializer())\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        feature_map = inputs\n",
    "\n",
    "        spatial_attention_fm = tf.matmul(tf.reshape(feature_map, [-1, self.C]), self.w_s) + self.b_s\n",
    "        spatial_attention_fm = tf.nn.sigmoid(tf.reshape(spatial_attention_fm, [-1, self.W * self.H]))\n",
    "        #         spatial_attention_fm = tf.clip_by_value(tf.nn.relu(tf.reshape(spatial_attention_fm,\n",
    "        #                                                                       [-1, W * H])),\n",
    "        #                                                 clip_value_min = 0,\n",
    "        #                                                 clip_value_max = 1)\n",
    "        attention = tf.reshape(tf.concat([spatial_attention_fm] * self.C, axis=1), [-1, self.H, self.W, self.C])\n",
    "        attended_fm = attention * feature_map\n",
    "        return attended_fm\n",
    "\"\"\"\n",
    "\n",
    "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding, lam_regularize, model_version):\n",
    "    \"\"\"\n",
    "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_capsule: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
    "    \"\"\"\n",
    "    outputs = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                           name='primarycap_conv2d',kernel_regularizer= regularizers.l2(lam_regularize))(inputs)\n",
    "    #if model_version == 'v2':     # MLF-CapsNet\n",
    "    #    outputs = layers.concatenate([inputs,outputs],axis=3,\n",
    "    #                                    name='concatenate')\n",
    "\n",
    "    #   outputs = layers.Conv2D(filters=256, kernel_size=1, strides=1, padding='valid',\n",
    "    #                          name='bottleneck_layer')(outputs)\n",
    "    #elif model_version == 'v1':   # MLF-CapsNet without bottleneck layer\n",
    "    outputs = layers.concatenate([inputs, outputs], axis=3, name='concatenate')\n",
    "\n",
    "\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(outputs)#(conca_maps)#(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# The following is another way to implement primary capsule layer. This is much slower.\n",
    "# Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
    "    outputs = []\n",
    "    for _ in range(n_channels):\n",
    "        output = layers.Conv2D(filters=dim_capsule, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
    "        outputs.append(layers.Reshape([output.get_shape().as_list()[1] ** 2, dim_capsule])(output))\n",
    "    outputs = layers.Concatenate(axis=1)(outputs)\n",
    "    return layers.Lambda(squash)(outputs)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def conca(inputs):\n",
    "    #[a , b] = inputs\n",
    "    conca_maps = K.concatenate(inputs, axis=3)\n",
    "    return conca_maps\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def add(inputs):\n",
    "    [a, b] = inputs\n",
    "    out = a + b\n",
    "    return out\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import layers, models, optimizers,regularizers\n",
    "#from capsulelayers import CapsuleLayer, PrimaryCap, Length\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "def deap_load(data_file,dimention,debaseline):\n",
    "    rnn_suffix = \".mat_win_128_rnn_dataset.pkl\"\n",
    "    label_suffix = \".mat_win_128_labels.pkl\"\n",
    "    arousal_or_valence = dimention\n",
    "    with_or_without = debaseline # 'yes','not'\n",
    "    dataset_dir = \"/home/bsipl_5/experiment/ijcnn-master/deap_shuffled_data/\" + with_or_without + \"_\" + arousal_or_valence + \"/\"\n",
    "\n",
    "    ###load training set\n",
    "    with open(dataset_dir + data_file + rnn_suffix, \"rb\") as fp:\n",
    "        rnn_datasets = pickle.load(fp)\n",
    "    with open(dataset_dir + data_file + label_suffix, \"rb\") as fp:\n",
    "        labels = pickle.load(fp)\n",
    "        labels = np.transpose(labels)\n",
    "\n",
    "    labels = np.asarray(pd.get_dummies(labels), dtype=np.int8)\n",
    "\n",
    "\n",
    "    # shuffle data\n",
    "    index = np.array(range(0, len(labels)))\n",
    "    np.random.shuffle(index)\n",
    "    rnn_datasets = rnn_datasets[index]  # .transpose(0,2,1)\n",
    "    labels = labels[index]\n",
    "\n",
    "    datasets = rnn_datasets.reshape(-1, 128, 32, 1).astype('float32')\n",
    "    labels = labels.astype('float32')\n",
    "\n",
    "    return datasets , labels\n",
    "\n",
    "def dreamer_load(sub,dimention,debaseline):\n",
    "    if debaseline == 'yes':\n",
    "        dataset_suffix = \"f_dataset.pkl\"\n",
    "        label_suffix = \"_labels.pkl\"\n",
    "        dataset_dir = \"/home/bsipl_5/experiment/Data/data_pre(-base)/\" + dimention + \"/\"\n",
    "    else:\n",
    "        dataset_suffix = \"_rnn_dataset.pkl\"\n",
    "        label_suffix = \"_labels.pkl\"\n",
    "        dataset_dir = '/home/bsipl_5/experiment/ijcnn-master/dreamer_shuffled_data/' + 'no_' + dimention + '/'\n",
    "\n",
    "    ###load training set\n",
    "    with open(dataset_dir + sub + dataset_suffix, \"rb\") as fp:\n",
    "        datasets = pickle.load(fp)\n",
    "    with open(dataset_dir + sub + '_' + dimention + label_suffix, \"rb\") as fp:\n",
    "        labels = pickle.load(fp)\n",
    "        labels = np.transpose(labels)\n",
    "\n",
    "    labels = labels > 3\n",
    "    labels = np.asarray(pd.get_dummies(labels), dtype=np.int8)\n",
    "\n",
    "\n",
    "    # shuffle data\n",
    "    index = np.array(range(0, len(labels)))\n",
    "    np.random.shuffle(index)\n",
    "    datasets = datasets[index]  # .transpose(0,2,1)\n",
    "    labels = labels[index]\n",
    "\n",
    "    datasets = datasets.reshape(-1, 128, 14, 1).astype('float32')\n",
    "    labels = labels.astype('float32')\n",
    "\n",
    "    return datasets , labels\n",
    "\"\"\"\n",
    "\n",
    "def CapsNet(input_shape, n_class, routings, model_version,lam_regularize):\n",
    "    \"\"\"\n",
    "    A Capsule Network .\n",
    "    :param input_shape: data shape, 3d, [width, height, channels]\n",
    "    :param n_class: number of classes\n",
    "    :param routings: number of routing iterations\n",
    "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
    "            `eval_model` can also be used for training.\n",
    "    \"\"\"\n",
    "    x = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Layer 1: Just a conventional Conv2D layer, 对DEAP，kernel_size=9；对DREAMER，kernel_size=6\n",
    "    conv1 = layers.Conv2D(filters=4, kernel_size=3, strides=1, padding='valid', activation='relu', name='conv1',kernel_regularizer=regularizers.l2(lam_regularize))(x) #kernel_size=9\n",
    "\n",
    "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
    "    # 对DEAP，kernel_size=9；对DREAMER，kernel_size=6\n",
    "    # 对CapsNet，strides=2，pading=‘valid’；对MLF-CapsNet，stides=1，padding='same'\n",
    "    primarycaps = PrimaryCap(conv1, dim_capsule=4, n_channels=128, kernel_size=5, strides=2, padding='valid',lam_regularize = lam_regularize,model_version =model_version )\n",
    "    \n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    micaps = CapsuleLayer(num_capsule=n_class, dim_capsule=8, routings=routings,\n",
    "                             name='micaps', lam_regularize = lam_regularize)(primarycaps)\n",
    "\n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out_caps = Length(name='capsnet')(micaps)\n",
    "\n",
    "    # Models for training and evaluation (prediction)\n",
    "    y = layers.Input(shape=(n_class,))\n",
    "    train_model = models.Model([x, y], out_caps)\n",
    "    eval_model = models.Model(x, out_caps)\n",
    "\n",
    "\n",
    "    return train_model, eval_model\n",
    "\n",
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))\n",
    "\n",
    "def train(model, data, args):\n",
    "    \"\"\"\n",
    "    Training a CapsuleNet\n",
    "    :param model: the CapsuleNet model\n",
    "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
    "    :param args: arguments\n",
    "    :return: The trained model\n",
    "    \"\"\"\n",
    "    # unpacking the data\n",
    "    (X_train, y_train), (X_test, y_test) = data\n",
    "\n",
    "    # callbacks\n",
    "    log = callbacks.CSVLogger(args.save_dir + '/' + 'log_fold.csv')\n",
    "    tb = callbacks.TensorBoard(log_dir=args.save_dir + '/tensorboard-logs_fold'+str(fold),\n",
    "                               batch_size=args.batch_size, histogram_freq=args.debug)\n",
    "    checkpoint = callbacks.ModelCheckpoint(args.save_dir + '/weights-{epoch:02d}_fold'+str(fold)+'.h5', monitor='val_acc',\n",
    "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: args.lr * (1.0 ** epoch))\n",
    "\n",
    "    #EarlyStop = callbacks.EarlyStopping(monitor='val_capsnet_acc', patience=5)\n",
    "    # compile the model\n",
    "    model.compile(optimizer= optimizers.Adam(lr=args.lr),\n",
    "                  loss= margin_loss,\n",
    "                  metrics={'capsnet': 'accuracy'})\n",
    "\n",
    "    \"\"\"\n",
    "    # Training without data augmentation:\n",
    "    model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs,\n",
    "              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint, lr_decay, EarlyStop])\n",
    "    \"\"\"\n",
    "\n",
    "    '''\n",
    "    # Training with validation set\n",
    "    model.fit([x_train, y_train], y_train ,  batch_size=args.batch_size, epochs=args.epochs,verbose = 1,\n",
    "              validation_split= 0.1 , callbacks=[log, tb, checkpoint, lr_decay])\n",
    "    '''\n",
    "\n",
    "    # Training without validation set\n",
    "    model.fit([X_train, y_train],y_train, batch_size=args.batch_size, epochs=args.epochs,\n",
    "                callbacks=[log, tb, lr_decay])\n",
    "\n",
    "\n",
    "    #from utils import plot_log\n",
    "    #plot_log(args.save_dir + '/log.csv', show=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "time_start_whole = time.time()\n",
    "\n",
    "#dataset_name = 'deap' #'deap' # dreamer\n",
    "#subjects = ['s21','s22','s23','s24','s25','s26','s28','s29','s30','s31','s32']  #  ['s01','s02','s03','s04','s05','s06','s07','s08','s09','s10','s11','s12','s13','s14','s15','s16']#,'s05']#,'s06','s07','s08']#,'s09','s10','s11','s12','s13','s14','s15','s16'，'s17','s18','s19','s20','s21','s22','s23','s24','s25','s26','s27','s28',]\n",
    "#subjects = ['s01'] #'s01','s02','s03','s04','s05','s06','s07','s08','s09','s10','s11','s12','s13','s14','s15','s16','s17','s18','s19','s20',\n",
    "#dimentions = ['dominance']#,'arousal','dominance']\n",
    "#debaseline = 'yes' # yes or not\n",
    "#tune_overfit = 'tune_overfit'\n",
    "#model_version = 'v2' # v0:'CapsNet', v1:'MLF-CapsNet(w/o)', v2:'MLF-CapsNet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--epochs EPOCHS] [--batch_size BATCH_SIZE]\n",
      "                             [--lam_regularize LAM_REGULARIZE] [-r ROUTINGS]\n",
      "                             [--debug DEBUG] [--save_dir SAVE_DIR] [-t]\n",
      "                             [-w WEIGHTS] [--lr LR] [--gpus GPUS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Sandrus\\AppData\\Roaming\\jupyter\\runtime\\kernel-1d32162a-711b-4c43-ba9c-4edece6f48c5.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandrus\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras import callbacks\n",
    "from keras.utils.vis_utils import plot_model\n",
    "#from keras.utils import multi_gpu_model\n",
    "\n",
    "# setting the hyper parameters\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"Capsule Network on \" + folder_day)\n",
    "parser.add_argument('--epochs', default=100, type=int)  \n",
    "parser.add_argument('--batch_size', default=20, type=int)\n",
    "parser.add_argument('--lam_regularize', default=0.0, type=float,\n",
    "                    help=\"The coefficient for the regularizers\")\n",
    "parser.add_argument('-r', '--routings', default=2, type=int,\n",
    "                    help=\"Number of iterations used in routing algorithm. should > 0\")\n",
    "parser.add_argument('--debug', default=0, type=int,\n",
    "                    help=\"Save weights by TensorBoard\")\n",
    "parser.add_argument('--save_dir', default='./result_/sub_dependent_/') # other\n",
    "parser.add_argument('-t', '--testing', action='store_true',\n",
    "                    help=\"Test the trained model on testing dataset\")\n",
    "parser.add_argument('-w', '--weights', default=None,\n",
    "                    help=\"The path of the saved weights. Should be specified when testing\")\n",
    "parser.add_argument('--lr', default=0.00001, type=float,\n",
    "                    help=\"Initial learning rate\")  # v0:0.0001, v2:0.00001\n",
    "parser.add_argument('--gpus', default=2, type=int)\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(time.asctime(time.localtime(time.time())))\n",
    "print(args)\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir)\n",
    "\n",
    "#if dataset_name == 'dreamer':          # load dreamer data\n",
    "#    datasets,labels = dreamer_load(subject,dimention,debaseline)\n",
    "#else:  # load deap data\n",
    "#    datasets,labels = deap_load(subject,dimention,debaseline)\n",
    "\n",
    "args.save_dir = args.save_dir\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir)\n",
    "\n",
    "#fold = 10\n",
    "#test_accuracy_allfold = np.zeros(shape=[0], dtype=float)\n",
    "#train_used_time_allfold = np.zeros(shape=[0], dtype=float)\n",
    "#test_used_time_allfold = np.zeros(shape=[0], dtype=float)\n",
    "#for curr_fold in range(fold):\n",
    "#fold_size = datasets.shape[0] // fold\n",
    "#indexes_list = [i for i in range(len(datasets))]\n",
    "#indexes = np.array(indexes_list)\n",
    "#split_list = [i for i in range(curr_fold * fold_size, (curr_fold + 1) * fold_size)]\n",
    "#split = np.array(split_list)\n",
    "#x_test = datasets[split]\n",
    "#y_test = labels[split]\n",
    "\n",
    "#split = np.array(list(set(indexes_list) ^ set(split_list)))\n",
    "#x_train = datasets[split]\n",
    "#y_train = labels[split]\n",
    "\n",
    "#train_sample = y_train.shape[0]\n",
    "#print(\"training examples:\", train_sample)\n",
    "#test_sample = y_test.shape[0]\n",
    "#print(\"test examples    :\", test_sample)\n",
    "\n",
    "# define model\n",
    "with tf.device('/cpu:0'):\n",
    "    model, eval_model = CapsNet(input_shape=X_train.shape[1:],\n",
    "                                                  n_class=len(np.unique(y_train)),\n",
    "                                                  routings=args.routings,\n",
    "                                                  model_version= model_version,\n",
    "                                                  lam_regularize = args.lam_regularize)\n",
    "model.summary()\n",
    "plot_model(model, to_file=args.save_dir+'/model_fold.png', show_shapes=True)\n",
    "\n",
    "# define muti-gpu model\n",
    "#multi_model = multi_gpu_model(model, gpus=args.gpus)\n",
    "\n",
    "# train\n",
    "train_start_time = time.time()\n",
    "#train(model=multi_model, data=((X_train, y_train), (X_dev, y_dev)), args=args,fold=curr_fold)\n",
    "train(model, data=((X_train, y_train), (X_dev, y_dev)), args=args)\n",
    "train_used_time = time.time() - train_start_time\n",
    "model.save_weights(args.save_dir + '/trained_model_fold.h5')\n",
    "print('Trained model saved to \\'%s/trained_model_fold%s.h5\\'' % (args.save_dir))\n",
    "print('Train time: ', train_used_time)\n",
    "\n",
    "#test\n",
    "#print('-' * 30 + 'fold  ' + str(curr_fold) + '  Begin: test' + '-' * 30)\n",
    "print('  Begin: test')\n",
    "\n",
    "test_start_time = time.time()\n",
    "y_pred = eval_model.predict(X_dev, batch_size=20)  # batch_size = 100\n",
    "dev_used_time = time.time() - test_start_time\n",
    "dev_acc = np.sum(np.argmax(y_pred, 1) == np.argmax(y_dev, 1)) / y_dev.shape[0]\n",
    "#print('shape of y_pred: ',y_pred.shape[0])\n",
    "#print('y_pred: ', y_pred)\n",
    "#print('y_test: ', y_test)\n",
    "print('(' + time.asctime(time.localtime(time.time())) + ') Dev set acc:', dev_acc, 'Dev set time: ',dev_used_time)\n",
    "#print('-' * 30 + 'fold  ' + str(curr_fold) + '  End: test' + '-' * 30)\n",
    "#test_accuracy_allfold = np.append(test_accuracy_allfold, test_acc_fold)\n",
    "#train_used_time_allfold = np.append(train_used_time_allfold, train_used_time_fold)\n",
    "#test_used_time_allfold = np.append(test_used_time_allfold, test_used_time_fold)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "###\n",
    "\"\"\"\n",
    "summary = pd.DataFrame({'fold': range(1,fold+1), 'Test accuracy': test_accuracy_allfold, 'train time': train_used_time_allfold, 'test time': test_used_time_allfold})\n",
    "hyperparam = pd.DataFrame({'average acc of 10 folds': np.mean(test_accuracy_allfold), 'average train time of 10 folds': np.mean(train_used_time_allfold), 'average test time of 10 folds': np.mean(test_used_time_allfold),'epochs': args.epochs, 'lr':args.lr, 'batch size': args.batch_size},index=['dimention/sub'])\n",
    "writer = pd.ExcelWriter(args.save_dir + '/'+'summary'+ '_'+subject+'.xlsx')\n",
    "summary.to_excel(writer, 'Result', index=False)\n",
    "hyperparam.to_excel(writer, 'HyperParam', index=False)\n",
    "writer.save()\n",
    "print('10 fold average accuracy: ', np.mean(test_accuracy_allfold))\n",
    "print('10 fold average train time: ', np.mean(train_used_time_allfold))\n",
    "print('10 fold average test time: ', np.mean(test_used_time_allfold))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
